{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nigam/miniconda3/lib/python3.11/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ase \n",
    "from ase.units import Bohr \n",
    "import torch\n",
    "import metatensor\n",
    "from metatensor import TensorMap, TensorBlock, Labels\n",
    "import matplotlib.pyplot as plt\n",
    "from ase.io import read \n",
    "import hickle\n",
    "from mlelec.utils.metatensor_utils import labels_where\n",
    "\n",
    "from mlelec.data.dataset import PeriodicDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"C2_rotated\"\n",
    "frames = read('examples/data/periodic/c2/C2_rotated.xyz', ':') # will be automated from filename  \n",
    "for f in frames:\n",
    "    f.pbc = [True, True, True]\n",
    "kfock = np.load('examples/data/periodic/c2/results_{}/kfocks.npy'.format(filename))\n",
    "rotated_dataset = PeriodicDataset(frames = frames, kgrid=[4,4,1], matrices_kpoint = kfock, target=[\"real_translation\"] ,device = \"cpu\")#, desired_shifts=desired_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 0],\n",
       " [1, -2, 0],\n",
       " [1, -1, 0],\n",
       " [1, 0, 0],\n",
       " [-2, 1, 0],\n",
       " [-2, -2, 0],\n",
       " [-2, -1, 0],\n",
       " [-2, 0, 0],\n",
       " [-1, -2, 0],\n",
       " [0, 1, 0],\n",
       " [0, -2, 0],\n",
       " [0, 0, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_dataset.desired_shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This should happen in the ML Dataset class #TODO\n",
    "## Fix orbital order \n",
    "from mlelec.utils.twocenter_utils import fix_orbital_order\n",
    "orbs = {6: [[1,0,0],[2,0,0],[2,1,1], [2,1,-1],[2,1,0]]}\n",
    "for T in rotated_dataset.matrices_translation.keys():\n",
    "    rotated_dataset.matrices_translation[T] = fix_orbital_order(rotated_dataset.matrices_translation[T], frames, orbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "withnegative_shifts = list(rotated_dataset.desired_shifts_sup.keys())\n",
    "# withnegative_shifts.remove((0,0,0))\n",
    "desired_shifts = list(rotated_dataset.desired_shifts)\n",
    "desired_shifts.remove([0,0,0]) # for now we remove the zero shift until i have incorporate featuires for zero shift into the feats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix to blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.utils.twocenter_utils import _to_blocks, _to_matrix, _to_coupled_basis, _to_uncoupled_basis, discard_nonhermiticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting matrices out from the dataset for now \n",
    "matrices = {}\n",
    "for s in rotated_dataset.desired_shifts[:]: # positive translations only \n",
    "    matrices[tuple(s)] = torch.from_numpy(rotated_dataset.matrices_translation[tuple(s)]).to('cuda')\n",
    "    # NORMALIZING the translated matrices for each structure - not sure this is for training - SKIP if not needed\n",
    "    # matrices[tuple(s)] = matrices[tuple(s)]/torch.linalg.norm(matrices[tuple(s)], axis = (1,2) ) [:,None, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/twocenter_utils.py:162: UserWarning: Matrix is neither hermitian nor antihermitian - attempting to use _toblocks for NH\n",
      "  warnings.warn(\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/metatensor_utils.py:129: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  block = TensorBlock(\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/metatensor_utils.py:91: DeviceWarning: Blocks values and keys for this TensorMap are on different devices: keys are always on CPU, and blocks values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorMap`.\n",
      "  return TensorMap(keys, blocks)\n"
     ]
    }
   ],
   "source": [
    "target_blocks = {}\n",
    "target_blocks = {}\n",
    "target_coupled_blocks = {}\n",
    "for s in desired_shifts[:]:\n",
    "    target_blocks[tuple(s)] = _to_blocks(matrices[tuple(s)], frames=frames, orbitals=orbs, NH=True) # matrix -> uncoupled\n",
    "    # Dont forget NH = True\n",
    "    target_coupled_blocks[tuple(s)] = _to_coupled_basis(target_blocks[tuple(s)], orbs, skip_symmetry=True) # uncouple -> coupled\n",
    "    # Dont forget skip_symmetry = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218761/3725357898.py:11: DeviceWarning: Blocks values and keys for this TensorMap are on different devices: keys are always on CPU, and blocks values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorMap`.\n",
      "  target = TensorMap(keys=keys, blocks=blocks)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "blocks = []\n",
    "tmap_keys = []\n",
    "for translation in target_coupled_blocks.keys():\n",
    "    for i, (key, block) in enumerate(target_coupled_blocks[translation].items()):\n",
    "        blocks.append(block.copy())\n",
    "        x = list(key)\n",
    "        x.extend(translation)\n",
    "        tmap_keys.append(x)\n",
    "keys = Labels(next(iter(target_coupled_blocks.values())).keys.names+[\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\"], values=np.asarray(tmap_keys))\n",
    "target = TensorMap(keys=keys, blocks=blocks)\n",
    "\n",
    "target = target.keys_to_samples('cell_shift_a')\n",
    "target = target.keys_to_samples('cell_shift_b')\n",
    "target = target.keys_to_samples('cell_shift_c')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.utils.metatensor_utils import labels_where\n",
    "from metatensor import Labels\n",
    "from mlelec.features.acdc import twocenter_hermitian_features, single_center_features, pair_features, twocenter_hermitian_features_periodic\n",
    "from mlelec.utils.twocenter_utils import map_targetkeys_to_featkeys\n",
    "from mlelec.features.acdc import twocenter_features_periodic_NH\n",
    "from mlelec.features.acdc_utils import _pca \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = {'cutoff': 4,\n",
    "          'max_radial':8, \n",
    "          'max_angular':4,\n",
    "          'atomic_gaussian_width':0.3,\n",
    "          'center_atom_weight':1,\n",
    "          \"radial_basis\": {\"Gto\": {}},\n",
    "          \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc.py:132: UserWarning: Using cutoff 7.0 for all pairs feature\n",
      "  warnings.warn(\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc_utils.py:614: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  newblock = TensorBlock(\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc_utils.py:624: DeviceWarning: Blocks values and keys for this TensorMap are on different devices: keys are always on CPU, and blocks values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorMap`.\n",
      "  X = TensorMap(\n"
     ]
    }
   ],
   "source": [
    "# two center features used for all translations except the 0,0,0 translation\n",
    "rhoij = pair_features(frames, hypers, order_nu=[1,1],  all_pairs=True, lcut=2, max_shift=[2,3,1], both_centers=True,)# device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218761/1895663004.py:20: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  blocks.append(TensorBlock(values = b.values[sidx],\n",
      "/tmp/ipykernel_218761/1895663004.py:25: DeviceWarning: Blocks values and keys for this TensorMap are on different devices: keys are always on CPU, and blocks values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorMap`.\n",
      "  rhoij = TensorMap(keys=rhoij.keys, blocks=blocks)\n"
     ]
    }
   ],
   "source": [
    "blocks = []\n",
    "# normalizing features - SKIP if needed \n",
    "# rhoij_norm = {tuple(t):[] for t in desired_shifts}\n",
    "# for i, (k,b) in enumerate(rhoij.items()):\n",
    "#       for t in desired_shifts[:]:\n",
    "#             tlab, tidx = labels_where(b.samples, selection=Labels(names=[\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\"], values=np.array([t]).reshape(1,3)), return_idx=True)\n",
    "#             rhoij_norm[tuple(t)].append(torch.linalg.norm(b.values[tidx])**2)\n",
    "\n",
    "# for t in desired_shifts:\n",
    "#       rhoij_norm[tuple(t)] =torch.sum(torch.tensor(rhoij_norm[tuple(t)])) \n",
    "\n",
    "for i, (k,b) in enumerate(rhoij.items()):\n",
    "      # tlab, tidx = labels_where(b.samples, selection=Labels(names=[\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\"], values=np.array([t]).reshape(1,3)), return_idx=True)\n",
    "\n",
    "      # b.values[tidx] = b.values[tidx]/torch.sqrt(rhoij_norm[tuple(t)]+1e-6) # dividing by norm of features, SKIP if needed \n",
    "\n",
    "      slab, sidx = labels_where(b.samples, selection=Labels(names=[\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\"], values=np.array(desired_shifts[:]).reshape(-1,3)), return_idx=True) # only retain tranlsations that we want - DONT SKIP \n",
    "\n",
    "\n",
    "      blocks.append(TensorBlock(values = b.values[sidx],\n",
    "                              components = b.components,\n",
    "                        samples = Labels(names = b.samples.names, values=np.asarray(b.samples.values[sidx])),\n",
    "                              properties = b.properties)\n",
    "                        )\n",
    "rhoij = TensorMap(keys=rhoij.keys, blocks=blocks)\n",
    "        \n",
    "# One can PCA the features if desired \n",
    "# rhoij_pca = _pca(rhoij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc.py:344: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  TensorBlock(\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc.py:398: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  TensorBlock(\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc.py:409: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  TensorBlock(\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc.py:425: DeviceWarning: Blocks values and keys for this TensorMap are on different devices: keys are always on CPU, and blocks values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorMap`.\n",
      "  return TensorMap(\n"
     ]
    }
   ],
   "source": [
    "hfeat = twocenter_features_periodic_NH(single_center=None, pair= rhoij) # take the pair feature and create features corresponding to different blocks of the hamiltonian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc.py:132: UserWarning: Using cutoff 2.0 for all pairs feature\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_218761/2812144406.py:8: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  tb = TensorBlock(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218761/2812144406.py:15: DeviceWarning: Blocks values and keys for this TensorMap are on different devices: keys are always on CPU, and blocks values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorMap`.\n",
      "  rhoij_zero_shift = TensorMap(keys = rhoij_zero_shift.keys, blocks=blocks)\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc.py:245: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  TensorBlock(\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc.py:290: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  TensorBlock(\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc.py:301: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  TensorBlock(\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc.py:316: DeviceWarning: Blocks values and keys for this TensorMap are on different devices: keys are always on CPU, and blocks values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorMap`.\n",
      "  return TensorMap(\n"
     ]
    }
   ],
   "source": [
    "rhonui = single_center_features(frames, hypers, order_nu=3, lcut=2, max_shift=[2,3,1], feature_names = hfeat.property_names)\n",
    "rhoij_zero_shift = pair_features(frames, hypers, order_nu=[1,1],  both_centers=True, all_pairs=True, lcut=2, max_shift=[1,1,1], feature_names = rhonui.property_names)\n",
    "# pair = rhoij\n",
    "blocks = []\n",
    "for i, (key, block) in enumerate(rhoij_zero_shift.items()):\n",
    "    samples_lab, idx = labels_where(block.samples, selection=Labels([\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\"], values=np.asarray([[0,0,0]]).reshape(-1, 3)), return_idx=True)\n",
    "    samples_lab =  Labels(samples_lab.names[:3], values=samples_lab.values[:,:3])\n",
    "    tb = TensorBlock(\n",
    "        samples=samples_lab,\n",
    "        values=block.values[idx],\n",
    "        properties=block.properties,\n",
    "        components = block.components\n",
    "    )\n",
    "    blocks.append(tb)\n",
    "rhoij_zero_shift = TensorMap(keys = rhoij_zero_shift.keys, blocks=blocks)\n",
    "hfeat0 = twocenter_features_periodic_NH(single_center= rhonui, pair = rhoij_zero_shift, shift=[0,0,0]) # take the pair feature and create features corresponding to different blocks of the hamiltonian\n",
    "hfeat0 = hfeat0.keys_to_samples('cell_shift_a')\n",
    "hfeat0 = hfeat0.keys_to_samples('cell_shift_b')\n",
    "hfeat0 = hfeat0.keys_to_samples('cell_shift_c')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hfeat0.property_names == hfeat.property_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nigam/miniconda3/lib/python3.11/site-packages/metatensor/operations/join.py:301: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  new_block = TensorBlock(\n",
      "/home/nigam/miniconda3/lib/python3.11/site-packages/metatensor/operations/join.py:326: DeviceWarning: Blocks values and keys for this TensorMap are on different devices: keys are always on CPU, and blocks values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorMap`.\n",
      "  tensor = TensorMap(keys=keys, blocks=blocks)\n"
     ]
    }
   ],
   "source": [
    "from metatensor.operations import join \n",
    "hfeat = join([hfeat, hfeat0],axis=\"properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.models.linear import LinearModelPeriodic\n",
    "from mlelec.utils.plot_utils import plot_hamiltonian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72, 6), (72, 6))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0].samples.values.shape, hfeat[0].samples.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 0], [1, -2, 0], [1, -1, 0], [1, 0, 0], [-2, 1, 0], [-2, -2, 0], [-2, -1, 0], [-2, 0, 0], [-1, -2, 0], [0, 1, 0], [0, -2, 0], [0, 0, 0]] 12\n"
     ]
    }
   ],
   "source": [
    "model = LinearModelPeriodic(hfeat, target, frames, orbs,\n",
    "                            cell_shifts= rotated_dataset.desired_shifts[:], \n",
    "                            nhidden=16, nlayers=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# optimizer= torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.LBFGS(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True)\n",
    "\n",
    "\n",
    "def train(model, target_blocks, nepochs=10, optimizer=None, scheduler=None, log_interval=10):\n",
    "    losses = []\n",
    "    for i in range(nepochs):\n",
    "        # def closure():\n",
    "        #     optimizer.zero_grad()\n",
    "        #     rmat = model.forward()\n",
    "        #     loss = 0\n",
    "        #     for s in rmat:            \n",
    "        #         loss += torch.linalg.norm(rmat[s]-matrices[tuple(s)])**2\n",
    "        #     # loss = torch.linalg.norm(rmat - matrices)**2\n",
    "        #     # loss.backward()\n",
    "        #     return loss\n",
    "\n",
    "        # optimizer.step(closure)\n",
    "        # loss = closure()\n",
    "\n",
    "    #---ADAM---- \n",
    "        optimizer.zero_grad()\n",
    "        rmat = model.forward()\n",
    "        loss = 0\n",
    "        for s in rmat:      \n",
    "            for (key, block) in rmat[s].items():\n",
    "                loss += torch.linalg.norm(block.values-target_blocks[tuple(s)].block(key).values)**2      \n",
    "            # loss += torch.sum((rmat[s]-matrices[tuple(s)])**2)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step(loss)\n",
    "        losses.append(loss.item())\n",
    "        if i%log_interval ==0:\n",
    "            print(i, loss.item())\n",
    "    return losses, model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0017503242706879973\n",
      "10 0.0018151219701394439\n",
      "20 0.0017396773910149932\n",
      "30 0.0018684456590563059\n",
      "40 0.001681140041910112\n",
      "50 0.00164368178229779\n",
      "60 0.00163083930965513\n",
      "70 0.0015999069437384605\n",
      "80 0.0015761853428557515\n",
      "90 0.0015561890322715044\n",
      "100 0.0015419040573760867\n",
      "110 0.0017378696938976645\n",
      "120 0.0015758720692247152\n",
      "130 0.0015651609282940626\n",
      "140 0.0028784102760255337\n",
      "150 0.0016673019854351878\n",
      "160 0.0014649612130597234\n",
      "170 0.0014152214862406254\n",
      "180 0.0013902579667046666\n",
      "190 0.001374551560729742\n",
      "200 0.0013600913807749748\n",
      "210 0.0013397824950516224\n",
      "220 0.001326170051470399\n",
      "230 0.0013066146057099104\n",
      "240 0.0012992577394470572\n",
      "250 0.0012790273176506162\n",
      "260 0.001262590754777193\n",
      "270 0.0012443746672943234\n",
      "280 0.0012369112810119987\n",
      "290 0.0012382029090076685\n",
      "300 0.0012149021495133638\n",
      "310 0.0012159771285951138\n",
      "320 0.0011900829849764705\n",
      "330 0.0011772013967856765\n",
      "340 0.0011727316305041313\n",
      "350 0.0011354292510077357\n",
      "360 0.0012393638025969267\n",
      "370 0.00326179969124496\n",
      "380 0.0017214989056810737\n",
      "390 0.001305406098254025\n",
      "400 0.0011346457758918405\n",
      "410 0.0010750413639470935\n",
      "420 0.0010436493903398514\n",
      "430 0.0010283527662977576\n",
      "440 0.0010143857216462493\n",
      "450 0.0010016732849180698\n",
      "460 0.0009893240639939904\n",
      "470 0.00098735885694623\n",
      "480 0.0009682027157396078\n",
      "490 0.0009618885815143585\n",
      "500 0.0009443510789424181\n",
      "510 0.0009387335157953203\n",
      "520 0.0009739347733557224\n",
      "530 0.0009165388182736933\n",
      "540 0.0009013678063638508\n",
      "550 0.0008931825868785381\n",
      "560 0.0008794036693871021\n",
      "570 0.0008720098412595689\n",
      "580 0.0008622952736914158\n",
      "590 0.0008705671061761677\n",
      "600 0.0009777516825124621\n",
      "610 0.0010198608506470919\n",
      "620 0.0012302809627726674\n",
      "630 0.0013657361268997192\n",
      "640 0.000864427478518337\n",
      "650 0.0008177459822036326\n",
      "660 0.0007916035829111934\n",
      "670 0.0007717301486991346\n",
      "680 0.0007645199075341225\n",
      "690 0.0007732701487839222\n",
      "700 0.0007683321018703282\n",
      "710 0.0007642097189091146\n",
      "720 0.0007331148372031748\n",
      "730 0.0007506300462409854\n",
      "740 0.0007129080477170646\n",
      "750 0.0007188818999566138\n",
      "760 0.0007144624833017588\n",
      "770 0.0006885982002131641\n",
      "780 0.0006729987799189985\n",
      "790 0.0006643400993198156\n",
      "800 0.0006558604654856026\n",
      "810 0.0006473134271800518\n",
      "820 0.000647960405331105\n",
      "830 0.0006531720864586532\n",
      "840 0.0006295357597991824\n",
      "850 0.0006212493753992021\n",
      "860 0.0006177836912684143\n",
      "870 0.0036601803731173277\n",
      "880 0.0010251597268506885\n",
      "890 0.001128745381720364\n",
      "900 0.000656703719869256\n",
      "910 0.0006190849817357957\n",
      "920 0.0008957968675531447\n",
      "930 0.000599500082898885\n",
      "940 0.0005803892272524536\n",
      "950 0.0005654102424159646\n",
      "960 0.0005461389664560556\n",
      "970 0.0005402759416028857\n",
      "980 0.0005330700078047812\n",
      "990 0.0005249485839158297\n",
      "1000 0.0005536572425626218\n",
      "1010 0.000577235477976501\n",
      "1020 0.0005102080176584423\n",
      "1030 0.0005037037772126496\n",
      "1040 0.0004917177720926702\n",
      "1050 0.0004831255064345896\n",
      "1060 0.0005314809968695045\n",
      "1070 0.0004706829786300659\n",
      "1080 0.0004801854374818504\n",
      "1090 0.0004575435013975948\n",
      "1100 0.00045199308078736067\n",
      "1110 0.00044602356501854956\n",
      "1120 0.00046161460340954363\n",
      "1130 0.0004899950581602752\n",
      "1140 0.00045762810623273253\n",
      "1150 0.00042880806722678244\n",
      "1160 0.0005153872771188617\n",
      "1170 0.001301419804804027\n",
      "1180 0.0008560408605262637\n",
      "1190 0.00044842742499895394\n",
      "1200 0.0004221241979394108\n",
      "1210 0.0005592346424236894\n",
      "1220 0.0004028215480502695\n",
      "1230 0.00045340164797380567\n",
      "1240 0.0003988427924923599\n",
      "1250 0.0004033405566588044\n",
      "1260 0.0003700392844621092\n",
      "1270 0.00036859422107227147\n",
      "1280 0.0003908605722244829\n",
      "1290 0.00038815211155451834\n",
      "1300 0.0003924046759493649\n",
      "1310 0.00035855078021995723\n",
      "1320 0.0003425389586482197\n",
      "1330 0.00033913200604729354\n",
      "1340 0.0003386875905562192\n",
      "1350 0.00034327543107792735\n",
      "1360 0.0003339777758810669\n",
      "1370 0.00034166284603998065\n",
      "1380 0.00031880458118394017\n",
      "1390 0.0003209382703062147\n",
      "1400 0.0004620724357664585\n",
      "1410 0.0005512846983037889\n",
      "1420 0.00033500842982903123\n",
      "1430 0.0035934934858232737\n",
      "1440 0.0007476214086636901\n",
      "1450 0.000572189106605947\n",
      "1460 0.00041417995817027986\n",
      "1470 0.0003268501895945519\n",
      "1480 0.00029060390079393983\n",
      "1490 0.0002781230432447046\n",
      "1500 0.00027520908042788506\n",
      "1510 0.000275668891845271\n",
      "1520 0.0002660284226294607\n",
      "1530 0.00027022574795410037\n",
      "1540 0.00032819126499816775\n",
      "1550 0.0002676337899174541\n",
      "1560 0.00027467362815514207\n",
      "1570 0.00028964708326384425\n",
      "1580 0.0002541423309594393\n",
      "1590 0.0002907325979322195\n",
      "1600 0.0002467018202878535\n",
      "1610 0.00023767694074194878\n",
      "1620 0.00022995486506260931\n",
      "1630 0.00023209645587485284\n",
      "1640 0.00022397926659323275\n",
      "1650 0.0004539325018413365\n",
      "1660 0.00024551080423407257\n",
      "1670 0.00022832407557871193\n",
      "1680 0.00026974943466484547\n",
      "1690 0.0032769048120826483\n",
      "1700 0.00024281932564917952\n",
      "1710 0.0002837813226506114\n",
      "1720 0.0002957355754915625\n",
      "1730 0.00040411765803582966\n",
      "1740 0.0002781014482025057\n",
      "1750 0.00021684644161723554\n",
      "1760 0.0001987295545404777\n",
      "1770 0.0001913166925078258\n",
      "1780 0.0001870056294137612\n",
      "1790 0.00020875094924122095\n",
      "1800 0.00019740524294320494\n",
      "1810 0.00018351536709815264\n",
      "1820 0.00018105968774762005\n",
      "1830 0.00023211077495943755\n",
      "1840 0.00020275275164749473\n",
      "1850 0.0001720773143460974\n",
      "1860 0.00018031912622973323\n",
      "1870 0.0002202845353167504\n",
      "1880 0.00016539954231120646\n",
      "1890 0.00016417211736552417\n",
      "1900 0.00017211082740686834\n",
      "1910 0.00023458868963643909\n",
      "1920 0.00020906115241814405\n",
      "1930 0.00017902051331475377\n",
      "1940 0.0001879328629001975\n",
      "1950 0.0015336447395384312\n",
      "1960 0.001466403715312481\n",
      "1970 0.0004508627171162516\n",
      "1980 0.00015213766891974956\n",
      "1990 0.00015551919932477176\n"
     ]
    }
   ],
   "source": [
    "losses, model, optimizer = train(model, target_coupled_blocks, nepochs=2000, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f13841bbed0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/metatensor_utils.py:129: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  block = TensorBlock(\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/metatensor_utils.py:91: DeviceWarning: Blocks values and keys for this TensorMap are on different devices: keys are always on CPU, and blocks values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorMap`.\n",
      "  return TensorMap(keys, blocks)\n"
     ]
    }
   ],
   "source": [
    "mblocks = model.forward()\n",
    "rmat = {}\n",
    "for s in model.cell_shifts:\n",
    "    rmat[tuple(s)] = _to_matrix(_to_uncoupled_basis(mblocks[tuple(s)]), frames = frames, orbitals=orbs,  NH=True)  # DONT FORGET NH=True      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdict = {\"mdict\": model.state_dict(), \"optimizer\": optimizer.state_dict(), \"epoch\":4000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mdict, \"rotated_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0] 0.004570793010892625\n",
      "[1, -2, 0] 0.005265318791018532\n",
      "[1, -1, 0] 0.002152167003790616\n",
      "[1, 0, 0] 0.0024277321396065942\n",
      "[-2, 1, 0] 0.006987371963764007\n",
      "[-2, -2, 0] 0.0020260091701494475\n",
      "[-2, -1, 0] 0.0018207808010117148\n",
      "[-2, 0, 0] 0.0027619864360295876\n",
      "[-1, -2, 0] 0.0010077719040180791\n",
      "[0, 1, 0] 0.0034215346115346206\n",
      "[0, -2, 0] 0.0020534574684471134\n",
      "[0, 0, 0] 0.004052818101030509\n"
     ]
    }
   ],
   "source": [
    "# error on individual translations\n",
    "for translation in model.cell_shifts: \n",
    "    print(translation, torch.linalg.norm(rmat[tuple(translation)]-matrices[tuple(translation)]).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_hamiltonian' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig, ax, mappable \u001b[38;5;241m=\u001b[39m \u001b[43mplot_hamiltonian\u001b[49m((matrices[(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m)])\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()[\u001b[38;5;241m0\u001b[39m], plot_abs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m fig\u001b[38;5;241m.\u001b[39mcolorbar(mappable, ax\u001b[38;5;241m=\u001b[39max)\n\u001b[1;32m      3\u001b[0m fig\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_hamiltonian' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax, mappable = plot_hamiltonian((matrices[(0,1,0)]).detach().cpu()[0], plot_abs = True)\n",
    "fig.colorbar(mappable, ax=ax)\n",
    "fig.tight_layout()\n",
    "# fig.savefig('/home/nigam/act010.png', dpi = 400, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: try diff loss functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "from typing import Union, List\n",
    "def loss_zero_shift(pred, target, device=None):\n",
    "    if device is None: \n",
    "        device = next(iter(pred.values())).device\n",
    "    assert pred.keys() == target.keys()\n",
    "    loss = 0\n",
    "    \n",
    "    loss += torch.sum((pred[tuple(s)]-target[tuple(s)])**2)\n",
    "    return loss\n",
    "\n",
    "def loss_fn_indiv_shift(rmat, matrices, specific_shift_idx:Union[str, List]=None, device=None):\n",
    "    #TODO: loss over particular shifts\n",
    "    if device is None: \n",
    "        device = rmat[(0,0,0)].device \n",
    "\n",
    "    #weight_minus = 1; weight_plus = 1 # we could still keep this?  \n",
    "    if not isinstance(next(iter(matrices.values())), torch.Tensor):\n",
    "        matrices = {k:torch.from_numpy(v).type(torch.float).to(device) for k,v in matrices.items()}\n",
    "    loss = 0\n",
    "    if isinstance(specific_shift_idx, list) or isinstance(specific_shift_idx,str):\n",
    "        raise NotImplementedError\n",
    "    # elif isinstance(specific_shift_idx, str):\n",
    "    #     if specific_shift_idx == \"positive\":\n",
    "    #         weight_minus=0\n",
    "    #     elif specific_shift_idx == \"negative\":\n",
    "    #         weight_plus=0\n",
    "           \n",
    "    for s in rmat.keys():            \n",
    "        loss += torch.linalg.norm(rmat[s]-matrices[tuple(s)])**2\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss_fn_combined(rsum, rdiff, expkL:dict, complex_target, device = None):\n",
    "    #TODO : support multiple k points \n",
    "    if device is None: \n",
    "        device = next(iter(rsum.values())).device\n",
    "        complex_target = complex_target.to(device)\n",
    "    assert rsum.keys() == rdiff.keys()\n",
    "    matrix = {}\n",
    "    for s in rsum.keys():\n",
    "        matrix[tuple(s)] = rsum[tuple(s)] + rdiff[tuple(s)]\n",
    "        matrix[tuple(-np.array(s))] = rsum[s] - rdiff[s]\n",
    "\n",
    "    recon_target = torch.zeros_like(complex_target, requires_grad=True, dtype = torch.complex64, device = device)\n",
    "    for s in matrix.keys():\n",
    "         recon_target = recon_target+ matrix[s]*expkL[s]\n",
    "    \n",
    "    loss = torch.tensordot((recon_target-complex_target),torch.conj(recon_target-complex_target)) \n",
    "    # equivalent to torch.linalg.norm((recon_target-complex_target))**2\n",
    "    assert torch.isclose(abs(loss), abs(loss.real))\n",
    "    return loss.real\n",
    "\n",
    "def get_predicted_matrices(rsum, rdiff, device=None):\n",
    "    if device is None: \n",
    "        device = next(iter(rsum.values())).device\n",
    "    assert rsum.keys() == rdiff.keys(), \"rsum and rdiff must have same keys\"\n",
    "    matrix = {}\n",
    "    for s in rsum.keys():\n",
    "        # sint = [int(x) for x in s[1:-1].split(\", \")]\n",
    "        matrix[s] = rsum[s] + rdiff[s]\n",
    "        matrix[tuple(-np.array(s))] = rsum[s] - rdiff[s]\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModelPeriodic(hfeat, target, frames, orbs,\n",
    "                            cell_shifts= withnegative_shifts, \n",
    "                            nhidden=16, nlayers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92716/37171875.py:44: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  pred_blocks.append(TensorBlock(values=pred.reshape((nsamples, ncomp, 1)),samples=block.samples,\n",
      "/tmp/ipykernel_92716/37171875.py:49: DeviceWarning: Blocks values and keys for this TensorMap are on different devices: keys are always on CPU, and blocks values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorMap`.\n",
      "  pred_tmap = TensorMap(self.target_blocks.keys, pred_blocks)\n",
      "/tmp/ipykernel_92716/37171875.py:67: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  blocks.append(TensorBlock(samples = Labels(target.sample_names[:-3], values=np.asarray(block.samples.values[i])[:,:-3]),\n",
      "/tmp/ipykernel_92716/37171875.py:71: DeviceWarning: Blocks values and keys for this TensorMap are on different devices: keys are always on CPU, and blocks values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorMap`.\n",
      "  tmap = TensorMap(target.keys, blocks)\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/metatensor_utils.py:129: DeviceWarning: Values and labels for this block are on different devices: labels are always on CPU, and values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorBlock`.\n",
      "  block = TensorBlock(\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/metatensor_utils.py:91: DeviceWarning: Blocks values and keys for this TensorMap are on different devices: keys are always on CPU, and blocks values are on device 'cuda:0'. If you are using PyTorch and need the labels to also be on cuda:0, you should use `metatensor.torch.TensorMap`.\n",
      "  return TensorMap(keys, blocks)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15.795816421508789\n",
      "1 166.77407836914062\n",
      "2 24.705739974975586\n",
      "3 45.38225173950195\n",
      "4 89.55624389648438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function ArrayWrapper.__init__.<locals>.mts_array_origin at 0x7f994462f880>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nigam/miniconda3/lib/python3.11/site-packages/metatensor/utils.py\", line 30, in inner\n",
      "    @functools.wraps(function)\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "MetatensorError",
     "evalue": "invalid parameter: tried to build a TensorMap from blocks with different origins: at least ('metatensor.data.array.torch') and ('unregistered origin') were detected",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMetatensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1300\u001b[39m):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# loss = closure()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# optimizer.step(closure)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m     rmat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn_indiv_shift(rmat, matrices) \u001b[38;5;66;03m#, specific_shift_idx = 'positive' )\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[50], line 54\u001b[0m, in \u001b[0;36mLinearModelPeriodic.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         pred_blocks\u001b[38;5;241m.\u001b[39mappend(block\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m     49\u001b[0m pred_tmap \u001b[38;5;241m=\u001b[39m TensorMap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_blocks\u001b[38;5;241m.\u001b[39mkeys, pred_blocks)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecon_mat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslations_to_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_tmap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# _to_matrix(_to_uncoupled_basis(pred_sum_dict[s]), frames = self.frames, orbitals=self.orbitals)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecon_mat\n",
      "Cell \u001b[0;32mIn[50], line 71\u001b[0m, in \u001b[0;36mLinearModelPeriodic.translations_to_keys\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m     66\u001b[0m         _,i \u001b[38;5;241m=\u001b[39m labels_where(block\u001b[38;5;241m.\u001b[39msamples, Labels([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_shift_a\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_shift_b\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_shift_c\u001b[39m\u001b[38;5;124m\"\u001b[39m], values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([translation[\u001b[38;5;241m0\u001b[39m], translation[\u001b[38;5;241m1\u001b[39m], translation[\u001b[38;5;241m2\u001b[39m]])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), return_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     67\u001b[0m         blocks\u001b[38;5;241m.\u001b[39mappend(TensorBlock(samples \u001b[38;5;241m=\u001b[39m Labels(target\u001b[38;5;241m.\u001b[39msample_names[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], values\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39masarray(block\u001b[38;5;241m.\u001b[39msamples\u001b[38;5;241m.\u001b[39mvalues[i])[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]),\n\u001b[1;32m     68\u001b[0m                                   values\u001b[38;5;241m=\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues[i],\n\u001b[1;32m     69\u001b[0m                                   components\u001b[38;5;241m=\u001b[39mblock\u001b[38;5;241m.\u001b[39mcomponents,\n\u001b[1;32m     70\u001b[0m                                   properties\u001b[38;5;241m=\u001b[39mblock\u001b[38;5;241m.\u001b[39mproperties))\n\u001b[0;32m---> 71\u001b[0m     tmap \u001b[38;5;241m=\u001b[39m \u001b[43mTensorMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     recon_blocks[\u001b[38;5;28mtuple\u001b[39m(translation)] \u001b[38;5;241m=\u001b[39m tmap\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_shifts[:]:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/tensor.py:114\u001b[0m, in \u001b[0;36mTensorMap.__init__\u001b[0;34m(self, keys, blocks)\u001b[0m\n\u001b[1;32m    109\u001b[0m     block\u001b[38;5;241m.\u001b[39m_move_ptr()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ptr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib\u001b[38;5;241m.\u001b[39mmts_tensormap(\n\u001b[1;32m    112\u001b[0m     keys\u001b[38;5;241m.\u001b[39m_as_mts_labels_t(), blocks_array, \u001b[38;5;28mlen\u001b[39m(blocks)\n\u001b[1;32m    113\u001b[0m )\n\u001b[0;32m--> 114\u001b[0m \u001b[43m_check_pointer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m blocks:\n\u001b[1;32m    117\u001b[0m     block\u001b[38;5;241m.\u001b[39m_is_inside_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/status.py:48\u001b[0m, in \u001b[0;36m_check_pointer\u001b[0;34m(pointer)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MetatensorError(last_error()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MetatensorError(last_error())\n",
      "\u001b[0;31mMetatensorError\u001b[0m: invalid parameter: tried to build a TensorMap from blocks with different origins: at least ('metatensor.data.array.torch') and ('unregistered origin') were detected"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# optimizer= torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.LBFGS(model.parameters(), lr=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True)\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    rmat = model.forward()\n",
    "    loss = loss_fn_indiv_shift(rmat, matrices )\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "losses = []\n",
    "for i in range(1300):\n",
    "    # loss = closure()\n",
    "    # optimizer.step(closure)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    rmat = model.forward()\n",
    "    loss = loss_fn_indiv_shift(rmat, matrices) #, specific_shift_idx = 'positive' )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step(loss)\n",
    "    losses.append(loss.item())\n",
    "    if i%1 ==0:\n",
    "        print(i, loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## band structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
