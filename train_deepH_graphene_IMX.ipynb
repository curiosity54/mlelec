{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab92a44-4e57-4112-bcdf-7bdba9f8888f",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84274a-981b-4bb7-86f0-eadf1e6ae409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import read\n",
    "from ase.visualize import view\n",
    "import numpy as np \n",
    "import torch \n",
    "import metatensor \n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from metatensor import Labels, TensorBlock, TensorMap\n",
    "from mlelec.data.dataset import PySCFPeriodicDataset\n",
    "\n",
    "from mlelec.utils.plot_utils import print_matrix, matrix_norm, block_matrix_norm\n",
    "\n",
    "from metatensor import load, sort\n",
    "from mlelec.utils.twocenter_utils import _to_coupled_basis\n",
    "from mlelec.utils.pbc_utils import matrix_to_blocks, kmatrix_to_blocks, TMap_bloch_sums, precompute_phase\n",
    "\n",
    "from mlelec.features.acdc import pair_features, single_center_features, twocenter_features_periodic_NH, twocenter_hermitian_features\n",
    "import rascaline\n",
    "\n",
    "from mlelec.metrics import L2_loss, L2_kspace_loss\n",
    "from mlelec.models.linear import LinearModelPeriodic\n",
    "\n",
    "from mlelec.utils.pbc_utils import blocks_to_matrix\n",
    "from mlelec.utils.symmetry import ClebschGordanReal\n",
    "CG = ClebschGordanReal(lmax = 3, device = device)\n",
    "\n",
    "def get_targets(dataset, device =\"cpu\", cutoff = None, target='fock', all_pairs= True):\n",
    "    \n",
    "    blocks = matrix_to_blocks(dataset, device = device, cutoff = cutoff, all_pairs = all_pairs, target = target)\n",
    "    coupled_blocks = _to_coupled_basis(blocks, skip_symmetry = True, device = device, translations = True)\n",
    "\n",
    "    blocks = blocks.to(arrays='numpy')\n",
    "    blocks = sort(blocks)\n",
    "    blocks = blocks.to(arrays='torch')\n",
    "    \n",
    "    coupled_blocks = coupled_blocks.to(arrays='numpy')\n",
    "    coupled_blocks = sort(coupled_blocks)\n",
    "    coupled_blocks = coupled_blocks.to(arrays='torch')\n",
    "    \n",
    "    return blocks, coupled_blocks\n",
    "\n",
    "def compute_features(dataset):\n",
    "\n",
    "    rhoij = pair_features(dataset.structures, hypers_atom, hypers_pair, order_nu = 1, all_pairs = all_pairs, both_centers = both_centers, mic = False,\n",
    "                          kmesh = dataset.kmesh, device = device, lcut = LCUT, return_rho0ij = return_rho0ij, counter = None, \n",
    "                          T_dict = None)  \n",
    "    \n",
    "    if both_centers and not return_rho0ij:\n",
    "        NU = 3\n",
    "    else:\n",
    "        NU = 2\n",
    "    rhonui = single_center_features(dataset.structures, hypers_atom, order_nu = NU, lcut = LCUT, device = device,\n",
    "                                    feature_names = rhoij.property_names)\n",
    "    \n",
    "    hfeat = twocenter_features_periodic_NH(single_center = rhonui, pair = rhoij, all_pairs = all_pairs)\n",
    "\n",
    "    return hfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedad6a8-e3dc-491f-90e7-bb4b4c9198c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "orbitals = {6: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]]}\n",
    "orbital_names = 'sto-3g'\n",
    "START = 0\n",
    "STOP = 4\n",
    "kmesh = [1,1,1]\n",
    "FT_norm = 2*np.prod(kmesh)\n",
    "\n",
    "cutoff = 6\n",
    "\n",
    "# features hypers\n",
    "max_radial  = 8\n",
    "max_angular = 5\n",
    "atomic_gaussian_width = 0.3\n",
    "return_rho0ij = False\n",
    "both_centers = False\n",
    "all_pairs = False\n",
    "LCUT = 3\n",
    "\n",
    "# Training \n",
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "nhidden = 256\n",
    "nlayers = 2\n",
    "nepochs = 20000\n",
    "nepochs_realH = 1000\n",
    "activation = 'SiLU'\n",
    "train_bias = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e4dca-c4cf-4e24-b28c-11d42e5476a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = './'\n",
    "\n",
    "data_dir = \"/exports/commonscratch/pegolo/cp2k_ham/deepH_graphene\"\n",
    "data_prefix = \"wgraphene\"\n",
    "\n",
    "root = f'{workdir}/examples/data/periodic'\n",
    "\n",
    "frames = read(f'{data_dir}/wrapped_deepH_graphene.xyz', slice(START, STOP))\n",
    "rfock = [np.load(f\"{data_dir}/{data_prefix}_{i}/realfock_{i}.npy\", allow_pickle = True).item() for i in range(START, STOP)]\n",
    "rover = [np.load(f\"{data_dir}/{data_prefix}_{i}/realoverlap_{i}.npy\", allow_pickle = True).item() for i in range(START, STOP)]\n",
    "\n",
    "dataset = PySCFPeriodicDataset(frames = frames, \n",
    "                               kmesh = kmesh, \n",
    "                               dimension = 2,\n",
    "                               fock_realspace = rfock, \n",
    "                               overlap_realspace = rover, \n",
    "                               device = device, \n",
    "                               orbs = orbitals[ORBS], \n",
    "                               orbs_name = 'sto-3g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97646a42-ed95-47a6-aedf-df03b14aad5a",
   "metadata": {},
   "source": [
    "# Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c2686-e4c1-48e8-b2f8-1d72b18b54f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_blocks, target_coupled_blocks = get_targets(dataset, cutoff = cutoff, device = device, all_pairs = all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1204a4-bf68-421e-85a9-15cafdafde0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_target_blocks = kmatrix_to_blocks(dataset, cutoff = cutoff, all_pairs = all_pairs)\n",
    "phase, indices = precompute_phase(target_blocks, dataset, cutoff = cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39d934b-f83b-4970-b8dc-016f1cbd879b",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3a6b8-9b9e-4c98-8077-837eec42043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers_pair = {'cutoff': cutoff,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': atomic_gaussian_width,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}}}\n",
    "\n",
    "hypers_atom = {'cutoff': 4,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': 0.3,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8f766-88a6-4ef0-be83-544502d401fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    hfeat = metatensor.load(f'{workdir}/features.npz').to(arrays = 'torch')\n",
    "except:\n",
    "    hfeat = compute_features(dataset)\n",
    "    metatensor.save(f'{workdir}/features', hfeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e983ab9c-321e-49ab-beea-6ba58e86f63d",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6737eca-e2db-4c68-9302-8c11612cb010",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModelPeriodic(twocfeat = hfeat, \n",
    "                            target_blocks = target_coupled_blocks,\n",
    "                            frames = dataset.structures, orbitals = dataset.basis, \n",
    "                            device = device,\n",
    "                            bias = train_bias,\n",
    "                            nhidden = nhidded, \n",
    "                            nlayers = nlayers,\n",
    "                            activation = activation,\n",
    "                            apply_norm = True)\n",
    "model = model.double()\n",
    "\n",
    "optimizers = []\n",
    "schedulers = []\n",
    "for i, key in enumerate(model.model):\n",
    "    optimizers.append(torch.optim.AdamW(model.model[key].parameters(), lr = 1e-3, betas = (0.8, 0.9)))\n",
    "    schedulers.append(torch.optim.lr_scheduler.ReduceLROnPlateau(optimizers[-1], factor = 0.8, patience = 30, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c6a274-ff97-47ff-bbf4-ea37f9327ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlelec.utils.twocenter_utils import _to_uncoupled_basis, map_targetkeys_to_featkeys\n",
    "from mlelec.utils.pbc_utils import precompute_phase, TMap_bloch_sums\n",
    "\n",
    "phase, indices = precompute_phase(target_blocks, dataset, cutoff = cutoff)\n",
    "\n",
    "loss_real = []\n",
    "loss_k = []\n",
    "losses = {}\n",
    "\n",
    "new_sched = True\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    model.train(True)\n",
    "\n",
    "    for ik, key in enumerate(model.model):\n",
    "        optimizers[ik].zero_grad()\n",
    "    \n",
    "    pred = model()\n",
    "    unc_pred = _to_uncoupled_basis(pred, cg = CG)\n",
    "    pred_kspace = TMap_bloch_sums(unc_pred, phase, indices)\n",
    "    \n",
    "    n_predictions = sum([np.prod(b.values.shape) for _, b in pred.items()])\n",
    "\n",
    "    if epoch < nepochs_realH:\n",
    "\n",
    "        # Compute the loss for each block\n",
    "        all_losses, epoch_loss = L2_loss(pred, target_coupled_blocks, loss_per_block = True)\n",
    "\n",
    "        # Total loss\n",
    "        epoch_loss = epoch_loss.item()\n",
    "        \n",
    "        # Append the values of the loss to a list\n",
    "        loss_k.append(L2_loss(pred_kspace, k_target_blocks, norm = FT_norm).item())\n",
    "        loss_real.append(epoch_loss)\n",
    "\n",
    "        # Loop through submodels and backpropagate\n",
    "        for ik, (loss, key) in enumerate(zip(all_losses, model.model)):\n",
    "            loss.backward(retain_graph = False)\n",
    "            torch.nn.utils.clip_grad_norm_(model.model[key].parameters(), 1)\n",
    "            optimizers[ik].step()\n",
    "            schedulers[ik].step(loss)\n",
    "            \n",
    "            if key not in losses:\n",
    "                losses[key] = [loss.item()]\n",
    "                learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "            else:\n",
    "                losses[key].append(loss.item())\n",
    "                learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "                \n",
    "    else:\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = L2_loss(pred_kspace, k_target_blocks, norm = FT_norm)\n",
    "        all_losses, loss_real_ = L2_loss(pred, target_coupled_blocks, loss_per_block = True)\n",
    "        for ik, (loss_, key) in enumerate(zip(all_losses, model.model)):\n",
    "            if key not in losses:\n",
    "                losses[key] = [loss_.item()]\n",
    "                learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "            else:\n",
    "                losses[key].append(loss_.item())\n",
    "                learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "        \n",
    "        # Total loss \n",
    "        epoch_loss = loss.item()\n",
    "        \n",
    "        # Append the values of the loss to a list\n",
    "        loss_real.append(loss_real_.item())\n",
    "        loss_k.append(epoch_loss)\n",
    "\n",
    "        loss.backward(retain_graph = True)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        for ik, key in enumerate(model.model):\n",
    "            optimizers[ik].step()\n",
    "            schedulers[ik].step(epoch_loss/len(model.model))\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss on all blocks {epoch_loss:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        print(f\"Epoch {epoch:>7d}, train loss real {loss_real[-1]:>15.10f}, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print('Saving', flush = True)\n",
    "        np.save(f'{workdir}/losses.npy', losses)\n",
    "        metatensor.save(f'{workdir}/predictions', pred)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': [optimizer.state_dict() for optimizer in optimizers],\n",
    "            'loss': [loss for loss in all_losses]\n",
    "            }, f'{workdir}/model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
