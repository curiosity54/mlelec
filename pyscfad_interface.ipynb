{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.data.dataset import precomputed_molecules, MoleculeDataset, MLDataset\n",
    "import torch\n",
    "from ase.io import read\n",
    "import ase\n",
    "from mlelec.models.linear import LinearTargetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading structures\n",
      "examples/data/water_1000/sto-3g/fock.hickle\n"
     ]
    }
   ],
   "source": [
    "h_data = MoleculeDataset(mol_name='water_1000', frame_slice=slice(0,10), data_path = 'examples/data/water_1000/sto-3g', aux_path = 'examples/data/water_1000/sto-3g', device='cuda', aux=['overlap', 'orbitals']) #frames =frames, frame_slice=':4', target_data={'fock': h}, aux=['overlap', 'orbitals'],aux_data = {'overlap': over, 'orbitals':orbs }\n",
    "h_ml = MLDataset(molecule_data=h_data, device ='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_ml._shuffle(random_seed=5381)\n",
    "# h_ml._split_indices(train_frac=0.7, val_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in h_ml.structures:\n",
    "    f.pbc = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training on a tiny dataset for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing features with default hypers\n",
      "cuda:0 cuda\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=10, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "cuda:0 cuda\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=10, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "cuda:0 cuda\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=10, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "cuda:0 cuda\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=10, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "cuda:0 cuda\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=10, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "cuda:0 cuda\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=10, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "cuda:0 cuda\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=448, out_features=10, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "cuda:0 cuda\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=10, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "cuda:0 cuda\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=10, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "cuda:0 cuda\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=192, out_features=10, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "cuda:0 cuda\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=10, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "cuda:0 cuda\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=10, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linmod = LinearTargetModel(dataset = h_ml, metrics = \"l2_loss\", nlayers = 1, nout = 1, nhidden = 10, bias = False, device = 'cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 cuda:0\n",
      "tensor(69.3503, device='cuda:0', dtype=torch.float64)\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "tensor(64.6939, device='cuda:0', dtype=torch.float64)\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "tensor(56.2616, device='cuda:0', dtype=torch.float64)\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "tensor(46.7579, device='cuda:0', dtype=torch.float64)\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "tensor(39.9948, device='cuda:0', dtype=torch.float64)\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "tensor(23.1046, device='cuda:0', dtype=torch.float64)\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "tensor(29.5107, device='cuda:0', dtype=torch.float64)\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "tensor(59.9916, device='cuda:0', dtype=torch.float64)\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "tensor(72.7700, device='cuda:0', dtype=torch.float64)\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "tensor(54.8793, device='cuda:0', dtype=torch.float64)\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0 cuda:0\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(linmod.parameters(), lr=0.01)\n",
    "for epoch in range(100):\n",
    "    loss = linmod.forward()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(torch.sqrt(loss.detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default features and model is quite bad - so no wonder losses are high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 cuda:0\n",
      "torch.Size([10, 7, 7])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "linmod.forward()\n",
    "fock = linmod.reconstructed_tensor\n",
    "print(fock.shape)\n",
    "print(fock.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fock = fock.type(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plugging in predicted fock matrix into pyscfad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nigam/miniconda3/lib/python3.11/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ['PYSCFAD_BACKEND']='torch'\n",
    "\n",
    "import torch\n",
    "from pyscf import gto\n",
    "\n",
    "from pyscfad import numpy as np\n",
    "from pyscfad import ops\n",
    "from pyscfad.ml.scf import hf\n",
    "import pyscf.pbc.tools.pyscf_ase as pyscf_ase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mol = gto.Mole()\n",
    "mol.atom = pyscf_ase.ase_atoms_to_pyscf(h_ml.structures[0])\n",
    "mol.basis = 'sto-3g'\n",
    "mol.build()\n",
    "fock = linmod.reconstructed_tensor[0].type(torch.float64)\n",
    "\n",
    "\n",
    "mf = hf.SCF(mol)\n",
    "\n",
    "mo_energy, mo_coeff = mf.eig(fock, s = torch.eye(fock.shape[-1], dtype = fock.dtype))\n",
    "mo_occ = mf.get_occ(mo_energy) # get_occ returns a numpy array\n",
    "mo_occ = ops.convert_to_tensor(mo_occ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fock.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-24.6125,  -2.7584,  -0.0690,   0.0000,  -0.0888,  -1.3858,  -1.5692],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fock[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dipole moment(X, Y, Z, Debye): -1.81266, -2.04481,  1.76728\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_267180/1939595782.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(fock.grad)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dm1 = mf.make_rdm1(mo_coeff, mo_occ)\n",
    "dip = mf.dip_moment(dm=dm1)\n",
    "dip_norm = np.linalg.norm(dip)\n",
    "dip_norm.backward(retain_graph=True)\n",
    "print(fock.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2543, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dip_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dip_norm.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    235\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    236\u001b[0m     (inputs,)\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    243\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 244\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:117\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    121\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "fock.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
