{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d8f12c-62ee-4d46-b766-c3b49d817fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635a399b-c69b-4438-bac6-4ba0712d0d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/pyscf/dft/libxc.py:771: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, corresponding to the original definition by Stephens et al. (issue 1480) and the same as the B3LYP functional in Gaussian. To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read\n",
    "from ase.visualize import view\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import torch \n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import rascaline\n",
    "\n",
    "import metatensor \n",
    "from metatensor import Labels, TensorBlock, TensorMap\n",
    "from metatensor import load, sort\n",
    "\n",
    "from mlelec.data.dataset import PySCFPeriodicDataset, split_by_Aij_mts\n",
    "from mlelec.utils.twocenter_utils import _to_coupled_basis\n",
    "from mlelec.utils.pbc_utils import matrix_to_blocks, kmatrix_to_blocks, TMap_bloch_sums, precompute_phase, kblocks_to_matrix, kmatrix_to_blocks, blocks_to_matrix, matrix_to_blocks\n",
    "from mlelec.utils.plot_utils import print_matrix, matrix_norm, block_matrix_norm, plot_block_errors\n",
    "from mlelec.features.acdc import pair_features, single_center_features, twocenter_features_periodic_NH, twocenter_hermitian_features\n",
    "from mlelec.models.linear import LinearModelPeriodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a54e69-a6cf-413a-a1fa-bec6edae654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(dataset, device =\"cpu\", cutoff = None, target='fock', all_pairs = False, sort_orbs = True):\n",
    "    \n",
    "    blocks = matrix_to_blocks(dataset, device = device, cutoff = cutoff, all_pairs = all_pairs, target = target, sort_orbs = sort_orbs)\n",
    "    coupled_blocks = _to_coupled_basis(blocks, skip_symmetry = True, device = device, translations = True)\n",
    "\n",
    "    blocks = blocks.to(arrays='numpy')\n",
    "    blocks = sort(blocks)\n",
    "    blocks = blocks.to(arrays='torch')\n",
    "    \n",
    "    coupled_blocks = coupled_blocks.to(arrays='numpy')\n",
    "    coupled_blocks = sort(coupled_blocks)\n",
    "    coupled_blocks = coupled_blocks.to(arrays='torch')\n",
    "    \n",
    "    return blocks, coupled_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc04a6ae-03c9-489e-b7e3-347cc86f8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(dataset, all_pairs=False):\n",
    "\n",
    "    rhoij = pair_features(dataset.structures, hypers_atom, hypers_pair, order_nu = 1, all_pairs = all_pairs, both_centers = both_centers,\n",
    "                          kmesh = dataset.kmesh, device = device, lcut = LCUT, return_rho0ij = return_rho0ij)  \n",
    "    \n",
    "    if both_centers and not return_rho0ij:\n",
    "        NU = 3\n",
    "    else:\n",
    "        NU = 2\n",
    "    rhonui = single_center_features(dataset.structures, hypers_atom, order_nu = NU, lcut = LCUT, device = device,\n",
    "                                    feature_names = rhoij.property_names)\n",
    "    \n",
    "    hfeat = twocenter_features_periodic_NH(single_center = rhonui, pair = rhoij, all_pairs = all_pairs)\n",
    "\n",
    "    return hfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0131f6a-72a2-445e-9384-0ba95406c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "orbitals = {\n",
    "    'sto-3g': {5: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               6: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               7: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]]}, \n",
    "    \n",
    "    'def2svp': {6: [[1,0,0],[2,0,0],[3,0,0],[2,1,1], [2,1,-1],[2,1,0], [3,1,1], [3,1,-1],[3,1,0], [3,2,-2], [3,2,-1],[3,2,0], [3,2,1],[3,2,2]]},\n",
    "    'benzene': {6: [[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], 1:[[1,0,0]]},\n",
    "    'gthszvmolopt': {\n",
    "        6: [[2, 0, 0], [2, 1, -1], [2, 1, 0], [2, 1, 1]],\n",
    "        \n",
    "        16: [[3,0,0], \n",
    "             [3,1,-1], [3,1,0], [3,1,1]],\n",
    "\n",
    "        42: [[4,0,0], \n",
    "             [5,0,0], \n",
    "             [4,1,-1], [4,1,0], [4,1,1], \n",
    "             [4, 2, -2], [4, 2, -1], [4, 2, 0], [4, 2, 1], [4, 2, 2]]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61756a8d-5fbe-4fd2-9521-4a4604762d6d",
   "metadata": {},
   "source": [
    "# QC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb8d851-377b-4bd5-a632-397f666731e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = './'\n",
    "START = 0 \n",
    "STOP = 10\n",
    "ORBS = 'sto-3g'\n",
    "root = f'{workdir}/examples/data/periodic/deepH_graphene/wrap/'\n",
    "data_dir = root\n",
    "frames = read(f'{data_dir}/wrapped_deepH_graphene.xyz', slice(START, STOP))\n",
    "rfock = [np.load(f\"{data_dir}/realfock_{i}.npy\", allow_pickle = True).item() for i in range(START, STOP)]\n",
    "rover = [np.load(f\"{data_dir}/realoverlap_{i}.npy\", allow_pickle = True).item() for i in range(START, STOP)]\n",
    "kmesh = [1,1,1]\n",
    "dataset = PySCFPeriodicDataset(frames = frames, \n",
    "                               kmesh = kmesh, \n",
    "                               dimension = 2,\n",
    "                               fock_realspace = rfock, \n",
    "                               overlap_realspace = rover, \n",
    "                               device = device, \n",
    "                               orbs = orbitals[ORBS], \n",
    "                               orbs_name = 'sto-3g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3873bac-ec05-4a55-aeb6-abc711ebe1b2",
   "metadata": {},
   "source": [
    "# Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ac2b01-c2a3-47e2-9296-323a5eddd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567ec238-ff88-4497-930f-fcb0551861ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_blocks, target_coupled_blocks = get_targets(dataset, cutoff = cutoff, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bc35a4fc-fb6e-4fd7-9376-6e83d9047b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_target_coupled_blocks = _to_coupled_basis(kmatrix_to_blocks(dataset, cutoff = cutoff), skip_symmetry = True, device = device, kspace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa78cad-0988-4be9-812a-2209fdc5a9bf",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db41bfa2-936d-4d9f-8f3c-22580d7959a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_radial  = 6\n",
    "max_angular = 4\n",
    "atomic_gaussian_width = 0.3\n",
    "\n",
    "hypers_pair = {'cutoff': cutoff,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': atomic_gaussian_width,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}}}\n",
    "\n",
    "\n",
    "hypers_atom = {'cutoff': 4,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': 0.3,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}}}\n",
    "\n",
    "\n",
    "return_rho0ij = False\n",
    "both_centers = False\n",
    "LCUT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e89ccaa-95c0-44d2-938c-3189761c58c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu pair features\n",
      "cpu single center features\n",
      "cpu single center features\n"
     ]
    }
   ],
   "source": [
    "features = compute_features(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2566f8d-43e1-4404-bf31-e80c1e2e7e57",
   "metadata": {},
   "source": [
    "# ML Dataset/Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a61b2ec7-8647-4c4c-a843-e38141d5b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metatensor.learn import Dataset, DataLoader, IndexedDataset\n",
    "from metatensor.learn.data import group as mts_group, group_and_join as group_and_join_mts\n",
    "import metatensor as mts\n",
    "from mlelec.data.dataset import split_by_Aij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06da6f3b-f22f-4e94-81d3-851162f746a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.78 s ± 1.01 s per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 10\n",
    "split_features_mts, split_target_mts = split_by_Aij_mts(target_coupled_blocks, features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a6ecf1e-63b8-403c-aa04-080b985e3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 ms ± 725 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 10\n",
    "split_features, split_target = split_by_Aij(target_coupled_blocks, features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b53aaef2-7c6b-420e-9938-ceb474a9c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_features, split_target = split_by_Aij(target_coupled_blocks, features = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b33a7a-f2b6-424b-a974-2e3b73184b63",
   "metadata": {},
   "source": [
    "## Using tensormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eda452c7-f3ce-4699-a801-435a51cb0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_features, split_target = split_by_Aij_mts(target_coupled_blocks, features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38900ab1-8d11-47b6-89d5-f184da2aae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = IndexedDataset(descriptor = list(split_features.values()), target = list(split_target.values()), sample_id = list(split_target.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "401203e3-4809-415a-a8d4-b89347c2e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "dataloader = DataLoader(ml_data, batch_size = batch_size, shuffle = True, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59794982-77a8-4ae4-b6cd-0176753165f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase, indices, kpts_idx = precompute_phase(target_coupled_blocks, dataset, cutoff = cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "37b9e355-1d5b-49f9-8c74-ac4a061b1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmap = []\n",
    "for batch in dataloader:\n",
    "    kmap.append(TMap_bloch_sums(batch.target, phase, indices, kpts_idx, return_tensormap = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b5ac5-c943-4a21-95e2-a652795b6923",
   "metadata": {},
   "source": [
    "## Metatensor native splitting by structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d5444484-9a93-4bde-9d7c-513f339ecb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_axis = \"samples\"\n",
    "split_by_dimension = \"structure\"\n",
    "\n",
    "grouped_labels = [mts.Labels(names = split_by_dimension, values = np.array([A])) for A in mts.unique_metadata(target_coupled_blocks, axis = split_by_axis, names = split_by_dimension)]\n",
    "split_target = mts.split(target_coupled_blocks, split_by_axis, grouped_labels)\n",
    "\n",
    "grouped_labels = [mts.Labels(names = split_by_dimension, values = np.array([A])) for A in mts.unique_metadata(features, axis = split_by_axis, names = split_by_dimension)]\n",
    "split_features = mts.split(features, split_by_axis, grouped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce082aef-50f9-4974-8501-d077de90419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = IndexedDataset(descriptor = split_features, target = split_target, sample_id = [g.values.tolist()[0][0] for g in grouped_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8f0a705e-2e24-4967-bd30-de8e513948d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = DataLoader(ml_data, batch_size = batch_size, shuffle = True, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc808f17-7880-4242-9109-bad6638e7f81",
   "metadata": {},
   "source": [
    "Split kspace targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a9e95cde-928d-4d13-935b-02013a2798de",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_axis = \"samples\"\n",
    "split_by_dimension = \"structure\"\n",
    "\n",
    "grouped_labels = [mts.Labels(names = split_by_dimension, values = np.array([A])) for A in mts.unique_metadata(k_target_coupled_blocks, axis = split_by_axis, names = split_by_dimension)]\n",
    "split_target_k = mts.split(k_target_coupled_blocks, split_by_axis, grouped_labels)\n",
    "\n",
    "grouped_labels = [mts.Labels(names = split_by_dimension, values = np.array([A])) for A in mts.unique_metadata(features, axis = split_by_axis, names = split_by_dimension)]\n",
    "split_features_k = mts.split(features, split_by_axis, grouped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee77aac7-6adb-4ffa-8f66-5b4a02e372dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_k = IndexedDataset(descriptor = split_features_k, target = split_target_k, sample_id = [g.values.tolist()[0][0] for g in grouped_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a2e61-609c-42ab-8dda-2f11ebe50a59",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "292e1e88-8d09-4e78-972e-055688f42012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.metrics import L2_loss, L2_kspace_loss\n",
    "from mlelec.utils.twocenter_utils import _to_uncoupled_basis, map_targetkeys_to_featkeys\n",
    "from mlelec.utils.pbc_utils import precompute_phase, TMap_bloch_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8ec08c42-7548-4d7d-b243-5ff30b668e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = LinearModelPeriodic(twocfeat = features, \n",
    "                            target_blocks = target_coupled_blocks,\n",
    "                            frames = dataset.structures, orbitals = dataset.basis, \n",
    "                            device = device,\n",
    "                            bias = True,\n",
    "                            nhidden = 128, \n",
    "                            nlayers = 1,\n",
    "                            activation = 'SiLU',\n",
    "                            apply_norm = True\n",
    "                           )\n",
    "model = model.double()\n",
    "\n",
    "nepoch = 1000\n",
    "\n",
    "optimizers = []\n",
    "schedulers = []\n",
    "for i, key in enumerate(model.model):\n",
    "    optimizers.append(torch.optim.AdamW(model.model[key].parameters(), lr = 1e-3, betas = (0.8, 0.9)))\n",
    "    schedulers.append(torch.optim.lr_scheduler.ReduceLROnPlateau(optimizers[-1], factor = 0.8, patience = 30, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "254ddf17-6837-4c81-823c-7ed539682a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase, indices, kpts_idx = precompute_phase(target_coupled_blocks, dataset, cutoff = cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c96517d-d601-4587-a1b4-7e39c536ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = DataLoader(ml_data, batch_size = batch_size, shuffle = False, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))\n",
    "dataloader_k = DataLoader(ml_data_k, batch_size = batch_size, shuffle = False, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a5e75a-0909-433c-8676-6f3b89d9df49",
   "metadata": {},
   "source": [
    "# TODO: change predict to handle kspace batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e11ca466-6978-4acc-8ab0-378136947d24",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[489, 1, 1]' is invalid for input of size 648",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ik, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel):\n\u001b[1;32m     47\u001b[0m     optimizers[ik]\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 49\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescriptor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m pred_kspace \u001b[38;5;241m=\u001b[39m TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/models/linear.py:596\u001b[0m, in \u001b[0;36mLinearModelPeriodic.predict_batch\u001b[0;34m(self, features, target_blocks, return_matrix)\u001b[0m\n\u001b[1;32m    591\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblockmodels[\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtuple\u001b[39m(k))](feat\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;66;03m# print(pred.shape, nsamples)\u001b[39;00m\n\u001b[1;32m    594\u001b[0m     pred_blocks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    595\u001b[0m         TensorBlock(\n\u001b[0;32m--> 596\u001b[0m             values\u001b[38;5;241m=\u001b[39m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnsamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    597\u001b[0m             samples\u001b[38;5;241m=\u001b[39mblock\u001b[38;5;241m.\u001b[39msamples,\n\u001b[1;32m    598\u001b[0m             components\u001b[38;5;241m=\u001b[39mblock\u001b[38;5;241m.\u001b[39mcomponents,\n\u001b[1;32m    599\u001b[0m             properties\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdummy_property,\n\u001b[1;32m    600\u001b[0m         )\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[489, 1, 1]' is invalid for input of size 648"
     ]
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "train_kspace = False\n",
    "# loss_real = []\n",
    "\n",
    "nepoch = 100\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    if epoch >= 0:\n",
    "        train_kspace = True\n",
    "\n",
    "    if not train_kspace:\n",
    "        # Train against real space targets\n",
    "        LOSS = 0\n",
    "        for ib, batch in enumerate(dataloader):\n",
    "            \n",
    "            model.train(True)\n",
    "            \n",
    "            for ik, key in enumerate(model.model):\n",
    "                optimizers[ik].zero_grad()\n",
    "            \n",
    "            pred = model.predict_batch(batch.descriptor, batch.target)\n",
    "            \n",
    "            # Compute the loss for each block\n",
    "            all_losses, epoch_loss = L2_loss(pred, batch.target, loss_per_block = True)\n",
    "    \n",
    "            # Total loss\n",
    "            epoch_loss = epoch_loss.item()\n",
    "            LOSS += epoch_loss\n",
    "    \n",
    "            # Loop through submodels and backpropagate\n",
    "            for ik, (loss, key) in enumerate(zip(all_losses, model.model)):\n",
    "                loss.backward(retain_graph = False)\n",
    "                torch.nn.utils.clip_grad_norm_(model.model[key].parameters(), 1)\n",
    "                optimizers[ik].step()\n",
    "                schedulers[ik].step(loss)\n",
    "\n",
    "    else:\n",
    "        # Train against k-space targets\n",
    "    \n",
    "        LOSS = 0\n",
    "        for ib, batch in enumerate(dataloader_k):\n",
    "\n",
    "            model.train(True)\n",
    "    \n",
    "            for ik, key in enumerate(model.model):\n",
    "                optimizers[ik].zero_grad()\n",
    "            \n",
    "            pred = model.predict_batch(batch.descriptor, batch.target)\n",
    "            pred_kspace = TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap = True)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = L2_loss(pred_kspace, batch.target, norm = 2)\n",
    "    \n",
    "            # Total loss \n",
    "            epoch_loss = loss.item()\n",
    "            LOSS += epoch_loss\n",
    "                   \n",
    "            loss.backward(retain_graph = True)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    \n",
    "            for ik, key in enumerate(model.model):\n",
    "                optimizers[ik].step()\n",
    "                schedulers[ik].step(epoch_loss/len(model.model))\n",
    "\n",
    "    if epoch >= 0: #% 10 == 0:\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss on all blocks {epoch_loss:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss real {loss_real[-1]:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        print(f\"Epoch {epoch:>7d}, train loss real {LOSS:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b26765c-21a5-4905-8e48-93cafb37067c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labels(\n",
       "    structure  center  neighbor  cell_shift_a  cell_shift_b  cell_shift_c\n",
       "        0        0        0           0             0             0\n",
       "        0        1        1           0             0             0\n",
       "        0        2        2           0             0             0\n",
       "        0        3        3           0             0             0\n",
       "        0        4        4           0             0             0\n",
       "        0        5        5           0             0             0\n",
       "        0        6        6           0             0             0\n",
       "        0        7        7           0             0             0\n",
       "        0        8        8           0             0             0\n",
       "        0        9        9           0             0             0\n",
       "        0        10       10          0             0             0\n",
       "        0        11       11          0             0             0\n",
       "        0        12       12          0             0             0\n",
       "        0        13       13          0             0             0\n",
       "        0        14       14          0             0             0\n",
       "        0        15       15          0             0             0\n",
       "        0        16       16          0             0             0\n",
       "        0        17       17          0             0             0\n",
       "        0        18       18          0             0             0\n",
       "        0        19       19          0             0             0\n",
       "        0        20       20          0             0             0\n",
       "        0        21       21          0             0             0\n",
       "        0        22       22          0             0             0\n",
       "        0        23       23          0             0             0\n",
       "        0        24       24          0             0             0\n",
       "        0        25       25          0             0             0\n",
       "        0        26       26          0             0             0\n",
       "        0        27       27          0             0             0\n",
       "        0        28       28          0             0             0\n",
       "        0        29       29          0             0             0\n",
       "        0        30       30          0             0             0\n",
       "        0        31       31          0             0             0\n",
       "        0        32       32          0             0             0\n",
       "        0        33       33          0             0             0\n",
       "        0        34       34          0             0             0\n",
       "        0        35       35          0             0             0\n",
       "        0        36       36          0             0             0\n",
       "        0        37       37          0             0             0\n",
       "        0        38       38          0             0             0\n",
       "        0        39       39          0             0             0\n",
       "        0        40       40          0             0             0\n",
       "        0        41       41          0             0             0\n",
       "        0        42       42          0             0             0\n",
       "        0        43       43          0             0             0\n",
       "        0        44       44          0             0             0\n",
       "        0        45       45          0             0             0\n",
       "        0        46       46          0             0             0\n",
       "        0        47       47          0             0             0\n",
       "        0        48       48          0             0             0\n",
       "        0        49       49          0             0             0\n",
       "        0        50       50          0             0             0\n",
       "        0        51       51          0             0             0\n",
       "        0        52       52          0             0             0\n",
       "        0        53       53          0             0             0\n",
       "        0        54       54          0             0             0\n",
       "        0        55       55          0             0             0\n",
       "        0        56       56          0             0             0\n",
       "        0        57       57          0             0             0\n",
       "        0        58       58          0             0             0\n",
       "        0        59       59          0             0             0\n",
       "        0        60       60          0             0             0\n",
       "        0        61       61          0             0             0\n",
       "        0        62       62          0             0             0\n",
       "        0        63       63          0             0             0\n",
       "        0        64       64          0             0             0\n",
       "        0        65       65          0             0             0\n",
       "        0        66       66          0             0             0\n",
       "        0        67       67          0             0             0\n",
       "        0        68       68          0             0             0\n",
       "        0        69       69          0             0             0\n",
       "        0        70       70          0             0             0\n",
       "        0        71       71          0             0             0\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.descriptor[0].samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fed1d450-b31f-4d57-82ab-a39b6dd1a754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labels(\n",
       "    structure  center  neighbor  ik\n",
       "        0        0        1      0\n",
       "        0        0        2      0\n",
       "        0        0        3      0\n",
       "        0        0        9      0\n",
       "        0        0        10     0\n",
       "        0        0        11     0\n",
       "        0        0        12     0\n",
       "        0        0        13     0\n",
       "        0        0        14     0\n",
       "        0        0        15     0\n",
       "        0        0        23     0\n",
       "        0        0        25     0\n",
       "        0        0        27     0\n",
       "        0        0        60     0\n",
       "        0        0        61     0\n",
       "        0        0        69     0\n",
       "        0        0        70     0\n",
       "        0        0        71     0\n",
       "        0        1        2      0\n",
       "        0        1        3      0\n",
       "        0        1        4      0\n",
       "        0        1        10     0\n",
       "        0        1        11     0\n",
       "        0        1        12     0\n",
       "        0        1        13     0\n",
       "        0        1        14     0\n",
       "        0        1        15     0\n",
       "        0        1        16     0\n",
       "        0        1        48     0\n",
       "        0        1        58     0\n",
       "        0        1        60     0\n",
       "        0        1        61     0\n",
       "        0        1        62     0\n",
       "        0        1        70     0\n",
       "        0        1        71     0\n",
       "        0        2        3      0\n",
       "        0        2        4      0\n",
       "        0        2        5      0\n",
       "        0        2        11     0\n",
       "        0        2        13     0\n",
       "        0        2        14     0\n",
       "        0        2        15     0\n",
       "        0        2        16     0\n",
       "        0        2        17     0\n",
       "        0        2        27     0\n",
       "        0        2        29     0\n",
       "        0        2        60     0\n",
       "        0        2        61     0\n",
       "        0        2        62     0\n",
       "        0        2        63     0\n",
       "        0        2        71     0\n",
       "        0        3        4      0\n",
       "        0        3        5      0\n",
       "        0        3        14     0\n",
       "        0        3        15     0\n",
       "        0        3        16     0\n",
       "        0        3        17     0\n",
       "        0        3        48     0\n",
       "        0        3        50     0\n",
       "        0        3        60     0\n",
       "        0        3        61     0\n",
       "        0        3        62     0\n",
       "        0        3        63     0\n",
       "        0        3        64     0\n",
       "        0        4        5      0\n",
       "        0        4        15     0\n",
       "        0        4        16     0\n",
       "        0        4        17     0\n",
       "        0        4        29     0\n",
       "        0        4        61     0\n",
       "        0        4        62     0\n",
       "        0        4        63     0\n",
       "        0        4        64     0\n",
       "        0        4        65     0\n",
       "        0        5        16     0\n",
       "        0        5        17     0\n",
       "        0        5        50     0\n",
       "        0        5        52     0\n",
       "        0        5        62     0\n",
       "        0        5        63     0\n",
       "        0        5        64     0\n",
       "        0        5        65     0\n",
       "        0        6        7      0\n",
       "        0        6        8      0\n",
       "        0        6        9      0\n",
       "        0        6        18     0\n",
       "        0        6        19     0\n",
       "        0        6        20     0\n",
       "        0        6        21     0\n",
       "        0        6        31     0\n",
       "        0        6        33     0\n",
       "        0        6        66     0\n",
       "        0        6        67     0\n",
       "        0        7        8      0\n",
       "        0        7        9      0\n",
       "        0        7        10     0\n",
       "        0        7        18     0\n",
       "        0        7        19     0\n",
       "        0        7        20     0\n",
       "        0        7        21     0\n",
       "        0        7        22     0\n",
       "        0        7        54     0\n",
       "        0        7        66     0\n",
       "        0        7        67     0\n",
       "        0        7        68     0\n",
       "        0        8        9      0\n",
       "        0        8        10     0\n",
       "        0        8        11     0\n",
       "        0        8        19     0\n",
       "        0        8        20     0\n",
       "        0        8        21     0\n",
       "        0        8        22     0\n",
       "        0        8        23     0\n",
       "        0        8        33     0\n",
       "        0        8        35     0\n",
       "        0        8        66     0\n",
       "        0        8        67     0\n",
       "        0        8        68     0\n",
       "        0        8        69     0\n",
       "        0        9        10     0\n",
       "        0        9        11     0\n",
       "        0        9        12     0\n",
       "        0        9        20     0\n",
       "        0        9        21     0\n",
       "        0        9        22     0\n",
       "        0        9        23     0\n",
       "        0        9        54     0\n",
       "        0        9        56     0\n",
       "        0        9        66     0\n",
       "        0        9        67     0\n",
       "        0        9        68     0\n",
       "        0        9        69     0\n",
       "        0        9        70     0\n",
       "        0        10       11     0\n",
       "        0        10       12     0\n",
       "        0        10       13     0\n",
       "        0        10       21     0\n",
       "        0        10       22     0\n",
       "        0        10       23     0\n",
       "        0        10       25     0\n",
       "        0        10       35     0\n",
       "        0        10       67     0\n",
       "        0        10       68     0\n",
       "        0        10       69     0\n",
       "        0        10       70     0\n",
       "        0        10       71     0\n",
       "        0        11       12     0\n",
       "        0        11       13     0\n",
       "        0        11       14     0\n",
       "        0        11       22     0\n",
       "        0        11       23     0\n",
       "        0        11       56     0\n",
       "        0        11       58     0\n",
       "        0        11       60     0\n",
       "        0        11       68     0\n",
       "        0        11       69     0\n",
       "        0        11       70     0\n",
       "        0        11       71     0\n",
       "        0        12       13     0\n",
       "        0        12       14     0\n",
       "        0        12       15     0\n",
       "        0        12       21     0\n",
       "        0        12       22     0\n",
       "        0        12       23     0\n",
       "        0        12       24     0\n",
       "        0        12       25     0\n",
       "        0        12       26     0\n",
       "        0        12       27     0\n",
       "        0        12       35     0\n",
       "        0        13       14     0\n",
       "        0        13       15     0\n",
       "        0        13       16     0\n",
       "        0        13       22     0\n",
       "        0        13       23     0\n",
       "        0        13       24     0\n",
       "        0        13       25     0\n",
       "        0        13       26     0\n",
       "        0        13       27     0\n",
       "        0        13       28     0\n",
       "        0        13       60     0\n",
       "        0        13       70     0\n",
       "        0        14       15     0\n",
       "        0        14       16     0\n",
       "        0        14       17     0\n",
       "        0        14       23     0\n",
       "        0        14       25     0\n",
       "        0        14       26     0\n",
       "        0        14       27     0\n",
       "        0        14       28     0\n",
       "        0        14       29     0\n",
       "        0        15       16     0\n",
       "        0        15       17     0\n",
       "        0        15       26     0\n",
       "        0        15       27     0\n",
       "        0        15       28     0\n",
       "        0        15       29     0\n",
       "        0        15       60     0\n",
       "        0        15       62     0\n",
       "        0        16       17     0\n",
       "        0        16       27     0\n",
       "        0        16       28     0\n",
       "        0        16       29     0\n",
       "        0        17       28     0\n",
       "        0        17       29     0\n",
       "        0        17       62     0\n",
       "        0        17       64     0\n",
       "        0        18       19     0\n",
       "        0        18       20     0\n",
       "        0        18       21     0\n",
       "        0        18       30     0\n",
       "        0        18       31     0\n",
       "        0        18       32     0\n",
       "        0        18       33     0\n",
       "        0        19       20     0\n",
       "        0        19       21     0\n",
       "        0        19       22     0\n",
       "        0        19       30     0\n",
       "        0        19       31     0\n",
       "        0        19       32     0\n",
       "        0        19       33     0\n",
       "        0        19       34     0\n",
       "        0        19       66     0\n",
       "        0        20       21     0\n",
       "        0        20       22     0\n",
       "        0        20       23     0\n",
       "        0        20       31     0\n",
       "        0        20       32     0\n",
       "        0        20       33     0\n",
       "        0        20       34     0\n",
       "        0        20       35     0\n",
       "        0        21       22     0\n",
       "        0        21       23     0\n",
       "        0        21       24     0\n",
       "        0        21       32     0\n",
       "        0        21       33     0\n",
       "        0        21       34     0\n",
       "        0        21       35     0\n",
       "        0        21       66     0\n",
       "        0        21       68     0\n",
       "        0        22       23     0\n",
       "        0        22       24     0\n",
       "        0        22       25     0\n",
       "        0        22       33     0\n",
       "        0        22       34     0\n",
       "        0        22       35     0\n",
       "        0        23       24     0\n",
       "        0        23       25     0\n",
       "        0        23       26     0\n",
       "        0        23       34     0\n",
       "        0        23       35     0\n",
       "        0        23       68     0\n",
       "        0        23       70     0\n",
       "        0        24       25     0\n",
       "        0        24       26     0\n",
       "        0        24       27     0\n",
       "        0        24       33     0\n",
       "        0        24       34     0\n",
       "        0        24       35     0\n",
       "        0        25       26     0\n",
       "        0        25       27     0\n",
       "        0        25       28     0\n",
       "        0        25       34     0\n",
       "        0        25       35     0\n",
       "        0        26       27     0\n",
       "        0        26       28     0\n",
       "        0        26       29     0\n",
       "        0        26       35     0\n",
       "        0        27       28     0\n",
       "        0        27       29     0\n",
       "        0        28       29     0\n",
       "        0        30       31     0\n",
       "        0        30       32     0\n",
       "        0        30       33     0\n",
       "        0        31       32     0\n",
       "        0        31       33     0\n",
       "        0        31       34     0\n",
       "        0        32       33     0\n",
       "        0        32       34     0\n",
       "        0        32       35     0\n",
       "        0        33       34     0\n",
       "        0        33       35     0\n",
       "        0        34       35     0\n",
       "        0        36       37     0\n",
       "        0        36       38     0\n",
       "        0        36       39     0\n",
       "        0        36       45     0\n",
       "        0        36       46     0\n",
       "        0        36       47     0\n",
       "        0        36       48     0\n",
       "        0        36       49     0\n",
       "        0        36       50     0\n",
       "        0        36       51     0\n",
       "        0        36       59     0\n",
       "        0        36       61     0\n",
       "        0        36       63     0\n",
       "        0        37       38     0\n",
       "        0        37       39     0\n",
       "        0        37       40     0\n",
       "        0        37       46     0\n",
       "        0        37       47     0\n",
       "        0        37       48     0\n",
       "        0        37       49     0\n",
       "        0        37       50     0\n",
       "        0        37       51     0\n",
       "        0        37       52     0\n",
       "        0        38       39     0\n",
       "        0        38       40     0\n",
       "        0        38       41     0\n",
       "        0        38       47     0\n",
       "        0        38       49     0\n",
       "        0        38       50     0\n",
       "        0        38       51     0\n",
       "        0        38       52     0\n",
       "        0        38       53     0\n",
       "        0        38       63     0\n",
       "        0        38       65     0\n",
       "        0        39       40     0\n",
       "        0        39       41     0\n",
       "        0        39       50     0\n",
       "        0        39       51     0\n",
       "        0        39       52     0\n",
       "        0        39       53     0\n",
       "        0        40       41     0\n",
       "        0        40       51     0\n",
       "        0        40       52     0\n",
       "        0        40       53     0\n",
       "        0        40       65     0\n",
       "        0        41       52     0\n",
       "        0        41       53     0\n",
       "        0        42       43     0\n",
       "        0        42       44     0\n",
       "        0        42       45     0\n",
       "        0        42       54     0\n",
       "        0        42       55     0\n",
       "        0        42       56     0\n",
       "        0        42       57     0\n",
       "        0        42       67     0\n",
       "        0        42       69     0\n",
       "        0        43       44     0\n",
       "        0        43       45     0\n",
       "        0        43       46     0\n",
       "        0        43       54     0\n",
       "        0        43       55     0\n",
       "        0        43       56     0\n",
       "        0        43       57     0\n",
       "        0        43       58     0\n",
       "        0        44       45     0\n",
       "        0        44       46     0\n",
       "        0        44       47     0\n",
       "        0        44       55     0\n",
       "        0        44       56     0\n",
       "        0        44       57     0\n",
       "        0        44       58     0\n",
       "        0        44       59     0\n",
       "        0        44       69     0\n",
       "        0        44       71     0\n",
       "        0        45       46     0\n",
       "        0        45       47     0\n",
       "        0        45       48     0\n",
       "        0        45       56     0\n",
       "        0        45       57     0\n",
       "        0        45       58     0\n",
       "        0        45       59     0\n",
       "        0        46       47     0\n",
       "        0        46       48     0\n",
       "        0        46       49     0\n",
       "        0        46       57     0\n",
       "        0        46       58     0\n",
       "        0        46       59     0\n",
       "        0        46       61     0\n",
       "        0        46       71     0\n",
       "        0        47       48     0\n",
       "        0        47       49     0\n",
       "        0        47       50     0\n",
       "        0        47       58     0\n",
       "        0        47       59     0\n",
       "        0        48       49     0\n",
       "        0        48       50     0\n",
       "        0        48       51     0\n",
       "        0        48       57     0\n",
       "        0        48       58     0\n",
       "        0        48       59     0\n",
       "        0        48       60     0\n",
       "        0        48       61     0\n",
       "        0        48       62     0\n",
       "        0        48       63     0\n",
       "        0        48       71     0\n",
       "        0        49       50     0\n",
       "        0        49       51     0\n",
       "        0        49       52     0\n",
       "        0        49       58     0\n",
       "        0        49       59     0\n",
       "        0        49       60     0\n",
       "        0        49       61     0\n",
       "        0        49       62     0\n",
       "        0        49       63     0\n",
       "        0        49       64     0\n",
       "        0        50       51     0\n",
       "        0        50       52     0\n",
       "        0        50       53     0\n",
       "        0        50       59     0\n",
       "        0        50       61     0\n",
       "        0        50       62     0\n",
       "        0        50       63     0\n",
       "        0        50       64     0\n",
       "        0        50       65     0\n",
       "        0        51       52     0\n",
       "        0        51       53     0\n",
       "        0        51       62     0\n",
       "        0        51       63     0\n",
       "        0        51       64     0\n",
       "        0        51       65     0\n",
       "        0        52       53     0\n",
       "        0        52       63     0\n",
       "        0        52       64     0\n",
       "        0        52       65     0\n",
       "        0        53       64     0\n",
       "        0        53       65     0\n",
       "        0        54       55     0\n",
       "        0        54       56     0\n",
       "        0        54       57     0\n",
       "        0        54       66     0\n",
       "        0        54       67     0\n",
       "        0        54       68     0\n",
       "        0        54       69     0\n",
       "        0        55       56     0\n",
       "        0        55       57     0\n",
       "        0        55       58     0\n",
       "        0        55       66     0\n",
       "        0        55       67     0\n",
       "        0        55       68     0\n",
       "        0        55       69     0\n",
       "        0        55       70     0\n",
       "        0        56       57     0\n",
       "        0        56       58     0\n",
       "        0        56       59     0\n",
       "        0        56       67     0\n",
       "        0        56       68     0\n",
       "        0        56       69     0\n",
       "        0        56       70     0\n",
       "        0        56       71     0\n",
       "        0        57       58     0\n",
       "        0        57       59     0\n",
       "        0        57       60     0\n",
       "        0        57       68     0\n",
       "        0        57       69     0\n",
       "        0        57       70     0\n",
       "        0        57       71     0\n",
       "        0        58       59     0\n",
       "        0        58       60     0\n",
       "        0        58       61     0\n",
       "        0        58       69     0\n",
       "        0        58       70     0\n",
       "        0        58       71     0\n",
       "        0        59       60     0\n",
       "        0        59       61     0\n",
       "        0        59       62     0\n",
       "        0        59       70     0\n",
       "        0        59       71     0\n",
       "        0        60       61     0\n",
       "        0        60       62     0\n",
       "        0        60       63     0\n",
       "        0        60       69     0\n",
       "        0        60       70     0\n",
       "        0        60       71     0\n",
       "        0        61       62     0\n",
       "        0        61       63     0\n",
       "        0        61       64     0\n",
       "        0        61       70     0\n",
       "        0        61       71     0\n",
       "        0        62       63     0\n",
       "        0        62       64     0\n",
       "        0        62       65     0\n",
       "        0        62       71     0\n",
       "        0        63       64     0\n",
       "        0        63       65     0\n",
       "        0        64       65     0\n",
       "        0        66       67     0\n",
       "        0        66       68     0\n",
       "        0        66       69     0\n",
       "        0        67       68     0\n",
       "        0        67       69     0\n",
       "        0        67       70     0\n",
       "        0        68       69     0\n",
       "        0        68       70     0\n",
       "        0        68       71     0\n",
       "        0        69       70     0\n",
       "        0        69       71     0\n",
       "        0        70       71     0\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.target[0].samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de51fa-a7f9-46d0-848a-2c53a8766968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e5cae-6516-475f-be74-138cf1f6835a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da116ef9-b9d9-4705-8355-868a24412b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e7f3e-2704-472c-bd15-44c732911fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a214f25-4a97-4475-9e81-5a4664959c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92e9d6fd-8c80-450c-a83f-2cce15f02697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss real  593.0036468974\n",
      "Epoch       1, train loss real   49.2449186928\n",
      "Epoch       2, train loss real    2.0704940922\n",
      "Epoch       3, train loss real    0.6410718528\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     pred_kspace \u001b[38;5;241m=\u001b[39m TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 54\u001b[0m     target_kspace \u001b[38;5;241m=\u001b[39m \u001b[43mTMap_bloch_sums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkpts_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensormap\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     loss \u001b[38;5;241m=\u001b[39m L2_loss(pred_kspace, target_kspace, norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/utils/pbc_utils.py:954\u001b[0m, in \u001b[0;36mTMap_bloch_sums\u001b[0;34m(target_blocks, phase, indices, kpts_idx, return_tensormap)\u001b[0m\n\u001b[1;32m    951\u001b[0m pshape \u001b[38;5;241m=\u001b[39m phase[kl][ifr, i, j]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# equivalent to torch.einsum('Tmnv,kT->kmnv', values.to(phase[kl][ifr, i, j]), phase[kl][ifr, i, j]), but faster\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m contraction \u001b[38;5;241m=\u001b[39m (phase[kl][ifr, i, j]\u001b[38;5;129m@values\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape(pshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39mvshape[\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m*\u001b[39mfactor\n\u001b[1;32m    956\u001b[0m \u001b[38;5;66;03m# if bt == 1 or bt == 2 or (bt == -1 and i != j):\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (bt \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "new_sched = True\n",
    "\n",
    "train_kspace = False\n",
    "\n",
    "loss_real = []\n",
    "\n",
    "nepoch = 100\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    if epoch >= 0:\n",
    "        train_kspace = True\n",
    "\n",
    "    LOSS = 0\n",
    "    for ib, batch in enumerate(dataloader):\n",
    "\n",
    "        model.train(True)\n",
    "\n",
    "        for ik, key in enumerate(model.model):\n",
    "            optimizers[ik].zero_grad()\n",
    "        \n",
    "        pred = model.predict_batch(batch.descriptor, batch.target)\n",
    "            \n",
    "        if not train_kspace:\n",
    "    \n",
    "            # Compute the loss for each block\n",
    "            all_losses, epoch_loss = L2_loss(pred, batch.target, loss_per_block = True)\n",
    "    \n",
    "            # Total loss\n",
    "            epoch_loss = epoch_loss.item()\n",
    "            LOSS += epoch_loss\n",
    "            # Append the values of the loss to a list\n",
    "            # loss_k.append(L2_loss(pred_kspace, k_target_blocks, norm = 2).item())\n",
    "            loss_real.append(epoch_loss)\n",
    "    \n",
    "            # Loop through submodels and backpropagate\n",
    "            for ik, (loss, key) in enumerate(zip(all_losses, model.model)):\n",
    "                loss.backward(retain_graph = False)\n",
    "                torch.nn.utils.clip_grad_norm_(model.model[key].parameters(), 1)\n",
    "                optimizers[ik].step()\n",
    "                schedulers[ik].step(loss)\n",
    "                \n",
    "                if key not in losses:\n",
    "                    losses[key] = [loss.item()]\n",
    "                    learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "                else:\n",
    "                    losses[key].append(loss.item())\n",
    "                    learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "                    \n",
    "        else:\n",
    "\n",
    "            pred_kspace = TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap = True)\n",
    "            target_kspace = TMap_bloch_sums(batch.target, phase, indices, kpts_idx, return_tensormap = True)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = L2_loss(pred_kspace, target_kspace, norm = 2)\n",
    "    \n",
    "            # Total loss \n",
    "            epoch_loss = loss.item()\n",
    "            LOSS += epoch_loss\n",
    "            \n",
    "            # Append the values of the loss to a list\n",
    "            all_losses, epoch_loss_real = L2_loss(pred, batch.target, loss_per_block=True)\n",
    "            # loss_real.append(epoch_loss_real.item())\n",
    "            # loss_k.append(epoch_loss)\n",
    "    \n",
    "                   \n",
    "            loss.backward(retain_graph = True)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    \n",
    "            # for ik, (loss_, key) in enumerate(zip(all_losses, model.model)):\n",
    "            for ik, key in enumerate(model.model):\n",
    "                optimizers[ik].step()\n",
    "                schedulers[ik].step(epoch_loss/len(model.model))\n",
    "                # if key not in losses:\n",
    "                #     # losses[key] = [loss_.item()]\n",
    "                #     learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "                # else:\n",
    "                #     # losses[key].append(loss_.item())\n",
    "                #     learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "\n",
    "        # print(ib, end = ' ')\n",
    "    if epoch >= 0: #% 10 == 0:\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss on all blocks {epoch_loss:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss real {loss_real[-1]:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        print(f\"Epoch {epoch:>7d}, train loss real {LOSS:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24f39f8b-0cb8-4c9f-8318-bbd23666dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_kspace = TMap_bloch_sums(target_coupled_blocks, phase, indices, kpts_idx, return_tensormap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad5b5e15-98f7-4f1d-9aec-2bd4bed5d3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss real 1276.7376358469\n",
      "Epoch       1, train loss real 1022.9805296847\n",
      "Epoch       2, train loss real  828.5650614440\n",
      "Epoch       3, train loss real  679.7027881757\n",
      "Epoch       4, train loss real  559.7761959069\n",
      "Epoch       5, train loss real  460.1125304624\n",
      "Epoch       6, train loss real  375.6945812240\n",
      "Epoch       7, train loss real  303.1982950560\n",
      "Epoch       8, train loss real  240.1988690185\n",
      "Epoch       9, train loss real  187.3344156086\n",
      "Epoch      10, train loss real  144.5820797534\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 69\u001b[0m\n\u001b[1;32m     64\u001b[0m all_losses, epoch_loss_real \u001b[38;5;241m=\u001b[39m L2_loss(pred, target_coupled_blocks, loss_per_block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# loss_real.append(epoch_loss_real.item())\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# loss_k.append(epoch_loss)\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ik, (loss_, key) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(all_losses, model\u001b[38;5;241m.\u001b[39mmodel)):\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%timeit -r 1 -n 1\n",
    "\n",
    "new_sched = True\n",
    "\n",
    "# batch_size = 360\n",
    "# dataloader = DataLoader(ml_data, batch_size = batch_size, shuffle = True, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))\n",
    "\n",
    "train_kspace = False\n",
    "\n",
    "# target_kspace = \n",
    "loss_real = []\n",
    "\n",
    "nepoch = 100\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    if epoch >= 0:\n",
    "        train_kspace = True\n",
    "\n",
    "    model.train(True)\n",
    "\n",
    "    for ik, key in enumerate(model.model):\n",
    "        optimizers[ik].zero_grad()\n",
    "    \n",
    "    pred = model()\n",
    "        \n",
    "    if not train_kspace:\n",
    "\n",
    "        # Compute the loss for each block\n",
    "        all_losses, epoch_loss = L2_loss(pred, target_coupled_blocks, loss_per_block = True)\n",
    "\n",
    "        # Total loss\n",
    "        epoch_loss = epoch_loss.item()\n",
    "        LOSS = epoch_loss\n",
    "        # Append the values of the loss to a list\n",
    "        # loss_k.append(L2_loss(pred_kspace, k_target_blocks, norm = 2).item())\n",
    "        loss_real.append(epoch_loss)\n",
    "\n",
    "        # Loop through submodels and backpropagate\n",
    "        for ik, (loss, key) in enumerate(zip(all_losses, model.model)):\n",
    "            loss.backward(retain_graph = False)\n",
    "            torch.nn.utils.clip_grad_norm_(model.model[key].parameters(), 1)\n",
    "            optimizers[ik].step()\n",
    "            schedulers[ik].step(loss)\n",
    "            \n",
    "            if key not in losses:\n",
    "                losses[key] = [loss.item()]\n",
    "                learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "            else:\n",
    "                losses[key].append(loss.item())\n",
    "                learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "                \n",
    "    else:\n",
    "\n",
    "        pred_kspace = TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap = True)\n",
    "        # target_kspace = TMap_bloch_sums(batch.target, phase, indices, kpts_idx, return_tensormap = True)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = L2_loss(pred_kspace, target_kspace, norm = 2)\n",
    "\n",
    "        # Total loss \n",
    "        epoch_loss = loss.item()\n",
    "        LOSS = epoch_loss        \n",
    "        # Append the values of the loss to a list\n",
    "        all_losses, epoch_loss_real = L2_loss(pred, target_coupled_blocks, loss_per_block=True)\n",
    "        # loss_real.append(epoch_loss_real.item())\n",
    "        # loss_k.append(epoch_loss)\n",
    "\n",
    "               \n",
    "        loss.backward(retain_graph = True)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        for ik, (loss_, key) in enumerate(zip(all_losses, model.model)):\n",
    "            optimizers[ik].step()\n",
    "            schedulers[ik].step(epoch_loss/len(model.model))\n",
    "            if key not in losses:\n",
    "                losses[key] = [loss_.item()]\n",
    "                learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "            else:\n",
    "                losses[key].append(loss_.item())\n",
    "                learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "\n",
    "    # print(ib, end = ' ')\n",
    "    if epoch >= 0: #% 10 == 0:\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss on all blocks {epoch_loss:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss real {loss_real[-1]:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        print(f\"Epoch {epoch:>7d}, train loss real {LOSS:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cece61d-bd13-4771-8f9e-c05b3418f93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
