{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d8f12c-62ee-4d46-b766-c3b49d817fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635a399b-c69b-4438-bac6-4ba0712d0d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/pyscf/dft/libxc.py:771: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, corresponding to the original definition by Stephens et al. (issue 1480) and the same as the B3LYP functional in Gaussian. To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read\n",
    "from ase.visualize import view\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import torch \n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import rascaline\n",
    "\n",
    "import metatensor \n",
    "from metatensor import Labels, TensorBlock, TensorMap\n",
    "from metatensor import load, sort\n",
    "\n",
    "from mlelec.data.dataset import PySCFPeriodicDataset, split_by_Aij_mts\n",
    "from mlelec.utils.twocenter_utils import _to_coupled_basis\n",
    "from mlelec.utils.pbc_utils import matrix_to_blocks, kmatrix_to_blocks, TMap_bloch_sums, precompute_phase, kblocks_to_matrix, kmatrix_to_blocks, blocks_to_matrix, matrix_to_blocks\n",
    "from mlelec.utils.plot_utils import print_matrix, matrix_norm, block_matrix_norm, plot_block_errors\n",
    "from mlelec.features.acdc import pair_features, single_center_features, twocenter_features_periodic_NH, twocenter_hermitian_features\n",
    "from mlelec.models.linear import LinearModelPeriodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a54e69-a6cf-413a-a1fa-bec6edae654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(dataset, device =\"cpu\", cutoff = None, target='fock', all_pairs = False, sort_orbs = True):\n",
    "    \n",
    "    blocks = matrix_to_blocks(dataset, device = device, cutoff = cutoff, all_pairs = all_pairs, target = target, sort_orbs = sort_orbs)\n",
    "    coupled_blocks = _to_coupled_basis(blocks, skip_symmetry = True, device = device, translations = True)\n",
    "\n",
    "    blocks = blocks.to(arrays='numpy')\n",
    "    blocks = sort(blocks)\n",
    "    blocks = blocks.to(arrays='torch')\n",
    "    \n",
    "    coupled_blocks = coupled_blocks.to(arrays='numpy')\n",
    "    coupled_blocks = sort(coupled_blocks)\n",
    "    coupled_blocks = coupled_blocks.to(arrays='torch')\n",
    "    \n",
    "    return blocks, coupled_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc04a6ae-03c9-489e-b7e3-347cc86f8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(dataset, all_pairs=False):\n",
    "\n",
    "    rhoij = pair_features(dataset.structures, hypers_atom, hypers_pair, order_nu = 1, all_pairs = all_pairs, both_centers = both_centers,\n",
    "                          kmesh = dataset.kmesh, device = device, lcut = LCUT, return_rho0ij = return_rho0ij)  \n",
    "    \n",
    "    if both_centers and not return_rho0ij:\n",
    "        NU = 3\n",
    "    else:\n",
    "        NU = 2\n",
    "    rhonui = single_center_features(dataset.structures, hypers_atom, order_nu = NU, lcut = LCUT, device = device,\n",
    "                                    feature_names = rhoij.property_names)\n",
    "    \n",
    "    hfeat = twocenter_features_periodic_NH(single_center = rhonui, pair = rhoij, all_pairs = all_pairs)\n",
    "\n",
    "    return hfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0131f6a-72a2-445e-9384-0ba95406c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "orbitals = {\n",
    "    'sto-3g': {5: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               6: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               7: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]]}, \n",
    "    \n",
    "    'def2svp': {6: [[1,0,0],[2,0,0],[3,0,0],[2,1,1], [2,1,-1],[2,1,0], [3,1,1], [3,1,-1],[3,1,0], [3,2,-2], [3,2,-1],[3,2,0], [3,2,1],[3,2,2]]},\n",
    "    'benzene': {6: [[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], 1:[[1,0,0]]},\n",
    "    'gthszvmolopt': {\n",
    "        6: [[2, 0, 0], [2, 1, -1], [2, 1, 0], [2, 1, 1]],\n",
    "        \n",
    "        16: [[3,0,0], \n",
    "             [3,1,-1], [3,1,0], [3,1,1]],\n",
    "\n",
    "        42: [[4,0,0], \n",
    "             [5,0,0], \n",
    "             [4,1,-1], [4,1,0], [4,1,1], \n",
    "             [4, 2, -2], [4, 2, -1], [4, 2, 0], [4, 2, 1], [4, 2, 2]]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61756a8d-5fbe-4fd2-9521-4a4604762d6d",
   "metadata": {},
   "source": [
    "# QC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb8d851-377b-4bd5-a632-397f666731e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = './'\n",
    "START = 0 \n",
    "STOP = 10\n",
    "ORBS = 'sto-3g'\n",
    "root = f'{workdir}/examples/data/periodic/deepH_graphene/wrap/'\n",
    "data_dir = root\n",
    "frames = read(f'{data_dir}/wrapped_deepH_graphene.xyz', slice(START, STOP))\n",
    "rfock = [np.load(f\"{data_dir}/realfock_{i}.npy\", allow_pickle = True).item() for i in range(START, STOP)]\n",
    "rover = [np.load(f\"{data_dir}/realoverlap_{i}.npy\", allow_pickle = True).item() for i in range(START, STOP)]\n",
    "kmesh = [1,1,1]\n",
    "dataset = PySCFPeriodicDataset(frames = frames, \n",
    "                               kmesh = kmesh, \n",
    "                               dimension = 2,\n",
    "                               fock_realspace = rfock, \n",
    "                               overlap_realspace = rover, \n",
    "                               device = device, \n",
    "                               orbs = orbitals[ORBS], \n",
    "                               orbs_name = 'sto-3g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3873bac-ec05-4a55-aeb6-abc711ebe1b2",
   "metadata": {},
   "source": [
    "# Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ac2b01-c2a3-47e2-9296-323a5eddd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567ec238-ff88-4497-930f-fcb0551861ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_blocks, target_coupled_blocks = get_targets(dataset, cutoff = cutoff, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "bc35a4fc-fb6e-4fd7-9376-6e83d9047b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_target_blocks = kmatrix_to_blocks(dataset, cutoff = cutoff)\n",
    "k_target_coupled_blocks = _to_coupled_basis(kmatrix_to_blocks(dataset, cutoff = cutoff), skip_symmetry = True, device = device, translations= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa78cad-0988-4be9-812a-2209fdc5a9bf",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db41bfa2-936d-4d9f-8f3c-22580d7959a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_radial  = 6\n",
    "max_angular = 4\n",
    "atomic_gaussian_width = 0.3\n",
    "\n",
    "hypers_pair = {'cutoff': cutoff,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': atomic_gaussian_width,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}}}\n",
    "\n",
    "\n",
    "hypers_atom = {'cutoff': 4,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': 0.3,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}}}\n",
    "\n",
    "\n",
    "return_rho0ij = False\n",
    "both_centers = False\n",
    "LCUT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e89ccaa-95c0-44d2-938c-3189761c58c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu pair features\n",
      "cpu single center features\n",
      "cpu single center features\n"
     ]
    }
   ],
   "source": [
    "features = compute_features(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2566f8d-43e1-4404-bf31-e80c1e2e7e57",
   "metadata": {},
   "source": [
    "# ML Dataset/Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a61b2ec7-8647-4c4c-a843-e38141d5b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metatensor.learn import Dataset, DataLoader, IndexedDataset\n",
    "from metatensor.learn.data import group as mts_group, group_and_join as group_and_join_mts\n",
    "import metatensor as mts\n",
    "from mlelec.data.dataset import split_by_Aij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06da6f3b-f22f-4e94-81d3-851162f746a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.78 s ± 1.01 s per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 10\n",
    "split_features_mts, split_target_mts = split_by_Aij_mts(target_coupled_blocks, features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a6ecf1e-63b8-403c-aa04-080b985e3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 ms ± 725 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 10\n",
    "split_features, split_target = split_by_Aij(target_coupled_blocks, features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b53aaef2-7c6b-420e-9938-ceb474a9c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_features, split_target = split_by_Aij(target_coupled_blocks, features = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b33a7a-f2b6-424b-a974-2e3b73184b63",
   "metadata": {},
   "source": [
    "## Using tensormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eda452c7-f3ce-4699-a801-435a51cb0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_features, split_target = split_by_Aij_mts(target_coupled_blocks, features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38900ab1-8d11-47b6-89d5-f184da2aae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = IndexedDataset(descriptor = list(split_features.values()), target = list(split_target.values()), sample_id = list(split_target.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "401203e3-4809-415a-a8d4-b89347c2e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "dataloader = DataLoader(ml_data, batch_size = batch_size, shuffle = True, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59794982-77a8-4ae4-b6cd-0176753165f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase, indices, kpts_idx = precompute_phase(target_coupled_blocks, dataset, cutoff = cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "37b9e355-1d5b-49f9-8c74-ac4a061b1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmap = []\n",
    "for batch in dataloader:\n",
    "    kmap.append(TMap_bloch_sums(batch.target, phase, indices, kpts_idx, return_tensormap = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b5ac5-c943-4a21-95e2-a652795b6923",
   "metadata": {},
   "source": [
    "## Metatensor native splitting by structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d5444484-9a93-4bde-9d7c-513f339ecb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_axis = \"samples\"\n",
    "split_by_dimension = \"structure\"\n",
    "\n",
    "grouped_labels = [mts.Labels(names = split_by_dimension, values = np.array([A])) for A in mts.unique_metadata(target_coupled_blocks, axis = split_by_axis, names = split_by_dimension)]\n",
    "split_target = mts.split(target_coupled_blocks, split_by_axis, grouped_labels)\n",
    "\n",
    "grouped_labels = [mts.Labels(names = split_by_dimension, values = np.array([A])) for A in mts.unique_metadata(features, axis = split_by_axis, names = split_by_dimension)]\n",
    "split_features = mts.split(features, split_by_axis, grouped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "ce082aef-50f9-4974-8501-d077de90419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = IndexedDataset(descriptor = split_features, target = split_target, sample_id = [g.values.tolist()[0][0] for g in grouped_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "8f0a705e-2e24-4967-bd30-de8e513948d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = DataLoader(ml_data, batch_size = batch_size, shuffle = True, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc808f17-7880-4242-9109-bad6638e7f81",
   "metadata": {},
   "source": [
    "Split kspace targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "a9e95cde-928d-4d13-935b-02013a2798de",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_axis = \"samples\"\n",
    "split_by_dimension = \"structure\"\n",
    "\n",
    "grouped_labels = [mts.Labels(names = split_by_dimension, values = np.array([A])) for A in mts.unique_metadata(k_target_coupled_blocks, axis = split_by_axis, names = split_by_dimension)]\n",
    "split_target_k = mts.split(k_target_coupled_blocks, split_by_axis, grouped_labels)\n",
    "\n",
    "grouped_labels = [mts.Labels(names = split_by_dimension, values = np.array([A])) for A in mts.unique_metadata(features, axis = split_by_axis, names = split_by_dimension)]\n",
    "split_features_k = mts.split(features, split_by_axis, grouped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ee77aac7-6adb-4ffa-8f66-5b4a02e372dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_k = IndexedDataset(descriptor = split_features_k, target = split_target_k, sample_id = [g.values.tolist()[0][0] for g in grouped_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a2e61-609c-42ab-8dda-2f11ebe50a59",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "292e1e88-8d09-4e78-972e-055688f42012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.metrics import L2_loss, L2_kspace_loss\n",
    "from mlelec.utils.twocenter_utils import _to_uncoupled_basis, map_targetkeys_to_featkeys\n",
    "from mlelec.utils.pbc_utils import precompute_phase, TMap_bloch_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "8ec08c42-7548-4d7d-b243-5ff30b668e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = LinearModelPeriodic(twocfeat = features, \n",
    "                            target_blocks = target_coupled_blocks,\n",
    "                            frames = dataset.structures, orbitals = dataset.basis, \n",
    "                            device = device,\n",
    "                            bias = True,\n",
    "                            nhidden = 128, \n",
    "                            nlayers = 1,\n",
    "                            activation = 'SiLU',\n",
    "                            apply_norm = True\n",
    "                           )\n",
    "model = model.double()\n",
    "\n",
    "nepoch = 1000\n",
    "\n",
    "optimizers = []\n",
    "schedulers = []\n",
    "for i, key in enumerate(model.model):\n",
    "    optimizers.append(torch.optim.AdamW(model.model[key].parameters(), lr = 1e-3, betas = (0.8, 0.9)))\n",
    "    schedulers.append(torch.optim.lr_scheduler.ReduceLROnPlateau(optimizers[-1], factor = 0.8, patience = 30, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "254ddf17-6837-4c81-823c-7ed539682a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase, indices, kpts_idx = precompute_phase(target_coupled_blocks, dataset, cutoff = cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "3c96517d-d601-4587-a1b4-7e39c536ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = DataLoader(ml_data, batch_size = batch_size, shuffle = False, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))\n",
    "dataloader_k = DataLoader(ml_data_k, batch_size = batch_size, shuffle = False, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a5e75a-0909-433c-8676-6f3b89d9df49",
   "metadata": {},
   "source": [
    "# TODO: change predict to handle kspace batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11ca466-6978-4acc-8ab0-378136947d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss 64142.5851537019\n",
      "Epoch       1, train loss 44121.4691543632\n",
      "Epoch       2, train loss 26671.7902572494\n",
      "Epoch       3, train loss 10914.9135279546\n",
      "Epoch       4, train loss 1462.5620362368\n",
      "Epoch       5, train loss  143.3554740790\n",
      "Epoch       6, train loss   29.6042534948\n",
      "Epoch       7, train loss   18.8608467736\n",
      "Epoch       8, train loss   14.8250137201\n",
      "Epoch       9, train loss   15.0656408635\n",
      "Epoch      10, train loss   13.3278190282\n",
      "Epoch      11, train loss   12.7500636296\n",
      "Epoch      12, train loss   15.5313876505\n",
      "Epoch      13, train loss    7.3950636156\n",
      "Epoch      14, train loss    6.7061585787\n",
      "Epoch      15, train loss    5.4418019128\n",
      "Epoch      16, train loss    5.2121799431\n",
      "Epoch      17, train loss    5.5940988561\n",
      "Epoch      18, train loss    4.4384333267\n",
      "Epoch      19, train loss    4.9458166384\n",
      "Epoch      20, train loss    3.4264787496\n",
      "Epoch      21, train loss    4.7324682023\n",
      "Epoch      22, train loss    4.7566075249\n",
      "Epoch      23, train loss    2.3245394599\n",
      "Epoch      24, train loss    2.4834343161\n",
      "Epoch      25, train loss    1.4068451077\n",
      "Epoch      26, train loss    1.5968268004\n",
      "Epoch      27, train loss    1.7491943134\n",
      "Epoch      28, train loss    0.9094970374\n",
      "Epoch      29, train loss    3.0847112098\n",
      "Epoch      30, train loss    5.3829764355\n",
      "Epoch      31, train loss    2.3814962857\n",
      "Epoch      32, train loss    2.1892355936\n",
      "Epoch      33, train loss    1.9527408131\n",
      "Epoch      34, train loss    3.5569257016\n",
      "Epoch      35, train loss    0.9631090148\n",
      "Epoch      36, train loss    0.7767774636\n",
      "Epoch      37, train loss    2.3529868090\n",
      "Epoch      38, train loss    1.2516032358\n",
      "Epoch      39, train loss    1.1086610020\n",
      "Epoch      40, train loss    0.8933770895\n",
      "Epoch      41, train loss    1.0855564262\n",
      "Epoch      42, train loss    1.2237328597\n",
      "Epoch      43, train loss    1.0807023342\n",
      "Epoch      44, train loss    1.1373208632\n",
      "Epoch      45, train loss    0.5158276237\n",
      "Epoch      46, train loss    0.7104754004\n",
      "Epoch      47, train loss    1.7817669637\n",
      "Epoch      48, train loss    0.4027157466\n",
      "Epoch      49, train loss    0.5012350581\n",
      "Epoch      50, train loss    1.0408264002\n",
      "Epoch      51, train loss    1.3528192904\n",
      "Epoch      52, train loss    0.5583970515\n",
      "Epoch      53, train loss    0.4188564548\n",
      "Epoch      54, train loss    0.4179308739\n",
      "Epoch      55, train loss    0.4723336174\n",
      "Epoch      56, train loss    0.4214496249\n",
      "Epoch      57, train loss    0.4247675169\n",
      "Epoch      58, train loss    0.3919714070\n",
      "Epoch      59, train loss    0.3585857131\n",
      "Epoch      60, train loss    0.3339124368\n",
      "Epoch      61, train loss    0.3021763425\n",
      "Epoch      62, train loss    0.2881447315\n",
      "Epoch      63, train loss    0.3069420009\n",
      "Epoch      64, train loss    0.2857262279\n",
      "Epoch      65, train loss    0.3805980741\n",
      "Epoch      66, train loss    0.4317286520\n",
      "Epoch      67, train loss    0.3127105100\n",
      "Epoch      68, train loss    0.3968098830\n",
      "Epoch      69, train loss    0.2824894067\n",
      "Epoch      70, train loss    0.4347160489\n",
      "Epoch      71, train loss    0.2391407882\n",
      "Epoch      72, train loss    0.2382209670\n",
      "Epoch      73, train loss    0.2058788085\n",
      "Epoch      74, train loss    0.2218884955\n",
      "Epoch      75, train loss    0.2061119421\n",
      "Epoch      76, train loss    0.2297272067\n",
      "Epoch      77, train loss    0.1850776018\n",
      "Epoch      78, train loss    0.1713654943\n",
      "Epoch      79, train loss    0.1697205628\n",
      "Epoch      80, train loss    0.1569055230\n",
      "Epoch      81, train loss    0.2309522196\n",
      "Epoch      82, train loss    0.1691377259\n",
      "Epoch      83, train loss    0.2380712716\n",
      "Epoch      84, train loss    0.1520826719\n",
      "Epoch      85, train loss    0.1758635272\n",
      "Epoch      86, train loss    0.2154293553\n",
      "Epoch      87, train loss    0.2089168286\n",
      "Epoch      88, train loss    0.1491055283\n",
      "Epoch      89, train loss    0.1695057403\n",
      "Epoch      90, train loss    0.1486190133\n",
      "Epoch      91, train loss    0.1730866364\n",
      "Epoch      92, train loss    0.1439260264\n",
      "Epoch      93, train loss    0.1931355231\n",
      "Epoch      94, train loss    0.1360104027\n",
      "Epoch      95, train loss    0.1383471276\n",
      "Epoch      96, train loss    0.1386248346\n"
     ]
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "train_kspace = False\n",
    "LOSS_LIST = []\n",
    "\n",
    "nepoch = 100\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    if epoch >= 50:\n",
    "        train_kspace = True\n",
    "\n",
    "    if not train_kspace:\n",
    "        # Train against real space targets\n",
    "        LOSS = 0\n",
    "        for ib, batch in enumerate(dataloader):\n",
    "            \n",
    "            model.train(True)\n",
    "            \n",
    "            for ik, key in enumerate(model.model):\n",
    "                optimizers[ik].zero_grad()\n",
    "            \n",
    "            pred = model.predict_batch(batch.descriptor, batch.target)\n",
    "            \n",
    "            # Compute the loss for each block\n",
    "            all_losses, epoch_loss = L2_loss(pred, batch.target, loss_per_block = True)\n",
    "    \n",
    "            # Total loss\n",
    "            epoch_loss = epoch_loss.item()\n",
    "            LOSS += epoch_loss\n",
    "    \n",
    "            # Loop through submodels and backpropagate\n",
    "            for ik, (loss, key) in enumerate(zip(all_losses, model.model)):\n",
    "                loss.backward(retain_graph = False)\n",
    "                torch.nn.utils.clip_grad_norm_(model.model[key].parameters(), 1)\n",
    "                optimizers[ik].step()\n",
    "                schedulers[ik].step(loss)\n",
    "\n",
    "    else:\n",
    "        # Train against k-space targets\n",
    "    \n",
    "        LOSS = 0\n",
    "        for ib, batch in enumerate(dataloader_k):\n",
    "\n",
    "            model.train(True)\n",
    "    \n",
    "            for ik, key in enumerate(model.model):\n",
    "                optimizers[ik].zero_grad()\n",
    "            \n",
    "            pred = model.predict_batch(batch.descriptor)\n",
    "            pred_kspace = TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap = True)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = L2_loss(pred_kspace, batch.target, norm = 2)\n",
    "    \n",
    "            # Total loss \n",
    "            epoch_loss = loss.item()\n",
    "            LOSS += epoch_loss\n",
    "                   \n",
    "            loss.backward(retain_graph = True)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    \n",
    "            for ik, key in enumerate(model.model):\n",
    "                optimizers[ik].step()\n",
    "                schedulers[ik].step(epoch_loss/len(model.model))\n",
    "\n",
    "    if epoch >= 0: #% 10 == 0:\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss on all blocks {epoch_loss:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss real {loss_real[-1]:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        LOSS_LIST.append(LOSS)\n",
    "        print(f\"Epoch {epoch:>7d}, train loss {LOSS:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de51fa-a7f9-46d0-848a-2c53a8766968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e5cae-6516-475f-be74-138cf1f6835a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da116ef9-b9d9-4705-8355-868a24412b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e7f3e-2704-472c-bd15-44c732911fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a214f25-4a97-4475-9e81-5a4664959c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92e9d6fd-8c80-450c-a83f-2cce15f02697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss real  593.0036468974\n",
      "Epoch       1, train loss real   49.2449186928\n",
      "Epoch       2, train loss real    2.0704940922\n",
      "Epoch       3, train loss real    0.6410718528\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     pred_kspace \u001b[38;5;241m=\u001b[39m TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 54\u001b[0m     target_kspace \u001b[38;5;241m=\u001b[39m \u001b[43mTMap_bloch_sums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkpts_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensormap\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     loss \u001b[38;5;241m=\u001b[39m L2_loss(pred_kspace, target_kspace, norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/utils/pbc_utils.py:954\u001b[0m, in \u001b[0;36mTMap_bloch_sums\u001b[0;34m(target_blocks, phase, indices, kpts_idx, return_tensormap)\u001b[0m\n\u001b[1;32m    951\u001b[0m pshape \u001b[38;5;241m=\u001b[39m phase[kl][ifr, i, j]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# equivalent to torch.einsum('Tmnv,kT->kmnv', values.to(phase[kl][ifr, i, j]), phase[kl][ifr, i, j]), but faster\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m contraction \u001b[38;5;241m=\u001b[39m (phase[kl][ifr, i, j]\u001b[38;5;129m@values\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape(pshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39mvshape[\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m*\u001b[39mfactor\n\u001b[1;32m    956\u001b[0m \u001b[38;5;66;03m# if bt == 1 or bt == 2 or (bt == -1 and i != j):\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (bt \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "new_sched = True\n",
    "\n",
    "train_kspace = False\n",
    "\n",
    "loss_real = []\n",
    "\n",
    "nepoch = 100\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    if epoch >= 0:\n",
    "        train_kspace = True\n",
    "\n",
    "    LOSS = 0\n",
    "    for ib, batch in enumerate(dataloader):\n",
    "\n",
    "        model.train(True)\n",
    "\n",
    "        for ik, key in enumerate(model.model):\n",
    "            optimizers[ik].zero_grad()\n",
    "        \n",
    "        pred = model.predict_batch(batch.descriptor, batch.target)\n",
    "            \n",
    "        if not train_kspace:\n",
    "    \n",
    "            # Compute the loss for each block\n",
    "            all_losses, epoch_loss = L2_loss(pred, batch.target, loss_per_block = True)\n",
    "    \n",
    "            # Total loss\n",
    "            epoch_loss = epoch_loss.item()\n",
    "            LOSS += epoch_loss\n",
    "            # Append the values of the loss to a list\n",
    "            # loss_k.append(L2_loss(pred_kspace, k_target_blocks, norm = 2).item())\n",
    "            loss_real.append(epoch_loss)\n",
    "    \n",
    "            # Loop through submodels and backpropagate\n",
    "            for ik, (loss, key) in enumerate(zip(all_losses, model.model)):\n",
    "                loss.backward(retain_graph = False)\n",
    "                torch.nn.utils.clip_grad_norm_(model.model[key].parameters(), 1)\n",
    "                optimizers[ik].step()\n",
    "                schedulers[ik].step(loss)\n",
    "                \n",
    "                if key not in losses:\n",
    "                    losses[key] = [loss.item()]\n",
    "                    learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "                else:\n",
    "                    losses[key].append(loss.item())\n",
    "                    learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "                    \n",
    "        else:\n",
    "\n",
    "            pred_kspace = TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap = True)\n",
    "            target_kspace = TMap_bloch_sums(batch.target, phase, indices, kpts_idx, return_tensormap = True)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = L2_loss(pred_kspace, target_kspace, norm = 2)\n",
    "    \n",
    "            # Total loss \n",
    "            epoch_loss = loss.item()\n",
    "            LOSS += epoch_loss\n",
    "            \n",
    "            # Append the values of the loss to a list\n",
    "            all_losses, epoch_loss_real = L2_loss(pred, batch.target, loss_per_block=True)\n",
    "            # loss_real.append(epoch_loss_real.item())\n",
    "            # loss_k.append(epoch_loss)\n",
    "    \n",
    "                   \n",
    "            loss.backward(retain_graph = True)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    \n",
    "            # for ik, (loss_, key) in enumerate(zip(all_losses, model.model)):\n",
    "            for ik, key in enumerate(model.model):\n",
    "                optimizers[ik].step()\n",
    "                schedulers[ik].step(epoch_loss/len(model.model))\n",
    "                # if key not in losses:\n",
    "                #     # losses[key] = [loss_.item()]\n",
    "                #     learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "                # else:\n",
    "                #     # losses[key].append(loss_.item())\n",
    "                #     learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "\n",
    "        # print(ib, end = ' ')\n",
    "    if epoch >= 0: #% 10 == 0:\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss on all blocks {epoch_loss:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss real {loss_real[-1]:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        print(f\"Epoch {epoch:>7d}, train loss real {LOSS:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24f39f8b-0cb8-4c9f-8318-bbd23666dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_kspace = TMap_bloch_sums(target_coupled_blocks, phase, indices, kpts_idx, return_tensormap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad5b5e15-98f7-4f1d-9aec-2bd4bed5d3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss real 1276.7376358469\n",
      "Epoch       1, train loss real 1022.9805296847\n",
      "Epoch       2, train loss real  828.5650614440\n",
      "Epoch       3, train loss real  679.7027881757\n",
      "Epoch       4, train loss real  559.7761959069\n",
      "Epoch       5, train loss real  460.1125304624\n",
      "Epoch       6, train loss real  375.6945812240\n",
      "Epoch       7, train loss real  303.1982950560\n",
      "Epoch       8, train loss real  240.1988690185\n",
      "Epoch       9, train loss real  187.3344156086\n",
      "Epoch      10, train loss real  144.5820797534\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 69\u001b[0m\n\u001b[1;32m     64\u001b[0m all_losses, epoch_loss_real \u001b[38;5;241m=\u001b[39m L2_loss(pred, target_coupled_blocks, loss_per_block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# loss_real.append(epoch_loss_real.item())\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# loss_k.append(epoch_loss)\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ik, (loss_, key) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(all_losses, model\u001b[38;5;241m.\u001b[39mmodel)):\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%timeit -r 1 -n 1\n",
    "\n",
    "new_sched = True\n",
    "\n",
    "# batch_size = 360\n",
    "# dataloader = DataLoader(ml_data, batch_size = batch_size, shuffle = True, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))\n",
    "\n",
    "train_kspace = False\n",
    "\n",
    "# target_kspace = \n",
    "loss_real = []\n",
    "\n",
    "nepoch = 100\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    if epoch >= 0:\n",
    "        train_kspace = True\n",
    "\n",
    "    model.train(True)\n",
    "\n",
    "    for ik, key in enumerate(model.model):\n",
    "        optimizers[ik].zero_grad()\n",
    "    \n",
    "    pred = model()\n",
    "        \n",
    "    if not train_kspace:\n",
    "\n",
    "        # Compute the loss for each block\n",
    "        all_losses, epoch_loss = L2_loss(pred, target_coupled_blocks, loss_per_block = True)\n",
    "\n",
    "        # Total loss\n",
    "        epoch_loss = epoch_loss.item()\n",
    "        LOSS = epoch_loss\n",
    "        # Append the values of the loss to a list\n",
    "        # loss_k.append(L2_loss(pred_kspace, k_target_blocks, norm = 2).item())\n",
    "        loss_real.append(epoch_loss)\n",
    "\n",
    "        # Loop through submodels and backpropagate\n",
    "        for ik, (loss, key) in enumerate(zip(all_losses, model.model)):\n",
    "            loss.backward(retain_graph = False)\n",
    "            torch.nn.utils.clip_grad_norm_(model.model[key].parameters(), 1)\n",
    "            optimizers[ik].step()\n",
    "            schedulers[ik].step(loss)\n",
    "            \n",
    "            if key not in losses:\n",
    "                losses[key] = [loss.item()]\n",
    "                learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "            else:\n",
    "                losses[key].append(loss.item())\n",
    "                learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "                \n",
    "    else:\n",
    "\n",
    "        pred_kspace = TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap = True)\n",
    "        # target_kspace = TMap_bloch_sums(batch.target, phase, indices, kpts_idx, return_tensormap = True)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = L2_loss(pred_kspace, target_kspace, norm = 2)\n",
    "\n",
    "        # Total loss \n",
    "        epoch_loss = loss.item()\n",
    "        LOSS = epoch_loss        \n",
    "        # Append the values of the loss to a list\n",
    "        all_losses, epoch_loss_real = L2_loss(pred, target_coupled_blocks, loss_per_block=True)\n",
    "        # loss_real.append(epoch_loss_real.item())\n",
    "        # loss_k.append(epoch_loss)\n",
    "\n",
    "               \n",
    "        loss.backward(retain_graph = True)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        for ik, (loss_, key) in enumerate(zip(all_losses, model.model)):\n",
    "            optimizers[ik].step()\n",
    "            schedulers[ik].step(epoch_loss/len(model.model))\n",
    "            if key not in losses:\n",
    "                losses[key] = [loss_.item()]\n",
    "                learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "            else:\n",
    "                losses[key].append(loss_.item())\n",
    "                learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "\n",
    "    # print(ib, end = ' ')\n",
    "    if epoch >= 0: #% 10 == 0:\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss on all blocks {epoch_loss:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss real {loss_real[-1]:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        print(f\"Epoch {epoch:>7d}, train loss real {LOSS:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cece61d-bd13-4771-8f9e-c05b3418f93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
