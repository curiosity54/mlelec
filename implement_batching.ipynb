{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d8f12c-62ee-4d46-b766-c3b49d817fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635a399b-c69b-4438-bac6-4ba0712d0d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/pyscf/dft/libxc.py:771: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, corresponding to the original definition by Stephens et al. (issue 1480) and the same as the B3LYP functional in Gaussian. To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read\n",
    "from ase.visualize import view\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import torch \n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import rascaline\n",
    "\n",
    "import metatensor \n",
    "from metatensor import Labels, TensorBlock, TensorMap\n",
    "from metatensor import load, sort\n",
    "\n",
    "from mlelec.data.dataset import PySCFPeriodicDataset, split_by_Aij_mts\n",
    "from mlelec.utils.twocenter_utils import _to_coupled_basis\n",
    "from mlelec.utils.pbc_utils import matrix_to_blocks, kmatrix_to_blocks, TMap_bloch_sums, precompute_phase, kblocks_to_matrix, kmatrix_to_blocks, blocks_to_matrix, matrix_to_blocks\n",
    "from mlelec.utils.plot_utils import print_matrix, matrix_norm, block_matrix_norm, plot_block_errors\n",
    "from mlelec.features.acdc import pair_features, single_center_features, twocenter_features_periodic_NH, twocenter_hermitian_features\n",
    "from mlelec.models.linear import LinearModelPeriodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a54e69-a6cf-413a-a1fa-bec6edae654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(dataset, device =\"cpu\", cutoff = None, target='fock', all_pairs = False, sort_orbs = True):\n",
    "    \n",
    "    blocks = matrix_to_blocks(dataset, device = device, cutoff = cutoff, all_pairs = all_pairs, target = target, sort_orbs = sort_orbs)\n",
    "    coupled_blocks = _to_coupled_basis(blocks, skip_symmetry = True, device = device, translations = True)\n",
    "\n",
    "    blocks = blocks.to(arrays='numpy')\n",
    "    blocks = sort(blocks)\n",
    "    blocks = blocks.to(arrays='torch')\n",
    "    \n",
    "    coupled_blocks = coupled_blocks.to(arrays='numpy')\n",
    "    coupled_blocks = sort(coupled_blocks)\n",
    "    coupled_blocks = coupled_blocks.to(arrays='torch')\n",
    "    \n",
    "    return blocks, coupled_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc04a6ae-03c9-489e-b7e3-347cc86f8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(dataset, all_pairs=False):\n",
    "\n",
    "    rhoij = pair_features(dataset.structures, hypers_atom, hypers_pair, order_nu = 1, all_pairs = all_pairs, both_centers = both_centers,\n",
    "                          kmesh = dataset.kmesh, device = device, lcut = LCUT, return_rho0ij = return_rho0ij)  \n",
    "    \n",
    "    if both_centers and not return_rho0ij:\n",
    "        NU = 3\n",
    "    else:\n",
    "        NU = 2\n",
    "    rhonui = single_center_features(dataset.structures, hypers_atom, order_nu = NU, lcut = LCUT, device = device,\n",
    "                                    feature_names = rhoij.property_names)\n",
    "    \n",
    "    hfeat = twocenter_features_periodic_NH(single_center = rhonui, pair = rhoij, all_pairs = all_pairs)\n",
    "\n",
    "    return hfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0131f6a-72a2-445e-9384-0ba95406c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "orbitals = {\n",
    "    'sto-3g': {5: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               6: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               7: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]]}, \n",
    "    \n",
    "    'def2svp': {6: [[1,0,0],[2,0,0],[3,0,0],[2,1,1], [2,1,-1],[2,1,0], [3,1,1], [3,1,-1],[3,1,0], [3,2,-2], [3,2,-1],[3,2,0], [3,2,1],[3,2,2]]},\n",
    "    'benzene': {6: [[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], 1:[[1,0,0]]},\n",
    "    'gthszvmolopt': {\n",
    "        6: [[2, 0, 0], [2, 1, -1], [2, 1, 0], [2, 1, 1]],\n",
    "        \n",
    "        16: [[3,0,0], \n",
    "             [3,1,-1], [3,1,0], [3,1,1]],\n",
    "\n",
    "        42: [[4,0,0], \n",
    "             [5,0,0], \n",
    "             [4,1,-1], [4,1,0], [4,1,1], \n",
    "             [4, 2, -2], [4, 2, -1], [4, 2, 0], [4, 2, 1], [4, 2, 2]]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61756a8d-5fbe-4fd2-9521-4a4604762d6d",
   "metadata": {},
   "source": [
    "# QC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb8d851-377b-4bd5-a632-397f666731e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = './'\n",
    "START = 0 \n",
    "STOP = 10\n",
    "ORBS = 'sto-3g'\n",
    "root = f'{workdir}/examples/data/periodic/deepH_graphene/wrap/'\n",
    "data_dir = root\n",
    "frames = read(f'{data_dir}/wrapped_deepH_graphene.xyz', slice(START, STOP))\n",
    "rfock = [np.load(f\"{data_dir}/realfock_{i}.npy\", allow_pickle = True).item() for i in range(START, STOP)]\n",
    "rover = [np.load(f\"{data_dir}/realoverlap_{i}.npy\", allow_pickle = True).item() for i in range(START, STOP)]\n",
    "kmesh = [1,1,1]\n",
    "dataset = PySCFPeriodicDataset(frames = frames, \n",
    "                               kmesh = kmesh, \n",
    "                               dimension = 2,\n",
    "                               fock_realspace = rfock, \n",
    "                               overlap_realspace = rover, \n",
    "                               device = device, \n",
    "                               orbs = orbitals[ORBS], \n",
    "                               orbs_name = 'sto-3g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3873bac-ec05-4a55-aeb6-abc711ebe1b2",
   "metadata": {},
   "source": [
    "# Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ac2b01-c2a3-47e2-9296-323a5eddd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567ec238-ff88-4497-930f-fcb0551861ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_blocks, target_coupled_blocks = get_targets(dataset, cutoff = cutoff, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc35a4fc-fb6e-4fd7-9376-6e83d9047b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_target_blocks = kmatrix_to_blocks(dataset, cutoff = cutoff)\n",
    "k_target_coupled_blocks = _to_coupled_basis(kmatrix_to_blocks(dataset, cutoff = cutoff), skip_symmetry = True, device = device, translations= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6f5bdfd-747b-415b-9253-5e7f10da740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase, indices, kpts_idx = precompute_phase(target_coupled_blocks, dataset, cutoff = cutoff)\n",
    "pred_k = TMap_bloch_sums(target_coupled_blocks, phase, indices, kpts_idx, return_tensormap = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa78cad-0988-4be9-812a-2209fdc5a9bf",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "db41bfa2-936d-4d9f-8f3c-22580d7959a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_radial  = 6\n",
    "max_angular = 4\n",
    "atomic_gaussian_width = 0.3\n",
    "\n",
    "hypers_pair = {'cutoff': cutoff,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': atomic_gaussian_width,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}}}\n",
    "\n",
    "\n",
    "hypers_atom = {'cutoff': 4,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': 0.3,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}}}\n",
    "\n",
    "\n",
    "return_rho0ij = False\n",
    "both_centers = False\n",
    "LCUT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "7e89ccaa-95c0-44d2-938c-3189761c58c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu pair features\n",
      "cpu single center features\n",
      "cpu single center features\n"
     ]
    }
   ],
   "source": [
    "features = compute_features(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2566f8d-43e1-4404-bf31-e80c1e2e7e57",
   "metadata": {},
   "source": [
    "# ML Dataset/Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a61b2ec7-8647-4c4c-a843-e38141d5b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metatensor.learn import Dataset, DataLoader, IndexedDataset\n",
    "from metatensor.learn.data import group as mts_group, group_and_join as group_and_join_mts\n",
    "import metatensor as mts\n",
    "from mlelec.data.dataset import split_by_Aij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06da6f3b-f22f-4e94-81d3-851162f746a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.78 s ± 1.01 s per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 10\n",
    "split_features_mts, split_target_mts = split_by_Aij_mts(target_coupled_blocks, features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a6ecf1e-63b8-403c-aa04-080b985e3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 ms ± 725 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 10\n",
    "split_features, split_target = split_by_Aij(target_coupled_blocks, features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b53aaef2-7c6b-420e-9938-ceb474a9c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_features, split_target = split_by_Aij(target_coupled_blocks, features = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b33a7a-f2b6-424b-a974-2e3b73184b63",
   "metadata": {},
   "source": [
    "## Using tensormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eda452c7-f3ce-4699-a801-435a51cb0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_features, split_target = split_by_Aij_mts(target_coupled_blocks, features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38900ab1-8d11-47b6-89d5-f184da2aae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = IndexedDataset(descriptor = list(split_features.values()), target = list(split_target.values()), sample_id = list(split_target.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "401203e3-4809-415a-a8d4-b89347c2e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "dataloader = DataLoader(ml_data, batch_size = batch_size, shuffle = True, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59794982-77a8-4ae4-b6cd-0176753165f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase, indices, kpts_idx = precompute_phase(target_coupled_blocks, dataset, cutoff = cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "37b9e355-1d5b-49f9-8c74-ac4a061b1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmap = []\n",
    "for batch in dataloader:\n",
    "    kmap.append(TMap_bloch_sums(batch.target, phase, indices, kpts_idx, return_tensormap = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b5ac5-c943-4a21-95e2-a652795b6923",
   "metadata": {},
   "source": [
    "## Metatensor native splitting by structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "d5444484-9a93-4bde-9d7c-513f339ecb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_axis = \"samples\"\n",
    "split_by_dimension = \"structure\"\n",
    "\n",
    "grouped_labels = [mts.Labels(names = split_by_dimension, values = np.array([A])) for A in mts.unique_metadata(target_coupled_blocks, axis = split_by_axis, names = split_by_dimension)]\n",
    "split_target = mts.split(target_coupled_blocks, split_by_axis, grouped_labels)\n",
    "\n",
    "grouped_labels = [mts.Labels(names = split_by_dimension, values = np.array([A])) for A in mts.unique_metadata(features, axis = split_by_axis, names = split_by_dimension)]\n",
    "split_features = mts.split(features, split_by_axis, grouped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ce082aef-50f9-4974-8501-d077de90419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = IndexedDataset(descriptor = split_features, target = split_target, sample_id = [g.values.tolist()[0][0] for g in grouped_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc808f17-7880-4242-9109-bad6638e7f81",
   "metadata": {},
   "source": [
    "Split kspace targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "a9e95cde-928d-4d13-935b-02013a2798de",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_axis = \"samples\"\n",
    "split_by_dimension = \"structure\"\n",
    "\n",
    "grouped_labels = [mts.Labels(names = split_by_dimension, values = np.array([A])) for A in mts.unique_metadata(k_target_coupled_blocks, axis = split_by_axis, names = split_by_dimension)]\n",
    "split_target_k = mts.split(k_target_coupled_blocks, split_by_axis, grouped_labels)\n",
    "\n",
    "grouped_labels = [mts.Labels(names = split_by_dimension, values = np.array([A])) for A in mts.unique_metadata(features, axis = split_by_axis, names = split_by_dimension)]\n",
    "split_features_k = mts.split(features, split_by_axis, grouped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "ee77aac7-6adb-4ffa-8f66-5b4a02e372dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_k = IndexedDataset(descriptor = split_features_k, target = split_target_k, sample_id = [g.values.tolist()[0][0] for g in grouped_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a2e61-609c-42ab-8dda-2f11ebe50a59",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "292e1e88-8d09-4e78-972e-055688f42012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.metrics import L2_loss, L2_kspace_loss\n",
    "from mlelec.utils.twocenter_utils import _to_uncoupled_basis, map_targetkeys_to_featkeys\n",
    "from mlelec.utils.pbc_utils import precompute_phase, TMap_bloch_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "8ec08c42-7548-4d7d-b243-5ff30b668e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = LinearModelPeriodic(twocfeat = features, \n",
    "                            target_blocks = target_coupled_blocks,\n",
    "                            frames = dataset.structures, orbitals = dataset.basis, \n",
    "                            device = device,\n",
    "                            bias = True,\n",
    "                            nhidden = 128, \n",
    "                            nlayers = 1,\n",
    "                            activation = 'SiLU',\n",
    "                            apply_norm = True\n",
    "                           )\n",
    "model = model.double()\n",
    "\n",
    "nepoch = 1000\n",
    "\n",
    "optimizers = []\n",
    "schedulers = []\n",
    "for i, key in enumerate(model.model):\n",
    "    optimizers.append(torch.optim.AdamW(model.model[key].parameters(), lr = 1e-3, betas = (0.8, 0.9)))\n",
    "    schedulers.append(torch.optim.lr_scheduler.ReduceLROnPlateau(optimizers[-1], factor = 0.8, patience = 30, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "254ddf17-6837-4c81-823c-7ed539682a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase, indices, kpts_idx = precompute_phase(target_coupled_blocks, dataset, cutoff = cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "3c96517d-d601-4587-a1b4-7e39c536ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = DataLoader(ml_data, batch_size = batch_size, shuffle = False, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))\n",
    "dataloader_k = DataLoader(ml_data_k, batch_size = batch_size, shuffle = False, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e11ca466-6978-4acc-8ab0-378136947d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss 64099.8212522554\n",
      "Epoch       1, train loss 44150.1930657664\n",
      "Epoch       2, train loss 26674.6991637603\n",
      "Epoch       3, train loss 10917.7301031400\n",
      "Epoch       4, train loss 1464.4126782936\n",
      "Epoch       5, train loss  145.1177953997\n",
      "Epoch       6, train loss   30.6126823539\n",
      "Epoch       7, train loss   19.7077010330\n",
      "Epoch       8, train loss   15.4932212290\n",
      "Epoch       9, train loss   15.4773647876\n",
      "Epoch      10, train loss   13.6507310172\n",
      "Epoch      11, train loss   13.0167951003\n",
      "Epoch      12, train loss   15.8249425456\n",
      "Epoch      13, train loss    7.5995894473\n",
      "Epoch      14, train loss    6.9821789883\n",
      "Epoch      15, train loss    5.6214293910\n",
      "Epoch      16, train loss    5.3257322724\n",
      "Epoch      17, train loss    5.7309572868\n",
      "Epoch      18, train loss    4.5858151378\n",
      "Epoch      19, train loss    5.0811340728\n",
      "Epoch      20, train loss    3.5698208473\n",
      "Epoch      21, train loss    4.8419821021\n",
      "Epoch      22, train loss    4.8125507777\n",
      "Epoch      23, train loss    2.3350211645\n",
      "Epoch      24, train loss    2.5375906856\n",
      "Epoch      25, train loss    1.5065589530\n",
      "Epoch      26, train loss    1.7008274707\n",
      "Epoch      27, train loss    1.8478618920\n",
      "Epoch      28, train loss    0.9558725998\n",
      "Epoch      29, train loss    3.1411059028\n",
      "Epoch      30, train loss    5.4274977041\n",
      "Epoch      31, train loss    2.4362038342\n",
      "Epoch      32, train loss    2.2392947194\n",
      "Epoch      33, train loss    1.9993964237\n",
      "Epoch      34, train loss    3.6273219508\n",
      "Epoch      35, train loss    1.0107883382\n",
      "Epoch      36, train loss    0.8249302483\n",
      "Epoch      37, train loss    2.3960685504\n",
      "Epoch      38, train loss    1.3011735002\n",
      "Epoch      39, train loss    1.1550782080\n",
      "Epoch      40, train loss    0.9473404465\n",
      "Epoch      41, train loss    1.1345131398\n",
      "Epoch      42, train loss    1.2630638639\n",
      "Epoch      43, train loss    1.1203796431\n",
      "Epoch      44, train loss    1.1762694206\n",
      "Epoch      45, train loss    0.5535060863\n",
      "Epoch      46, train loss    0.7485391576\n",
      "Epoch      47, train loss    1.8145314381\n",
      "Epoch      48, train loss    0.4431684868\n",
      "Epoch      49, train loss    0.5242323640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/Software/mlelec/src/mlelec/models/linear.py:574: UserWarning: Using train target_blocks, otherwise provide test target_blocks\n",
      "  warnings.warn('Using train hfeat, otherwise provide test hfeat')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch      50, train loss    1.0668415374\n",
      "Epoch      51, train loss    1.3058728456\n",
      "Epoch      52, train loss    0.5429070372\n",
      "Epoch      53, train loss    0.5534921993\n",
      "Epoch      54, train loss    0.5043684919\n",
      "Epoch      55, train loss    0.4577762065\n",
      "Epoch      56, train loss    0.5248318676\n",
      "Epoch      57, train loss    0.4451914575\n",
      "Epoch      58, train loss    0.3486237178\n",
      "Epoch      59, train loss    0.2837883841\n",
      "Epoch      60, train loss    0.2686054429\n",
      "Epoch      61, train loss    0.2645887641\n",
      "Epoch      62, train loss    0.2408161128\n",
      "Epoch      63, train loss    0.2389337685\n",
      "Epoch      64, train loss    0.2322626037\n",
      "Epoch      65, train loss    0.2169345446\n",
      "Epoch      66, train loss    0.2111775954\n",
      "Epoch      67, train loss    0.2091001209\n",
      "Epoch      68, train loss    0.2004720243\n",
      "Epoch      69, train loss    0.1929587508\n",
      "Epoch      70, train loss    0.1920110071\n",
      "Epoch      71, train loss    0.1892915645\n",
      "Epoch      72, train loss    0.2062768902\n",
      "Epoch      73, train loss    0.1988025887\n",
      "Epoch      74, train loss    0.1847175963\n",
      "Epoch      75, train loss    0.2059692535\n",
      "Epoch      76, train loss    0.1797090260\n",
      "Epoch      77, train loss    0.1970397558\n",
      "Epoch      78, train loss    0.1788830479\n",
      "Epoch      79, train loss    0.1948356912\n",
      "Epoch      80, train loss    0.1687722273\n",
      "Epoch      81, train loss    0.1698549907\n",
      "Epoch      82, train loss    0.1641018016\n",
      "Epoch      83, train loss    0.1622942271\n",
      "Epoch      84, train loss    0.1757134318\n",
      "Epoch      85, train loss    0.1580588610\n",
      "Epoch      86, train loss    0.1712873603\n",
      "Epoch      87, train loss    0.1655668273\n",
      "Epoch      88, train loss    0.1509486913\n",
      "Epoch      89, train loss    0.1517273512\n",
      "Epoch      90, train loss    0.1545271180\n",
      "Epoch      91, train loss    0.1498399803\n",
      "Epoch      92, train loss    0.1705196570\n",
      "Epoch      93, train loss    0.1670774885\n",
      "Epoch      94, train loss    0.1485329553\n",
      "Epoch      95, train loss    0.1591662314\n",
      "Epoch      96, train loss    0.1428566152\n",
      "Epoch      97, train loss    0.1410844387\n",
      "Epoch      98, train loss    0.1450173272\n",
      "Epoch      99, train loss    0.1393366437\n"
     ]
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "train_kspace = False\n",
    "LOSS_LIST = []\n",
    "\n",
    "nepoch = 100\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    if epoch >= 50:\n",
    "        train_kspace = True\n",
    "\n",
    "    if not train_kspace:\n",
    "        # Train against real space targets\n",
    "        LOSS = 0\n",
    "        for ib, batch in enumerate(dataloader):\n",
    "            \n",
    "            model.train(True)\n",
    "            \n",
    "            for ik, key in enumerate(model.model):\n",
    "                optimizers[ik].zero_grad()\n",
    "            \n",
    "            pred = model.predict_batch(batch.descriptor, batch.target)\n",
    "            \n",
    "            # Compute the loss for each block\n",
    "            all_losses, epoch_loss = L2_loss(pred, batch.target, loss_per_block = True)\n",
    "    \n",
    "            # Total loss\n",
    "            epoch_loss = epoch_loss.item()\n",
    "            LOSS += epoch_loss\n",
    "    \n",
    "            # Loop through submodels and backpropagate\n",
    "            for ik, (loss, key) in enumerate(zip(all_losses, model.model)):\n",
    "                loss.backward(retain_graph = False)\n",
    "                torch.nn.utils.clip_grad_norm_(model.model[key].parameters(), 1)\n",
    "                optimizers[ik].step()\n",
    "                schedulers[ik].step(loss)\n",
    "\n",
    "    else:\n",
    "        # Train against k-space targets\n",
    "    \n",
    "        LOSS = 0\n",
    "        for ib, batch in enumerate(dataloader_k):\n",
    "\n",
    "            model.train(True)\n",
    "    \n",
    "            for ik, key in enumerate(model.model):\n",
    "                optimizers[ik].zero_grad()\n",
    "            \n",
    "            pred = model.predict_batch(batch.descriptor)\n",
    "            pred_kspace = TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap = True)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = L2_loss(pred_kspace, batch.target, norm = 2)\n",
    "    \n",
    "            # Total loss \n",
    "            epoch_loss = loss.item()\n",
    "            LOSS += epoch_loss\n",
    "                   \n",
    "            loss.backward(retain_graph = True)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    \n",
    "            for ik, key in enumerate(model.model):\n",
    "                optimizers[ik].step()\n",
    "                schedulers[ik].step(epoch_loss/len(model.model))\n",
    "\n",
    "    if epoch >= 0: #% 10 == 0:\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss on all blocks {epoch_loss:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss real {loss_real[-1]:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        LOSS_LIST.append(LOSS)\n",
    "        print(f\"Epoch {epoch:>7d}, train loss {LOSS:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "d1de51fa-a7f9-46d0-848a-2c53a8766968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7cfc6bccea10>]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(LOSS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e5cae-6516-475f-be74-138cf1f6835a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da116ef9-b9d9-4705-8355-868a24412b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e7f3e-2704-472c-bd15-44c732911fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a214f25-4a97-4475-9e81-5a4664959c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92e9d6fd-8c80-450c-a83f-2cce15f02697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss real  593.0036468974\n",
      "Epoch       1, train loss real   49.2449186928\n",
      "Epoch       2, train loss real    2.0704940922\n",
      "Epoch       3, train loss real    0.6410718528\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     pred_kspace \u001b[38;5;241m=\u001b[39m TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 54\u001b[0m     target_kspace \u001b[38;5;241m=\u001b[39m \u001b[43mTMap_bloch_sums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkpts_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensormap\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     loss \u001b[38;5;241m=\u001b[39m L2_loss(pred_kspace, target_kspace, norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/utils/pbc_utils.py:954\u001b[0m, in \u001b[0;36mTMap_bloch_sums\u001b[0;34m(target_blocks, phase, indices, kpts_idx, return_tensormap)\u001b[0m\n\u001b[1;32m    951\u001b[0m pshape \u001b[38;5;241m=\u001b[39m phase[kl][ifr, i, j]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# equivalent to torch.einsum('Tmnv,kT->kmnv', values.to(phase[kl][ifr, i, j]), phase[kl][ifr, i, j]), but faster\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m contraction \u001b[38;5;241m=\u001b[39m (phase[kl][ifr, i, j]\u001b[38;5;129m@values\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape(pshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39mvshape[\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m*\u001b[39mfactor\n\u001b[1;32m    956\u001b[0m \u001b[38;5;66;03m# if bt == 1 or bt == 2 or (bt == -1 and i != j):\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (bt \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "new_sched = True\n",
    "\n",
    "train_kspace = False\n",
    "\n",
    "loss_real = []\n",
    "\n",
    "nepoch = 100\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    if epoch >= 0:\n",
    "        train_kspace = True\n",
    "\n",
    "    LOSS = 0\n",
    "    for ib, batch in enumerate(dataloader):\n",
    "\n",
    "        model.train(True)\n",
    "\n",
    "        for ik, key in enumerate(model.model):\n",
    "            optimizers[ik].zero_grad()\n",
    "        \n",
    "        pred = model.predict_batch(batch.descriptor, batch.target)\n",
    "            \n",
    "        if not train_kspace:\n",
    "    \n",
    "            # Compute the loss for each block\n",
    "            all_losses, epoch_loss = L2_loss(pred, batch.target, loss_per_block = True)\n",
    "    \n",
    "            # Total loss\n",
    "            epoch_loss = epoch_loss.item()\n",
    "            LOSS += epoch_loss\n",
    "            # Append the values of the loss to a list\n",
    "            # loss_k.append(L2_loss(pred_kspace, k_target_blocks, norm = 2).item())\n",
    "            loss_real.append(epoch_loss)\n",
    "    \n",
    "            # Loop through submodels and backpropagate\n",
    "            for ik, (loss, key) in enumerate(zip(all_losses, model.model)):\n",
    "                loss.backward(retain_graph = False)\n",
    "                torch.nn.utils.clip_grad_norm_(model.model[key].parameters(), 1)\n",
    "                optimizers[ik].step()\n",
    "                schedulers[ik].step(loss)\n",
    "                \n",
    "                if key not in losses:\n",
    "                    losses[key] = [loss.item()]\n",
    "                    learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "                else:\n",
    "                    losses[key].append(loss.item())\n",
    "                    learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "                    \n",
    "        else:\n",
    "\n",
    "            pred_kspace = TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap = True)\n",
    "            target_kspace = TMap_bloch_sums(batch.target, phase, indices, kpts_idx, return_tensormap = True)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = L2_loss(pred_kspace, target_kspace, norm = 2)\n",
    "    \n",
    "            # Total loss \n",
    "            epoch_loss = loss.item()\n",
    "            LOSS += epoch_loss\n",
    "            \n",
    "            # Append the values of the loss to a list\n",
    "            all_losses, epoch_loss_real = L2_loss(pred, batch.target, loss_per_block=True)\n",
    "            # loss_real.append(epoch_loss_real.item())\n",
    "            # loss_k.append(epoch_loss)\n",
    "    \n",
    "                   \n",
    "            loss.backward(retain_graph = True)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    \n",
    "            # for ik, (loss_, key) in enumerate(zip(all_losses, model.model)):\n",
    "            for ik, key in enumerate(model.model):\n",
    "                optimizers[ik].step()\n",
    "                schedulers[ik].step(epoch_loss/len(model.model))\n",
    "                # if key not in losses:\n",
    "                #     # losses[key] = [loss_.item()]\n",
    "                #     learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "                # else:\n",
    "                #     # losses[key].append(loss_.item())\n",
    "                #     learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "\n",
    "        # print(ib, end = ' ')\n",
    "    if epoch >= 0: #% 10 == 0:\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss on all blocks {epoch_loss:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss real {loss_real[-1]:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        print(f\"Epoch {epoch:>7d}, train loss real {LOSS:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24f39f8b-0cb8-4c9f-8318-bbd23666dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_kspace = TMap_bloch_sums(target_coupled_blocks, phase, indices, kpts_idx, return_tensormap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad5b5e15-98f7-4f1d-9aec-2bd4bed5d3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss real 1276.7376358469\n",
      "Epoch       1, train loss real 1022.9805296847\n",
      "Epoch       2, train loss real  828.5650614440\n",
      "Epoch       3, train loss real  679.7027881757\n",
      "Epoch       4, train loss real  559.7761959069\n",
      "Epoch       5, train loss real  460.1125304624\n",
      "Epoch       6, train loss real  375.6945812240\n",
      "Epoch       7, train loss real  303.1982950560\n",
      "Epoch       8, train loss real  240.1988690185\n",
      "Epoch       9, train loss real  187.3344156086\n",
      "Epoch      10, train loss real  144.5820797534\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 69\u001b[0m\n\u001b[1;32m     64\u001b[0m all_losses, epoch_loss_real \u001b[38;5;241m=\u001b[39m L2_loss(pred, target_coupled_blocks, loss_per_block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# loss_real.append(epoch_loss_real.item())\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# loss_k.append(epoch_loss)\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ik, (loss_, key) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(all_losses, model\u001b[38;5;241m.\u001b[39mmodel)):\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%timeit -r 1 -n 1\n",
    "\n",
    "new_sched = True\n",
    "\n",
    "# batch_size = 360\n",
    "# dataloader = DataLoader(ml_data, batch_size = batch_size, shuffle = True, collate_fn = lambda x: group_and_join_mts(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))\n",
    "\n",
    "train_kspace = False\n",
    "\n",
    "# target_kspace = \n",
    "loss_real = []\n",
    "\n",
    "nepoch = 100\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    if epoch >= 0:\n",
    "        train_kspace = True\n",
    "\n",
    "    model.train(True)\n",
    "\n",
    "    for ik, key in enumerate(model.model):\n",
    "        optimizers[ik].zero_grad()\n",
    "    \n",
    "    pred = model()\n",
    "        \n",
    "    if not train_kspace:\n",
    "\n",
    "        # Compute the loss for each block\n",
    "        all_losses, epoch_loss = L2_loss(pred, target_coupled_blocks, loss_per_block = True)\n",
    "\n",
    "        # Total loss\n",
    "        epoch_loss = epoch_loss.item()\n",
    "        LOSS = epoch_loss\n",
    "        # Append the values of the loss to a list\n",
    "        # loss_k.append(L2_loss(pred_kspace, k_target_blocks, norm = 2).item())\n",
    "        loss_real.append(epoch_loss)\n",
    "\n",
    "        # Loop through submodels and backpropagate\n",
    "        for ik, (loss, key) in enumerate(zip(all_losses, model.model)):\n",
    "            loss.backward(retain_graph = False)\n",
    "            torch.nn.utils.clip_grad_norm_(model.model[key].parameters(), 1)\n",
    "            optimizers[ik].step()\n",
    "            schedulers[ik].step(loss)\n",
    "            \n",
    "            if key not in losses:\n",
    "                losses[key] = [loss.item()]\n",
    "                learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "            else:\n",
    "                losses[key].append(loss.item())\n",
    "                learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "                \n",
    "    else:\n",
    "\n",
    "        pred_kspace = TMap_bloch_sums(pred, phase, indices, kpts_idx, return_tensormap = True)\n",
    "        # target_kspace = TMap_bloch_sums(batch.target, phase, indices, kpts_idx, return_tensormap = True)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = L2_loss(pred_kspace, target_kspace, norm = 2)\n",
    "\n",
    "        # Total loss \n",
    "        epoch_loss = loss.item()\n",
    "        LOSS = epoch_loss        \n",
    "        # Append the values of the loss to a list\n",
    "        all_losses, epoch_loss_real = L2_loss(pred, target_coupled_blocks, loss_per_block=True)\n",
    "        # loss_real.append(epoch_loss_real.item())\n",
    "        # loss_k.append(epoch_loss)\n",
    "\n",
    "               \n",
    "        loss.backward(retain_graph = True)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        for ik, (loss_, key) in enumerate(zip(all_losses, model.model)):\n",
    "            optimizers[ik].step()\n",
    "            schedulers[ik].step(epoch_loss/len(model.model))\n",
    "            if key not in losses:\n",
    "                losses[key] = [loss_.item()]\n",
    "                learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "            else:\n",
    "                losses[key].append(loss_.item())\n",
    "                learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "\n",
    "    # print(ib, end = ' ')\n",
    "    if epoch >= 0: #% 10 == 0:\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss on all blocks {epoch_loss:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss real {loss_real[-1]:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        print(f\"Epoch {epoch:>7d}, train loss real {LOSS:>15.10f}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cece61d-bd13-4771-8f9e-c05b3418f93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
