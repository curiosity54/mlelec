{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jigyasa/miniconda3/lib/python3.12/site-packages/pyscf/dft/libxc.py:771: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, corresponding to the original definition by Stephens et al. (issue 1480) and the same as the B3LYP functional in Gaussian. To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import metatensor.torch as mts\n",
    "from metatensor.torch import TensorMap, Labels, TensorBlock\n",
    "import ase \n",
    "from mlelec.data.dataset import QMDataset\n",
    "from mlelec.utils.target_utils import get_targets\n",
    "from mlelec.utils.twocenter_utils import _to_coupled_basis,_to_uncoupled_basis_old\n",
    "torch.set_default_dtype(torch.float64)\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlelec.data.pyscf_calculator import _instantiate_pyscf_mol\n",
    "import pyscf.pbc.tools.pyscf_ase as pyscf_ase\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import pyscf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_xhat(frame, basis='sto-3g', fix_xyz=False, device='cpu'):\n",
    "    mol = pyscf.gto.Mole()\n",
    "    mol.atom = pyscf_ase.ase_atoms_to_pyscf(frame)\n",
    "    mol.basis = basis\n",
    "    mol.symmetry = False    \n",
    "    mol.verbose = 2\n",
    "    mol.build() \n",
    "    with mol.with_common_orig((0,0,0)):\n",
    "       x= torch.from_numpy(mol.intor('int1e_r', comp=3)).to(device = device)\n",
    "    if fix_xyz:\n",
    "        x = torch.roll(x, shifts=-1, dims=0)\n",
    "\n",
    "    return x.moveaxis(0,-1)\n",
    "\n",
    "# other integrals to try mol.intor('cint1e_kin_sph') #+ mol.intor('cint1e_nuc_sph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jigyasa/scratch/mlelec/src/mlelec/data/dataset.py:210: UserWarning: Overlap matrices not provided\n",
      "  warnings.warn(\"Overlap matrices not provided\")\n"
     ]
    }
   ],
   "source": [
    "frames = [ase.Atoms('H2O', positions=[[0, 0, 0], [0, 0, 1], [0, 1, 0]], pbc = False)]\n",
    "H = torch.randn(1,7,7) \n",
    "H = H + H.transpose(-1,-2)   \n",
    "qmdata = QMDataset(frames = frames, \n",
    "                   fock_realspace= H,\n",
    "                   dimension = 0, \n",
    "                   orbs = {8:[[1,0,0],[2,0,0],[2,1,0], [2,1,1], [2,1,-1]], 1:[[1,0,0]]},\n",
    "                   orbs_name = 'sto-3g',    \n",
    "                   device = 'cpu'\n",
    ")   \n",
    "blocks, coupled_blocks  = get_targets(qmdata, cutoff = 4, device = 'cpu', all_pairs = False, sort_orbs =True, return_uncoupled=True)\n",
    "# just needed them for keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat_sto3g = []\n",
    "xhat_def2 = []\n",
    "for f in frames:\n",
    "    # _instantiate_pyscf_mol(frames[0], basis=\"sto-3g\"\n",
    "    xhat_sto3g.append(compute_xhat(f, basis='sto-3g'))\n",
    "    xhat_def2.append(compute_xhat(f, basis='def2-svp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1102e-16,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1102e-16,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xhat_sto3g[0][...,0] - xhat_sto3g[0][...,0].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor - to -blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go from matrix to blocks of the shape(nsample, nmi, nmj,3, nprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jigyasa/scratch/mlelec/src/mlelec/utils/pbc_utils.py:1706: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  assert np.isclose(torch.norm(matrixT - matrixmT.T).item(), 0.0), f\"Failed to check H({T}) = H({mT})^\\dagger\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 2.2204e-16, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [2.2204e-16, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlelec.utils.pbc_utils import blocks_to_matrix, matrix_to_blocks\n",
    "\n",
    "blocks_to_matrix(matrix_to_blocks(qmdata, cutoff = 10), qmdata)[0][0,0,0] - qmdata.fock_realspace[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jigyasa/scratch/mlelec/src/mlelec/utils/pbc_utils.py:147: UserWarning: high_rank must be True if matrix is a 3D tensor, setting to True\n",
      "  high_rank = True\n"
     ]
    }
   ],
   "source": [
    "xhat_blocks= matrix_to_blocks(qmdata, matrix= xhat_sto3g, cutoff = 10, high_rank = True, )\n",
    "# xhat_blocks = mts.remove_dimension(xhat_blocks, 'samples', 'cell_shift_a') \n",
    "# xhat_blocks = mts.remove_dimension(xhat_blocks, 'samples', 'cell_shift_b') \n",
    "# xhat_blocks= mts.remove_dimension(xhat_blocks, 'samples', 'cell_shift_c') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# couple blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a 3D tensor, we should have the same block structure, except, we have an additional components dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_3D = []\n",
    "position_components = Labels(['m_3'], values = torch.tensor([-1,0,1]).reshape(3,-1))\n",
    "for block in blocks:\n",
    "    nsample, nmi, nmj, nprop = block.values.shape\n",
    "    blocks_3D.append(\n",
    "        TensorBlock( values = torch.randn(nsample, nmi, nmj,3, nprop), \n",
    "                    components = [block.components[0],  block.components[1], position_components],\n",
    "                    properties = block.properties,\n",
    "                    samples = block.samples,\n",
    "        )\n",
    "    )\n",
    "key_names = blocks.keys.names + [\"l_3\"]\n",
    "key_value = torch.nn.functional.pad(blocks.keys.values, (0,1), mode='constant', value=1) \n",
    "\n",
    "uncoupled_blocks_3D = TensorMap( Labels(key_names, key_value) , blocks_3D)\n",
    "\n",
    "uncoupled_blocks_3D = mts.remove_dimension(uncoupled_blocks_3D, 'samples', 'cell_shift_a') \n",
    "uncoupled_blocks_3D = mts.remove_dimension(uncoupled_blocks_3D, 'samples', 'cell_shift_b') \n",
    "uncoupled_blocks_3D = mts.remove_dimension(uncoupled_blocks_3D, 'samples', 'cell_shift_c') \n",
    "uncoupled_blocks_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 12 blocks\n",
       "keys: block_type  species_i  n_i  l_i  species_j  n_j  l_j  l_3\n",
       "          -1          1       1    0       1       1    0    1\n",
       "          0           1       1    0       1       1    0    1\n",
       "          0           8       1    0       8       1    0    1\n",
       "          0           8       1    0       8       2    0    1\n",
       "          0           8       1    0       8       2    1    1\n",
       "          0           8       2    0       8       2    0    1\n",
       "          0           8       2    0       8       2    1    1\n",
       "          0           8       2    1       8       2    1    1\n",
       "          1           1       1    0       1       1    0    1\n",
       "          2           1       1    0       8       1    0    1\n",
       "          2           1       1    0       8       2    0    1\n",
       "          2           1       1    0       8       2    1    1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupled_blocks = _to_coupled_basis(xhat_blocks, skip_symmetry = False, device = 'cpu', translations = True, high_rank = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decouple blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "unc = _to_uncoupled_basis_old(coupled_blocks,device = 'cpu', translations = None, high_rank = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 0, 1, 1, 0, 1], dtype=torch.int32) tensor(1.3323e-15)\n",
      "tensor([1, 1, 1, 0, 1, 1, 0, 1], dtype=torch.int32) tensor(4.4409e-16)\n",
      "tensor([-1,  1,  1,  0,  1,  1,  0,  1], dtype=torch.int32) tensor(0.)\n",
      "tensor([2, 1, 1, 0, 8, 1, 0, 1], dtype=torch.int32) tensor(3.1038e-17)\n",
      "tensor([2, 1, 1, 0, 8, 2, 0, 1], dtype=torch.int32) tensor(1.9230e-16)\n",
      "tensor([2, 1, 1, 0, 8, 2, 1, 1], dtype=torch.int32) tensor(6.1448e-16)\n",
      "tensor([0, 8, 1, 0, 8, 1, 0, 1], dtype=torch.int32) tensor(4.4409e-16)\n",
      "tensor([0, 8, 1, 0, 8, 2, 0, 1], dtype=torch.int32) tensor(1.1102e-16)\n",
      "tensor([0, 8, 1, 0, 8, 2, 1, 1], dtype=torch.int32) tensor(8.1218e-17)\n",
      "tensor([0, 8, 2, 0, 8, 2, 0, 1], dtype=torch.int32) tensor(4.4409e-16)\n",
      "tensor([0, 8, 2, 0, 8, 2, 1, 1], dtype=torch.int32) tensor(9.9920e-16)\n",
      "tensor([0, 8, 2, 1, 8, 2, 1, 1], dtype=torch.int32) tensor(2.1896e-15)\n"
     ]
    }
   ],
   "source": [
    "import metatensor.torch as mts\n",
    "\n",
    "for k,b in unc.items():\n",
    "    b1 = xhat_blocks.block(k)\n",
    "    print(k.values, torch.norm(b.values - b1.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBlock\n",
       "    samples (2): ['structure', 'center', 'neighbor', 'cell_shift_a', 'cell_shift_b', 'cell_shift_c']\n",
       "    components (1, 1, 3): ['m_i', 'm_j', 'm_3']\n",
       "    properties (1): ['dummy']\n",
       "    gradients: None"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blocks to tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.utils.pbc_utils import inverse_bloch_sum, _orbs_offsets, _components_idx, ISQRT_2, _atom_blocks_idx\n",
    "from ase.units import Bohr\n",
    "import warnings\n",
    "\n",
    "def blocks_to_matrix(blocks, dataset, device=None, cg = None, all_pairs = False, sort_orbs = True, detach = False, check_hermiticity = True, high_rank = False):\n",
    "\n",
    "    if device is None:\n",
    "        device = dataset.device\n",
    "        \n",
    "    if \"L\" in blocks.keys.names:\n",
    "        from mlelec.utils.twocenter_utils import _to_uncoupled_basis\n",
    "        blocks = _to_uncoupled_basis(blocks, cg = cg, device = device)\n",
    "\n",
    "    orbs_tot, orbs_offset = _orbs_offsets(dataset.basis)\n",
    "    atom_blocks_idx = _atom_blocks_idx(dataset.structures, orbs_tot)\n",
    "    orbs_mult = {\n",
    "        species: \n",
    "                {tuple(k): v\n",
    "            for k, v in zip(\n",
    "                *np.unique(\n",
    "                    np.asarray(dataset.basis[species])[:, :2],\n",
    "                    axis=0,\n",
    "                    return_counts=True,\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "        for species in dataset.basis\n",
    "    }\n",
    "\n",
    "    reconstructed_matrices = []\n",
    "    \n",
    "    # bt1\n",
    "    bt1factor = ISQRT_2\n",
    "    if all_pairs:\n",
    "        bt1factor /= 2\n",
    "\n",
    "    # bt 2 \n",
    "    bt2_factor_p=0.5\n",
    "    if not all_pairs:\n",
    "        bt2_factor_p=1\n",
    "\n",
    "    for A in range(len(dataset.structures)):\n",
    "        norbs = np.sum([orbs_tot[ai] for ai in dataset.structures[A].numbers])\n",
    "        reconstructed_matrices.append({})\n",
    "\n",
    "    # loops over block types\n",
    "    for key, block in blocks.items():\n",
    "        block_type = key[\"block_type\"]\n",
    "        ai, ni, li = key[\"species_i\"], key[\"n_i\"], key[\"l_i\"]\n",
    "        aj, nj, lj = key[\"species_j\"], key[\"n_j\"], key[\"l_j\"]\n",
    "        \n",
    "        #----sorting ni,li,nj,lj---\n",
    "        if sort_orbs:\n",
    "            fac=1 # sorted orbs - we only count everything once\n",
    "            if ai == aj and (ni == nj and li == lj): #except these diag blocks\n",
    "                fac=2 #so we need to divide by 2 to avoic double count\n",
    "        else: \n",
    "            # no sorting -->  we count everything twice\n",
    "            fac=2\n",
    "        #----sorting ni,li,nj,lj---\n",
    "        # What's the multiplicity of the orbital type, ex. 2p_x, 2p_y, 2p_z makes the multiplicity \n",
    "        # of a p block = 3\n",
    "        orbs_i = orbs_mult[ai]\n",
    "        orbs_j = orbs_mult[aj]\n",
    "        \n",
    "        # The shape of the block corresponding to the orbital pair\n",
    "        shapes = {\n",
    "            (k1 + k2): (orbs_i[tuple(k1)], orbs_j[tuple(k2)])\n",
    "            for k1 in orbs_i\n",
    "            for k2 in orbs_j\n",
    "        }\n",
    "        # where does orbital PHI = (ni, li) start within a block of atom i\n",
    "        phioffset = orbs_offset[(ai, ni, li)] \n",
    "        # where does orbital PSI = (nj,lj) start within a block of atom j\n",
    "        psioffset = orbs_offset[(aj, nj, lj)]\n",
    "\n",
    "        # loops over samples (structure, i, j)\n",
    "\n",
    "        samples = block.samples.values.tolist()\n",
    "        \n",
    "        blockvalues = block.values\n",
    "        if detach:\n",
    "            blockvalues = blockvalues.detach() #.clone()\n",
    "\n",
    "        for sample, blockval in zip(samples, blockvalues[:,:,:,0]):\n",
    "    \n",
    "        # for sample, blockval in zip(block.samples.values, block.values):\n",
    "\n",
    "            if blockval.numel() == 0:\n",
    "                # Empty block\n",
    "                continue        \n",
    "\n",
    "            A, i, j, Tx, Ty, Tz = sample #.tolist()\n",
    "            T = Tx, Ty, Tz\n",
    "            mT = -Tx, -Ty, -Tz\n",
    "\n",
    "            other_fac = 1\n",
    "            if i == j and T != (0,0,0) and not all_pairs:\n",
    "                other_fac = 0.5\n",
    "\n",
    "            # bt 0\n",
    "            if not sort_orbs:\n",
    "                bt0_factor_p = 0.5\n",
    "            else: \n",
    "                if not(ni==nj and li==lj):\n",
    "                    bt0_factor_p = 1\n",
    "                else:\n",
    "                    bt0_factor_p = 0.5\n",
    "            bt0_factor_m = bt0_factor_p*other_fac\n",
    "\n",
    "            # bt 2 \n",
    "            # bt2_factor_p=0.5\n",
    "            # if not all_pairs:\n",
    "            #     bt2_factor_p=1\n",
    "            bt2_factor_m = bt2_factor_p*other_fac\n",
    "\n",
    "            # bt 1\n",
    "            bt1_fact_fin = bt1factor/fac*other_fac\n",
    "\n",
    "            \n",
    "            if T not in reconstructed_matrices[A]:\n",
    "                assert mT not in reconstructed_matrices[A], \"why is mT present but not T?\"\n",
    "                norbs = np.sum([orbs_tot[ai] for ai in dataset.structures[A].numbers])\n",
    "                reconstructed_matrices[A][T] = torch.zeros(norbs, norbs, device = device)\n",
    "                reconstructed_matrices[A][mT] = torch.zeros(norbs, norbs, device = device)\n",
    "\n",
    "            matrix_T  = reconstructed_matrices[A][T]\n",
    "            matrix_mT = reconstructed_matrices[A][mT]\n",
    "            # beginning of the block corresponding to the atom i-j pair\n",
    "            i_start, j_start = atom_blocks_idx[(A, i, j)]\n",
    "            # where does orbital (ni, li) end (or how large is it)\n",
    "            phi_end = shapes[(ni, li, nj, lj)][0]  # orb end\n",
    "            # where does orbital (nj, lj) end (or how large is it)\n",
    "            psi_end = shapes[(ni, li, nj, lj)][1]  \n",
    "\n",
    "            iphi_jpsi_slice = slice(i_start + phioffset , i_start + phioffset + phi_end),\\\n",
    "                              slice(j_start + psioffset , j_start + psioffset + psi_end)\n",
    "            ipsi_jphi_slice = slice(i_start + psioffset , i_start + psioffset + psi_end),\\\n",
    "                              slice(j_start + phioffset , j_start + phioffset + phi_end),\n",
    "                            \n",
    "            jphi_ipsi_slice = slice(j_start + phioffset , j_start + phioffset + phi_end),\\\n",
    "                              slice(i_start + psioffset , i_start + psioffset + psi_end)\n",
    "            \n",
    "            jpsi_iphi_slice = slice(j_start + psioffset , j_start + psioffset + psi_end),\\\n",
    "                              slice(i_start + phioffset , i_start + phioffset + phi_end)\n",
    "\n",
    "            # if detach:\n",
    "            #     bv = blockval[:, :, 0].detach().clone()\n",
    "            # else:\n",
    "                # bv = blockval[:, :, 0]\n",
    "            bv = blockval #[:, :, 0]\n",
    "            # position of the orbital within this block\n",
    "            if block_type == 0:\n",
    "                # <i \\phi| H(T)|j \\psi> = # <i \\phi| H(-T)|j \\psi>^T \n",
    "                # if not sort_orbs:\n",
    "                #     ff = 0.5\n",
    "                # else: \n",
    "                #     # ff = 0.5\n",
    "                #     if not(ni==nj and li==lj):\n",
    "                #         ff = 1\n",
    "                #     else:\n",
    "                #         ff = 0.5\n",
    "\n",
    "                matrix_T[iphi_jpsi_slice] += bv*bt0_factor_p\n",
    "                matrix_mT[jpsi_iphi_slice] += bv.T*bt0_factor_m\n",
    "                \n",
    "            elif block_type == 2:\n",
    "                \n",
    "                # ff=0.5\n",
    "                # if not all_pairs:\n",
    "                #     ff=1\n",
    "                \n",
    "                matrix_T[iphi_jpsi_slice] += bv*bt2_factor_p\n",
    "                matrix_mT[jpsi_iphi_slice] += bv.T*bt2_factor_m\n",
    "                \n",
    "            elif abs(block_type) == 1:\n",
    "                # Eq (1) <i \\phi| H(T)|j \\psi> = # block_(+1)ijT + block_(-1)ijT \n",
    "                # Eq (2) <j \\phi| H(-T)|i \\psi> = # block_(+1)ijT - block_(-1)ijT \n",
    "                # Eq (3) <j \\psi| H(-T)|i \\phi> = # block_(+1)ijT^\\dagger + block_(-1)ijT^\\dagger (Transpose of Eq1) \n",
    "                # Eq (4) <i \\psi| H(T)|j \\phi> = # block_(+1)ijT^\\dagger - block_(-1)ijT^\\dagger (Transpose of Eq2)\n",
    "                bv = bv*bt1_fact_fin\n",
    "\n",
    "                if block_type == 1:\n",
    "                    # first half of Eq (1) \n",
    "                    matrix_T[iphi_jpsi_slice] += bv\n",
    "                    # first half of Eq (2)\n",
    "                    matrix_mT[jphi_ipsi_slice] += bv\n",
    "                    # first half of Eq (3)\n",
    "                    matrix_mT[jpsi_iphi_slice] += bv.T\n",
    "                    # first half of Eq (4)\n",
    "                    matrix_T[ ipsi_jphi_slice] += bv.T\n",
    "        \n",
    "                else:\n",
    "                    # second half of Eq (1)\n",
    "                    matrix_T[iphi_jpsi_slice] += bv\n",
    "                    # second half of Eq (2)\n",
    "                    matrix_mT[jphi_ipsi_slice] -= bv\n",
    "                    # second half of Eq (3)\n",
    "                    matrix_mT[jpsi_iphi_slice] += bv.T\n",
    "                    # second half of Eq (4)\n",
    "                    matrix_T[ipsi_jphi_slice ] -= bv.T\n",
    "         \n",
    "    if check_hermiticity:\n",
    "        for A, matrix in enumerate(reconstructed_matrices):\n",
    "            Ts = list(matrix.keys())\n",
    "            for T in Ts:\n",
    "                mT = tuple(-t for t in T)\n",
    "            \n",
    "                assert torch.all(torch.isclose(matrix[T] - reconstructed_matrices[A][mT].T, torch.zeros_like(matrix[T]))), torch.norm(matrix[T] - reconstructed_matrices[A][mT].T).item()\n",
    "\n",
    "    return reconstructed_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 1] doesn't match the broadcast shape [1, 1, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mblocks_to_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxhat_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqmdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[81], line 164\u001b[0m, in \u001b[0;36mblocks_to_matrix\u001b[0;34m(blocks, dataset, device, cg, all_pairs, sort_orbs, detach, check_hermiticity)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# position of the orbital within this block\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# <i \\phi| H(T)|j \\psi> = # <i \\phi| H(-T)|j \\psi>^T \u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# if not sort_orbs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m#     else:\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m#         ff = 0.5\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mmatrix_T\u001b[49m\u001b[43m[\u001b[49m\u001b[43miphi_jpsi_slice\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbv\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbt0_factor_p\u001b[49m\n\u001b[1;32m    165\u001b[0m     matrix_mT[jpsi_iphi_slice] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bv\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m*\u001b[39mbt0_factor_m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m block_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    168\u001b[0m     \n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# ff=0.5\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# if not all_pairs:\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m#     ff=1\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 1] doesn't match the broadcast shape [1, 1, 1]"
     ]
    }
   ],
   "source": [
    "blocks_to_matrix(xhat_blocks, qmdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_2Dx = []; blocks_2Dy = []; blocks_2Dz = []\n",
    "for block in uncoupled_blocks_3D:\n",
    "    nsample, nmi, nmj, _, nprop = block.values.shape\n",
    "    blocks_2Dy.append(\n",
    "        TensorBlock( values = block.values[...,0,:], \n",
    "                    components = [block.components[0],  block.components[1]],\n",
    "                    properties = block.properties,\n",
    "                      samples = block.samples,\n",
    "        ))\n",
    "    blocks_2Dz.append(\n",
    "        TensorBlock( values = block.values[...,1,:], \n",
    "                    components = [block.components[0],  block.components[1]],\n",
    "                    properties = block.properties,\n",
    "                    samples = block.samples,\n",
    "        ))\n",
    "    blocks_2Dx.append(\n",
    "        TensorBlock( values = block.values[...,2,:], \n",
    "                    components = [block.components[0],  block.components[1]],\n",
    "                    properties = block.properties,\n",
    "                    samples = block.samples,\n",
    "        ))\n",
    "uncoupled_blocks_2Dx = TensorMap( blocks.keys , blocks_2Dx)\n",
    "uncoupled_blocks_2Dy = TensorMap( blocks.keys , blocks_2Dy)\n",
    "uncoupled_blocks_2Dz = TensorMap( blocks.keys , blocks_2Dz)\n",
    "\n",
    "# uncoupled_blocks_2Dx = mts.remove_dimension(uncoupled_blocks_2Dx, 'samples', 'cell_shift_a') \n",
    "# uncoupled_blocks_2Dx = mts.remove_dimension(uncoupled_blocks_2Dx, 'samples', 'cell_shift_b') \n",
    "# uncoupled_blocks_2Dx = mts.remove_dimension(uncoupled_blocks_2Dx, 'samples', 'cell_shift_c') \n",
    "\n",
    "# uncoupled_blocks_2Dy = mts.remove_dimension(uncoupled_blocks_2Dy, 'samples', 'cell_shift_a') \n",
    "# uncoupled_blocks_2Dy = mts.remove_dimension(uncoupled_blocks_2Dy, 'samples', 'cell_shift_b') \n",
    "# uncoupled_blocks_2Dy = mts.remove_dimension(uncoupled_blocks_2Dy, 'samples', 'cell_shift_c') \n",
    "\n",
    "# uncoupled_blocks_2Dz = mts.remove_dimension(uncoupled_blocks_2Dz, 'samples', 'cell_shift_a') \n",
    "# uncoupled_blocks_2Dz = mts.remove_dimension(uncoupled_blocks_2Dz, 'samples', 'cell_shift_b') \n",
    "# uncoupled_blocks_2Dz = mts.remove_dimension(uncoupled_blocks_2Dz, 'samples', 'cell_shift_c') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupled_blocks_2Dx = _to_coupled_basis(uncoupled_blocks_2Dx, skip_symmetry = False, device = 'cpu', translations = None)\n",
    "coupled_blocks_2Dy = _to_coupled_basis(uncoupled_blocks_2Dy, skip_symmetry = False, device = 'cpu', translations = None)\n",
    "coupled_blocks_2Dz = _to_coupled_basis(uncoupled_blocks_2Dz, skip_symmetry = False, device = 'cpu', translations = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a tensor map from these three individually coupled tmaps, concatenating them along the components dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupled_blocks_2Dx = coupled_blocks_2Dx.keys_to_properties(['species_i', 'n_i', 'l_i','species_j', 'n_j', 'l_j'])\n",
    "coupled_blocks_2Dy = coupled_blocks_2Dy.keys_to_properties(['species_i', 'n_i', 'l_i','species_j', 'n_j', 'l_j'])\n",
    "coupled_blocks_2Dz = coupled_blocks_2Dz.keys_to_properties(['species_i', 'n_i', 'l_i','species_j', 'n_j', 'l_j'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncoupled_2 = []\n",
    "for (bx,by,bz) in zip(coupled_blocks_2Dy.blocks(), coupled_blocks_2Dz.blocks(),coupled_blocks_2Dx.blocks()):\n",
    "    ## Assert that the blocks correspond to the same keys\n",
    "    assert bx.values.shape == by.values.shape == bz.values.shape\n",
    "    uncoupled_2.append( TensorBlock( values = torch.stack([bx.values, by.values,bz.values], dim=2), \n",
    "                                    components = [bx.components[0],position_components], \n",
    "                                    samples = bx.samples, \n",
    "                                    properties = bx.properties\n",
    "    ) )\n",
    "newkeys = Labels(coupled_blocks_2Dz.keys.names+['L2'], values = torch.nn.functional.pad(coupled_blocks_2Dz.keys.values, (0,1), mode='constant', value=1))\n",
    "uncoupled_2 = TensorMap(newkeys, uncoupled_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 6 blocks\n",
       "keys: block_type  L  L2\n",
       "          0       0  1\n",
       "          0       1  1\n",
       "          0       2  1\n",
       "          1       0  1\n",
       "          2       0  1\n",
       "          2       1  1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncoupled_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _to_coupled_basis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8360],\n",
       "         [ 1.1375],\n",
       "         [ 0.0626]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupled_blocks[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1, 0, 1): {1: tensor([[[-0.8360,  1.1375,  0.0626]]])}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupled_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.utils.symmetry import ClebschGordanReal\n",
    "CG = ClebschGordanReal(10, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def couple(decoupled, iterate = 0, cg=None, selfdevice= 'cpu', lmax=10):\n",
    "       \n",
    "        coupled = {}\n",
    "\n",
    "        # when called on a matrix, turns it into a dict form to which we can\n",
    "        # apply the generic algorithm\n",
    "        if not isinstance(decoupled, dict):\n",
    "            l2 = (decoupled.shape[-1] - 1) // 2\n",
    "            decoupled = {(): {l2: decoupled}}\n",
    "\n",
    "        # runs over the tuple of (partly) decoupled terms\n",
    "        for ltuple, lcomponents in decoupled.items():\n",
    "            # each is a list of L terms\n",
    "            for lc in lcomponents.keys():\n",
    "                # this is the actual matrix-valued coupled term,\n",
    "                # of shape (..., 2l1+1, 2l2+1), transforming as Y^m1_l1 Y^m2_l2\n",
    "                dec_term = lcomponents[lc]\n",
    "                l1 = (dec_term.shape[-2] - 1) // 2\n",
    "                l2 = (dec_term.shape[-1] - 1) // 2\n",
    "\n",
    "                # there is a certain redundance: the L value is also the last entry\n",
    "                # in ltuple\n",
    "                if lc != l2:\n",
    "                    raise ValueError(\n",
    "                        \"Inconsistent shape for coupled angular momentum block.\"\n",
    "                    )\n",
    "\n",
    "                # in the new coupled term, prepend (l1,l2) to the existing label\n",
    "                device = dec_term.device\n",
    "                if device != selfdevice:\n",
    "                    dec_term = dec_term.to(selfdevice)\n",
    "                \n",
    "                coupled[(l1, l2) + ltuple] = {}\n",
    "                for L in range(\n",
    "                    max(l1, l2) - min(l1, l2), min(lmax, (l1 + l2)) + 1\n",
    "                ):\n",
    "                    # Lterm = torch.einsum('spmn,mnM->spM', dec_term, self._cg[(l1, l2, L)])\n",
    "                    coupled[(l1, l2) + ltuple][L] = torch.tensordot(dec_term, cg._cg[(l1, l2, L)].to(dec_term), dims=2)\n",
    "\n",
    "        # repeat if required\n",
    "        if iterate > 0:\n",
    "            coupled = couple(coupled, iterate - 1, cg= cg, selfdevice=selfdevice,lmax=lmax)\n",
    "        return coupled\n",
    "\n",
    "def decouple(coupled, iterate: int = 0, cg=None, selfdevice= 'cpu', lmax=10 ):\n",
    "        decoupled = {}\n",
    "        # applies the decoupling to each entry in the dictionary\n",
    "        for ltuple, lcomponents in coupled.items():\n",
    "            # the initial pair in the key indicates the decoupled terms that generated\n",
    "            # the L entries\n",
    "            l1, l2 = ltuple[:2]\n",
    "            # shape of the coupled matrix (last entry is the 2L+1 M terms)\n",
    "            # if lcomponents == {}:\n",
    "            #     print(f'here,{ltuple}')\n",
    "            #     continue\n",
    "            shape = next(iter(lcomponents.values())).shape[:-1]\n",
    "            dtype_ = next(iter(lcomponents.values())).dtype\n",
    "\n",
    "            dec_term = torch.zeros(shape+ ( 2 * l1 + 1, 2 * l2 + 1),device=selfdevice, dtype = dtype_)\n",
    "            for L in range(max(l1, l2) - min(l1, l2), min(lmax, (l1 + l2)) + 1):\n",
    "                # supports missing L components, e.g. if they are zero because of symmetry\n",
    "                if L not in lcomponents:\n",
    "                    continue\n",
    "                # decouples the L term into m1, m2 components\n",
    "                # a = torch.einsum('spM,mnM->spmn', lcomponents[L], self._cg[(l1, l2, L)])\n",
    "                # dec_term+=torch.tensordot(lcomponents[L], self._cg[(l1, l2, L)].to(dtype_), dims=([2],[2]))\n",
    "                dec_term+=torch.tensordot(lcomponents[L], cg._cg[(l1, l2, L)].to(dtype_), dims=([-1],[-1])) #CHECK<<<<<< \n",
    "            if not ltuple[2:] in decoupled:\n",
    "                decoupled[ltuple[2:]] = {}\n",
    "            decoupled[ltuple[2:]][l2] = dec_term\n",
    "\n",
    "        # rinse, repeat\n",
    "        if iterate > 0:\n",
    "            decoupled = decouple(decoupled, iterate - 1,cg= cg, selfdevice=selfdevice,lmax=lmax)\n",
    "        # if we got a fully decoupled state, just return an array\n",
    "        if ltuple[2:] == ():\n",
    "            decoupled = next(iter(decoupled[()].values()))\n",
    "        return decoupled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1,5,7,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = couple(a, 1,cg = CG,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = decouple(b, 1, cg = CG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 7, 3])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1898e-06)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(c - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = b.copy()\n",
    "bb[(2, 2, 3, 1)] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = decouple(bb, 1, cg = CG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3, 1)\n",
      "(2, 2, 3, 1) 0\n",
      "torch.Size([2, 6]) torch.Size([1, 1]) torch.Size([1, 1])\n",
      "(2, 2, 3, 1) 1\n",
      "torch.Size([2, 6]) torch.Size([1, 3]) torch.Size([3, 1])\n",
      "(2, 2, 3, 1) 2\n",
      "torch.Size([2, 6]) torch.Size([1, 5]) torch.Size([5, 1])\n",
      "(2, 2, 3, 1) 3\n",
      "torch.Size([2, 6]) torch.Size([1, 7]) torch.Size([7, 1])\n",
      "(2, 2, 3, 1) 4\n",
      "torch.Size([2, 6]) torch.Size([1, 9]) torch.Size([9, 1])\n",
      "(2, 3, 3, 1)\n",
      "(2, 3, 3, 1) 1\n",
      "torch.Size([2, 6]) torch.Size([1, 3]) torch.Size([3, 1])\n",
      "(2, 3, 3, 1) 2\n",
      "torch.Size([2, 6]) torch.Size([1, 5]) torch.Size([5, 1])\n",
      "(2, 3, 3, 1) 3\n",
      "torch.Size([2, 6]) torch.Size([1, 7]) torch.Size([7, 1])\n",
      "(2, 3, 3, 1) 4\n",
      "torch.Size([2, 6]) torch.Size([1, 9]) torch.Size([9, 1])\n",
      "(2, 3, 3, 1) 5\n",
      "torch.Size([2, 6]) torch.Size([1, 11]) torch.Size([11, 1])\n",
      "(2, 4, 3, 1)\n",
      "(2, 4, 3, 1) 2\n",
      "torch.Size([2, 6]) torch.Size([1, 5]) torch.Size([5, 1])\n",
      "(2, 4, 3, 1) 3\n",
      "torch.Size([2, 6]) torch.Size([1, 7]) torch.Size([7, 1])\n",
      "(2, 4, 3, 1) 4\n",
      "torch.Size([2, 6]) torch.Size([1, 9]) torch.Size([9, 1])\n",
      "(2, 4, 3, 1) 5\n",
      "torch.Size([2, 6]) torch.Size([1, 11]) torch.Size([11, 1])\n",
      "(2, 4, 3, 1) 6\n",
      "torch.Size([2, 6]) torch.Size([1, 13]) torch.Size([13, 1])\n"
     ]
    }
   ],
   "source": [
    "coupled = a\n",
    "for coupledkey in coupled:\n",
    "    k = coupledkey[1]\n",
    "    print(coupledkey)\n",
    "    for L in coupled[coupledkey]:\n",
    "        print(coupledkey, L)\n",
    "                    # block_idx = tuple(idx) + (k, L)\n",
    "                    # skip blocks that are zero because of symmetry - TBD \n",
    "                    # if ai == aj and ni == nj and li == lj:\n",
    "                    #     parity = (-1) ** (li + lj + L)\n",
    "                    #     if ((parity == -1 and block_type in (0, 1)) or (parity == 1 and block_type == -1)) and not skip_symmetry:\n",
    "                    #         continue\n",
    "                    \n",
    "        print(block.samples.values.shape, coupled[coupledkey][L].shape, torch.moveaxis(coupled[coupledkey][L], -1, -2).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
