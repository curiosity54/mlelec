{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "162d0473-d1cf-4306-9a2b-d03218bbfe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c52587-7c8f-4b08-8508-8d45b56d9250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "import hickle\n",
    "\n",
    "from ase.io import read\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from mlelec.data.qmdataset import QMDataset\n",
    "from mlelec.data.mldataset import MLDataset\n",
    "from mlelec.models.linear_integrated import LinearModelPeriodic\n",
    "from mlelec.utils.pbc_utils import blocks_to_matrix\n",
    "from mlelec.utils.twocenter_utils import _to_uncoupled_basis, unfix_orbital_order\n",
    "from mlelec.metrics import Eigval_loss, L2_loss_meanzero, L2_loss\n",
    "import metatensor.torch as mts\n",
    "from metatensor.learn import DataLoader\n",
    "\n",
    "import os\n",
    "os.environ[\"PYSCFAD_BACKEND\"] = \"torch\"\n",
    "from pyscf import gto\n",
    "from pyscfad import numpy as pynp\n",
    "from pyscfad import ops\n",
    "from pyscfad.ml.scf import hf\n",
    "import pyscf.pbc.tools.pyscf_ase as pyscf_ase\n",
    "from mlelec.data.pyscf_calculator import _instantiate_pyscf_mol\n",
    "\n",
    "import xitorch\n",
    "from xitorch.linalg import symeig\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.utils import io as ipy_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06d52ddf-8bd5-4385-802a-c533ac4f1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orbitals = {\n",
    "    'sto-3g': {1: [[1,0,0]], \n",
    "               5: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               6: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               7: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]],\n",
    "               8: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]]\n",
    "               }, \n",
    "    \n",
    "    'def2svp': {1: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]],\n",
    "                6: [[1,0,0],[2,0,0],[3,0,0],[2,1,1], [2,1,-1],[2,1,0], [3,1,1], [3,1,-1],[3,1,0], [3,2,-2], [3,2,-1],[3,2,0], [3,2,1],[3,2,2]],\n",
    "                8: [[1,0,0],[2,0,0],[3,0,0],[2,1,1], [2,1,-1],[2,1,0], [3,1,1], [3,1,-1],[3,1,0], [3,2,-2], [3,2,-1],[3,2,0], [3,2,1],[3,2,2]]\n",
    "                },\n",
    "}\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4998d7ce-9947-4561-8ed0-5e1aa203aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_radial  = 12\n",
    "max_angular = 6\n",
    "atomic_gaussian_width = 0.3\n",
    "cutoff = 3.5\n",
    "\n",
    "hypers_pair = {'cutoff': cutoff,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': atomic_gaussian_width,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}}}\n",
    "\n",
    "hypers_atom = {'cutoff': cutoff,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': 0.5,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}}}\n",
    "\n",
    "\n",
    "return_rho0ij = False\n",
    "both_centers = False\n",
    "LCUT = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5d36f-6fea-41d2-8b30-0e05e514b1f4",
   "metadata": {},
   "source": [
    "# QMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d5bfc3-c835-45de-b318-c96150bfafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = '../examples/data/water_1000'\n",
    "every = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6d217-5003-47a7-bfd8-73acd29687cc",
   "metadata": {},
   "source": [
    "For the moment, we need to create multiple QMDataset (analogous to MoleculeDataset), one for the large basis, one for the small one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7188d839-579f-4b16-bab2-e5f78cc033ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmdata = QMDataset.from_file(frames_path = f'{workdir}/water_1000.xyz', \n",
    "                             fock_realspace_path = f'{workdir}/def2svp/focks.npy', \n",
    "                             overlap_realspace_path = f'{workdir}/def2svp/overlaps.npy',\n",
    "                             dimension = 0, \n",
    "                             device = 'cpu', \n",
    "                             orbs_name='def2svp', \n",
    "                             orbs=orbitals['def2svp'], \n",
    "                             frame_slice = f'::{every}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be02c4fe-273e-46b5-9c04-7dae558b62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmdata_sto3G = QMDataset.from_file(frames_path = f'{workdir}/water_1000.xyz', \n",
    "                         dimension = 0,\n",
    "                         fock_realspace_path = f'{workdir}/sto-3g/fock.hickle',\n",
    "                         overlap_realspace_path = f'{workdir}/sto-3g/overlap.hickle',\n",
    "                         device = device, \n",
    "                         orbs = orbitals['sto-3g'], \n",
    "                         orbs_name = 'sto-3g',\n",
    "                         frame_slice = f'::{every}'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd27e2a-cb78-4dad-b2f2-70b6a42c68dc",
   "metadata": {},
   "source": [
    "# MLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af9ffb89-465f-44a3-806a-dd0b244aeba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu pair features\n",
      "cpu single center features\n",
      "cpu single center features\n"
     ]
    }
   ],
   "source": [
    "from mlelec.data.mldataset import MLDataset\n",
    "mldata = MLDataset(qmdata, \n",
    "                   item_names = ['fock_blocks', 'fock_realspace', 'overlap_realspace', 'eigenvalues', 'atom_resolved_density'],\n",
    "                   # features = mldata.features,\n",
    "                   cutoff = hypers_pair['cutoff'],\n",
    "                   hypers_atom = hypers_atom,\n",
    "                   hypers_pair = hypers_pair,\n",
    "                   lcut = 3,\n",
    "                   train_frac = 0.7,\n",
    "                   val_frac = 0.2,\n",
    "                   test_frac = 0.1,\n",
    "                   shuffle = False,\n",
    "                   model_basis = orbitals['sto-3g'],\n",
    "                   fix_p_orbital_order = True,\n",
    "                   aux_overlap = qmdata_sto3G.overlap_realspace\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dca2cf-2183-4b8e-820e-5a10e9e5a8b5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abada533-7c2b-423c-bd6c-46516fc8fe74",
   "metadata": {},
   "source": [
    "Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4873e8-95f2-4fe6-8524-a135e1f2d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_random_seed(seed):\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     # random.seed(seed)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed(seed)\n",
    "#         torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f9fed-8da2-4a2d-a300-6c9d54c0b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlelec.models.equivariant_nonlinear_model import EquivariantNonlinearModel\n",
    "# seed = 0\n",
    "# set_random_seed(seed)\n",
    "\n",
    "# model = EquivariantNonlinearModel(mldata, device = device, nhidden = 4, nlayers = 1, activation = 'SiLU', apply_norm = True)\n",
    "# model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9aa9e-f265-45b1-95c9-1c1b942c0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 0\n",
    "# set_random_seed(0)\n",
    "\n",
    "# model_old = LinearModelPeriodic(twocfeat = mldata.features, \n",
    "#                             target_blocks = mldata.model_metadata,\n",
    "#                             frames = mldata.structures, \n",
    "#                             orbitals = mldata.model_basis, \n",
    "#                             device = device,\n",
    "#                             nhidden = 4, \n",
    "#                             nlayers = 1,\n",
    "#                             activation = 'SiLU',\n",
    "#                            apply_norm = True\n",
    "#                            )\n",
    "# model_old = model_old.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "002d5e14-e65d-488c-8155-2f9107ec2222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlelec.models.equivariant_nonlinear_lightning import LitEquivariantNonlinearModel, MSELoss\n",
    "from mlelec.models.equivariant_nonlinear_lightning import MLDatasetDataModule\n",
    "from mlelec.callbacks.logging import LoggingCallback\n",
    "from mlelec.callbacks.progress_bar import ProgressBar\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "# Assuming you have `train_dataset`, `val_dataset`, and `test_dataset`\n",
    "data_module = MLDatasetDataModule(mldata, batch_size=2)\n",
    "\n",
    "# Initialize with custom loss function and keyword arguments for derived predictions\n",
    "\n",
    "model = LitEquivariantNonlinearModel(\n",
    "    mldata=mldata,\n",
    "    nhidden=4,\n",
    "    nlayers=1,\n",
    "    activation='ReLU',\n",
    "    apply_norm=True,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=MSELoss(),\n",
    "    is_indirect = True,\n",
    "    eigenvalues = True,\n",
    "    atom_resolved_density = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "199332cf-de07-46eb-b83d-ee2873546425",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f8f5634-a98d-4e74-8951-830201e7b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.00,\n",
    "    patience=100,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62a8a4a1-432c-472d-a2c1-c869c1dc82c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = ProgressBar()\n",
    "logger_callback = LoggingCallback(log_every_n_epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da403782-fee8-4f4b-a8eb-27a71f8b9ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=100, \n",
    "                     accelerator='cpu', \n",
    "                     # logger = logger, \n",
    "                     # log_every_n_steps = 1, \n",
    "                     check_val_every_n_epoch=10,\n",
    "                     callbacks=[early_stopping, progress_bar]\n",
    "                     # callbacks=[early_stopping, logger_callback],\n",
    "                     # enable_progress_bar=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62d396bf-f399-4855-99cd-30dea65ddaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                      | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | model | EquivariantNonlinearModel | 126 K  | train\n",
      "------------------------------------------------------------\n",
      "126 K     Trainable params\n",
      "0         Non-trainable params\n",
      "126 K     Total params\n",
      "0.507     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96eed1dac1f4f21a7f4d9fc2335df52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The shape of A & M must match (A: torch.Size([7, 7]), M: torch.Size([24, 24]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1028\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1028\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1057\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1057\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 311\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    314\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/models/equivariant_nonlinear_lightning.py:167\u001b[0m, in \u001b[0;36mLitEquivariantNonlinearModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    165\u001b[0m targets \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mfock_blocks\n\u001b[1;32m    166\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m--> 167\u001b[0m derived_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_derived_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mderived_pred_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m derived_metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    170\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/models/equivariant_nonlinear_lightning.py:229\u001b[0m, in \u001b[0;36mLitEquivariantNonlinearModel.compute_derived_predictions\u001b[0;34m(self, predictions, batch, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m target_eigenvalues \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meigenvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_atom_resolved_density \u001b[38;5;129;01mor\u001b[39;00m target_eigenvalues:\n\u001b[0;32m--> 229\u001b[0m     eigsys \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_eigenvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_eigenvectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_atom_resolved_density\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target_atom_resolved_density:\n\u001b[1;32m    231\u001b[0m         eigenvalues, eigenvectors \u001b[38;5;241m=\u001b[39m eigsys\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/data/derived_properties.py:22\u001b[0m, in \u001b[0;36mcompute_eigenvalues\u001b[0;34m(As, Ms, return_eigenvectors)\u001b[0m\n\u001b[1;32m     20\u001b[0m Ax \u001b[38;5;241m=\u001b[39m xitorch\u001b[38;5;241m.\u001b[39mLinearOperator\u001b[38;5;241m.\u001b[39mm(A[index])\n\u001b[1;32m     21\u001b[0m Mx \u001b[38;5;241m=\u001b[39m xitorch\u001b[38;5;241m.\u001b[39mLinearOperator\u001b[38;5;241m.\u001b[39mm(M[index]) \u001b[38;5;28;01mif\u001b[39;00m M \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m eigvals, eigvecs \u001b[38;5;241m=\u001b[39m \u001b[43msymeig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m eigenvalues[index] \u001b[38;5;241m=\u001b[39m eigvals\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_eigenvectors:\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/xitorch/linalg/symeig.py:110\u001b[0m, in \u001b[0;36msymeig\u001b[0;34m(A, neig, mode, M, bck_options, method, **fwd_options)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m M \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     assert_runtime(M\u001b[38;5;241m.\u001b[39mis_hermitian, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe linear operator M must be Hermitian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m     \u001b[43massert_runtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe shape of A & M must match (A: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m, M: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     assert_runtime(\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled() \u001b[38;5;129;01mor\u001b[39;00m M\u001b[38;5;241m.\u001b[39mis_getparamnames_implemented,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe _getparamnames(self, prefix) of linear operator M must be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimplemented if using symeig with grad enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m mode \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/xitorch/_utils/assertfuncs.py:17\u001b[0m, in \u001b[0;36massert_runtime\u001b[0;34m(cond, msg)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_runtime\u001b[39m(cond, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cond:\n\u001b[0;32m---> 17\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The shape of A & M must match (A: torch.Size([7, 7]), M: torch.Size([24, 24]))"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14631e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8e7b7-6fec-4618-b8b6-f0382ace52e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a25b2-8f0a-4273-852c-6bdd33efc7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8321152c-90ff-4dcd-9d28-3a3cb9e5a9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f30c8-9431-4280-b8a9-2f3e9bbf3dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d40498-a172-41b3-bc81-4678f7ca43d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e47ef-7f5f-4e37-99ce-96897c4df13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8260a5-e9d6-43a7-bfef-7a1a92abbee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6b4e0-14a2-4529-ac0a-060b049c401c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79289546-d87d-43fd-a738-96b974ebb5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "201951f7-263e-45c9-a63b-db0718fb89dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 24, 24])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.overlap_realspace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8110df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = mts.learn.DataLoader(mldata.train_dataset, batch_size = 2, collate_fn=mldata.group_and_join)\n",
    "batch = next(iter(dl))\n",
    "# pred = model.forward(batch.features, mldata.model_metadata)\n",
    "# MSELoss().compute(pred, batch.fock_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b675a52-d827-434f-8bbc-797befc39cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-3, weight_decay = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058f772-1313-49cd-bb3d-24b8f5bd7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N. of eigenvalues to match (this needs to be adapted for diverse datasets\n",
    "n_eig_to_match = 5\n",
    "dl = mts.learn.DataLoader(mldata.train_dataset, batch_size = 16, collate_fn=mldata.group_and_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b28a8-9604-455f-a645-f5754a1a9bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nepoch = 1000\n",
    "nevery = 10\n",
    "losses = []\n",
    "losses_e = []\n",
    "losses_ard = []\n",
    "losses_evec = []\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_e = 0\n",
    "    epoch_loss_evec = 0\n",
    "    epoch_loss_ard = 0\n",
    "    eig_sum = 0\n",
    "    \n",
    "\n",
    "    for ib, batch in enumerate(dl):\n",
    "        \n",
    "        model.train(True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model.forward(batch.features, mldata.model_metadata)\n",
    "\n",
    "        loss = L2_loss(pred,)\n",
    "        # HT = blocks_to_matrix(pred, qmdata_sto3G, detach = False)\n",
    "        # HT = [HT[i][0,0,0] for i in batch.sample_id] # Required for now\n",
    "\n",
    "        # pred_eigvals, pred_ard, pred_C, _ = compute_ard_vec(qmdata_sto3G, batch, HT, device, overlap = [qmdata_sto3G.overlap_realspace[i] for i in batch.sample_id])\n",
    "\n",
    "        # loss_e = Eigval_loss(pred_eigvals[:, :n_eig_to_match], batch.eigenvalues[:, :n_eig_to_match])\n",
    "        \n",
    "        # loss_ard = torch.sum((pred_ard - batch.atom_resolved_density)**2)\n",
    "        \n",
    "        # pred_ev_0 = torch.norm(pred_C[:, :, :n_eig_to_match], dim = (1))\n",
    "        # targ_ev_0 = torch.norm(batch.eigenvectors[:, :, :n_eig_to_match], dim = (1))\n",
    "        # loss_evec = torch.sum((pred_ev_0 - targ_ev_0)**2)\n",
    "        \n",
    "        # loss = loss_ard + loss_e + loss_evec\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss_e += loss_e.item()\n",
    "        epoch_loss_evec += loss_evec.item()\n",
    "        epoch_loss_ard += loss_ard.item()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    scheduler.step(epoch_loss)\n",
    "    losses.append(epoch_loss)\n",
    "    losses_e.append(epoch_loss_e)\n",
    "    losses_evec.append(epoch_loss_evec)\n",
    "    losses_ard.append(epoch_loss_ard)\n",
    "    \n",
    "    if epoch % nevery == 0:\n",
    "        print(f\"Epoch {epoch:>7d}, train loss {epoch_loss:>15.10f}; rmse_eig={np.sqrt(epoch_loss_e/5/160):>12.10f} rmse_evec={np.sqrt(epoch_loss_evec/5/160):>12.10f} rmse_ard={np.sqrt(epoch_loss_ard/160/9):>12.10f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04666d44-78fb-44ef-b27e-728d3511f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "mldata.model_basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b60fd-d959-4820-b030-96042b1f5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.forward(batch.features, mldata.model_metadata)\n",
    "HT = blocks_to_matrix(pred, qmdata_sto3G, detach = False)\n",
    "HT = [HT[i][0,0,0] for i in batch.sample_id] # Required for now\n",
    "\n",
    "pred_eigvals, pred_ard, pred_C, _ = compute_ard_vec(qmdata_sto3G, batch, HT, device, overlap = [qmdata_sto3G.overlap_realspace[i] for i in batch.sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8db17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_old(batch.features, mldata.model_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf9b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.fock_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b83a1-a15b-48ee-8a26-519e37636e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.atom_resolved_density.shape)\n",
    "print(pred_ard.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58876e0-efb2-4174-8380-331772bcf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(mldata.test_dataset, batch_size = len(mldata.test_dataset), collate_fn=mldata.group_and_join)\n",
    "train_dl = DataLoader(mldata.train_dataset, batch_size = len(mldata.train_dataset), collate_fn=mldata.group_and_join)\n",
    "\n",
    "fig_e, ax_e = plt.subplots()\n",
    "fig_a, ax_a = plt.subplots()\n",
    "fig_evec, ax_evec = plt.subplots()\n",
    "\n",
    "data = {}\n",
    "for dl_, lbl in zip([train_dl, test_dl], ['train', 'test']):\n",
    "    batch = next(iter(dl_))\n",
    "    pred = model(batch.features, mldata.model_metadata)\n",
    "\n",
    "    HT = blocks_to_matrix(pred, qmdata, detach = True)\n",
    "    HT = [HT[i][0,0,0] for i in batch.sample_id]\n",
    "    \n",
    "    pred_eigvals, pred_ard, pred_eigvec, pred_rho = compute_ard_vec(qmdata, batch, HT, device, overlap = [qmdata.overlap_realspace[i] for i in batch.sample_id])\n",
    "\n",
    "    ax_e.plot(batch.eigenvalues[:,:n_eig_to_match].flatten(), pred_eigvals[:,:n_eig_to_match].detach().flatten(), '.', label = lbl)\n",
    "    ax_e.plot([-21, 2], [-21, 2], 'k')\n",
    "    ax_e.set_title('Eigenvalues')\n",
    "    ax_e.legend()\n",
    "\n",
    "    ax_a.plot(batch.atom_resolved_density.flatten(), pred_ard.detach().flatten(), '.', label = lbl)\n",
    "    ax_a.plot([0,7], [0,7], 'k')\n",
    "    ax_a.set_title('Mayer bond charges')\n",
    "    ax_a.legend()\n",
    "\n",
    "    pred_evn, targ_evn = torch.norm(pred_eigvec[:, :, :n_eig_to_match], dim = (1)), torch.norm(batch.eigenvectors[:, :, :n_eig_to_match], dim = (1))\n",
    "    \n",
    "    ax_evec.plot(targ_evn.flatten(), pred_evn.detach().flatten(), '.', label = lbl)\n",
    "    xmin, xmax = ax_evec.get_xlim()\n",
    "    ymin, ymax = ax_evec.get_ylim()\n",
    "    xmin = np.min([xmin,ymin])\n",
    "    xmax = np.max([xmax,ymax])\n",
    "    ax_evec.plot([xmin,xmax], [xmin,xmax], 'k')\n",
    "    ax_evec.set_title('evec')\n",
    "    ax_evec.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1617430a-3515-4204-aa44-c9b8d0029e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dl))\n",
    "print(batch.sample_id)\n",
    "dl_frames = [qmdata.structures[A] for A in batch.sample_id]\n",
    "\n",
    "pred = model(batch.features, mldata.model_metadata)\n",
    "\n",
    "HT = blocks_to_matrix(pred, qmdata, detach = True)\n",
    "HT = [HT[i][0,0,0] for i in batch.sample_id]\n",
    "\n",
    "fock_predictions = torch.stack(HT)\n",
    "\n",
    "fock_predictions = unfix_orbital_order(\n",
    "    fock_predictions,\n",
    "    dl_frames,\n",
    "    qmdata_sto3G.basis,\n",
    ")\n",
    "\n",
    "fock_targets = unfix_orbital_order(\n",
    "    batch.fock_realspace,\n",
    "    dl_frames,\n",
    "    mldata.qmdata.basis,\n",
    ")\n",
    "\n",
    "fock_sto3g = unfix_orbital_order(\n",
    "    qmdata_sto3G.fock_realspace,\n",
    "    dl_frames,\n",
    "    qmdata_sto3G.basis,\n",
    ")\n",
    "\n",
    "over_large = unfix_orbital_order(\n",
    "    batch.overlap_realspace,\n",
    "    dl_frames,\n",
    "    mldata.qmdata.basis,\n",
    ")\n",
    "\n",
    "over_small = unfix_orbital_order(\n",
    "    torch.stack([qmdata_sto3G.overlap_realspace[i] for i in batch.sample_id]),\n",
    "    dl_frames,\n",
    "    qmdata_sto3G.basis,\n",
    ")\n",
    "\n",
    "with ipy_io.capture_output():\n",
    "    dipole_targets = compute_dipole_moment(\n",
    "        dl_frames,\n",
    "        fock_targets,\n",
    "        over_large,\n",
    "        qmdata.basis_name\n",
    "    )\n",
    "\n",
    "with ipy_io.capture_output():\n",
    "    dipole_predictions = compute_dipole_moment(\n",
    "        dl_frames,\n",
    "        fock_predictions,\n",
    "        over_small,\n",
    "        qmdata_sto3G.basis_name\n",
    "    )\n",
    "\n",
    "with ipy_io.capture_output():\n",
    "    dipole_sto3g = compute_dipole_moment(\n",
    "        dl_frames,\n",
    "        fock_sto3g,\n",
    "        over_small,\n",
    "        qmdata_sto3G.basis_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60168fff-f183-4a70-b20c-daa9987beee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.units import Bohr, Debye\n",
    "au_to_debye = Bohr/Debye\n",
    "\n",
    "ms = 12\n",
    "mew = .7\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = dipole_targets.flatten().detach().cpu() * au_to_debye\n",
    "y_sto3g = dipole_sto3g.flatten().detach().cpu() * au_to_debye\n",
    "y_ml = dipole_predictions.flatten().detach().cpu() * au_to_debye\n",
    "\n",
    "rmse_sto3g = np.sqrt(torch.mean(y_sto3g - x)**2)\n",
    "rmse_ml = np.sqrt(torch.mean(y_ml - x)**2)\n",
    "\n",
    "ax.plot(x, y_sto3g, 'v', \n",
    "         markeredgewidth = mew,\n",
    "         markeredgecolor = 'k',\n",
    "         markersize = ms, \n",
    "         label = 'STO-3G', \n",
    "         alpha = 1)\n",
    "\n",
    "ax.plot(x, y_ml, 'o',\n",
    "         markeredgewidth = mew,\n",
    "         markeredgecolor = 'k',\n",
    "         markersize = ms, \n",
    "         label = 'ML', \n",
    "         alpha = 1)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "xm, xM = ax.get_xlim()\n",
    "ym, yM = ax.get_ylim()\n",
    "m = np.min([xm,ym])\n",
    "M = np.max([xM,yM])\n",
    "ax.plot([m,M], [m,M], '--k')\n",
    "ax.set_xlim(m, M)\n",
    "ax.set_ylim(m, M)\n",
    "\n",
    "ax.text(0.6, 0.3, fr'$\\mathrm{{RMSE_{{\\text{{STO-3G}}}}}}={rmse_sto3g:.3f}\\,$D', transform = ax.transAxes, ha = 'left')\n",
    "ax.text(0.6, 0.25, f'$\\mathrm{{RMSE_{{ML}}}}={rmse_ml:.3f}\\,$D', transform = ax.transAxes, ha = 'left')\n",
    "\n",
    "ax.set_xlabel('Target dipoles (D)')\n",
    "ax.set_ylabel('Predicted dipoles (D)')\n",
    "\n",
    "# ax.set_title('Indirect training from def2-svp to a STO-3G-like model.\\nTargets: eigenvalues; ARD; eigenvector norms over AOs\\nTest set contains 50 water molecules')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
