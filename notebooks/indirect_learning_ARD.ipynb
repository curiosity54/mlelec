{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "162d0473-d1cf-4306-9a2b-d03218bbfe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c52587-7c8f-4b08-8508-8d45b56d9250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "import hickle\n",
    "\n",
    "from ase.io import read\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from mlelec.data.qmdataset import QMDataset\n",
    "from mlelec.data.mldataset import MLDataset\n",
    "from mlelec.models.linear_integrated import LinearModelPeriodic\n",
    "from mlelec.utils.pbc_utils import blocks_to_matrix\n",
    "from mlelec.utils.twocenter_utils import _to_uncoupled_basis, unfix_orbital_order\n",
    "from mlelec.metrics import Eigval_loss, L2_loss_meanzero, L2_loss\n",
    "import metatensor.torch as mts\n",
    "from metatensor.learn import DataLoader\n",
    "\n",
    "import os\n",
    "os.environ[\"PYSCFAD_BACKEND\"] = \"torch\"\n",
    "from pyscf import gto\n",
    "from pyscfad import numpy as pynp\n",
    "from pyscfad import ops\n",
    "from pyscfad.ml.scf import hf\n",
    "import pyscf.pbc.tools.pyscf_ase as pyscf_ase\n",
    "from mlelec.data.pyscf_calculator import _instantiate_pyscf_mol\n",
    "\n",
    "import xitorch\n",
    "from xitorch.linalg import symeig\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.utils import io as ipy_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ea695542-9d6e-41a1-9494-42792b315e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metatensor.learn import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aaccef1-cad2-4284-9537-af28fa5b67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigval_vec(dataset, batch, Hk, return_rho = False, return_eigenvectors = False):\n",
    "    eig = []\n",
    "    rho = []\n",
    "    eigvec = []\n",
    "    for A, H, S in zip(batch.sample_id, Hk, batch.overlap_realspace):\n",
    "        # Compute eigenvalues and eigenvectors\n",
    "        # eigvals, eigvecs = symeig(Ax, M = Mx)\n",
    "        eigvals, C = symeig(xitorch.LinearOperator.m(H), M = xitorch.LinearOperator.m(S))\n",
    "        if return_rho:\n",
    "            frame = dataset.structures[A]\n",
    "            natm = len(frame)\n",
    "            ncore = sum(dataset.ncore[s] for s in frame.numbers)\n",
    "            nelec = sum(frame.numbers) - ncore\n",
    "            occ = torch.tensor([2.0 if i < nelec//2 else 0.0 for i in range(C.shape[-1])], dtype = torch.float64, requires_grad = True, device = device)\n",
    "            # occ = torch.tensor([2.0 for i in range(C.shape[-1])], dtype = torch.float64, requires_grad = True, device = device)\n",
    "            rho.append(torch.einsum('n,...in,...jn->ij...', occ, C, C.conj()))\n",
    "        eig.append(eigvals)\n",
    "        if return_eigenvectors:\n",
    "            eigvec.append(C)\n",
    "\n",
    "    to_return = [eig]\n",
    "    if return_rho:\n",
    "        try:\n",
    "            rho = torch.stack(rho)\n",
    "        except:\n",
    "            pass\n",
    "        to_return.append(rho)\n",
    "    if return_eigenvectors:\n",
    "        try:\n",
    "            eigvec = torch.stack(eigvec)\n",
    "        except:\n",
    "            pass\n",
    "        to_return.append(eigvec)\n",
    "    return tuple(to_return)\n",
    "\n",
    "def compute_ard_vec(dataset, batch, HT, device, overlap = None):\n",
    "    basis = dataset.basis\n",
    "    ard_ = []\n",
    "    eig = []\n",
    "    Cs = []\n",
    "    rhos = []\n",
    "\n",
    "    overlap = batch.overlap_realspace if overlap is None else overlap\n",
    "    \n",
    "    for A, H, S in zip(batch.sample_id, HT, overlap):\n",
    "        frame = dataset.structures[A]\n",
    "        natm = len(frame)\n",
    "        ncore = sum(dataset.ncore[s] for s in frame.numbers)\n",
    "        nelec = sum(frame.numbers) - ncore\n",
    "        split_idx = [len(basis[s]) for s in frame.numbers]\n",
    "        needed = True if len(np.unique(split_idx)) > 1 else False\n",
    "        \n",
    "        max_dim = np.max(split_idx)\n",
    "        \n",
    "        eigvals, C = symeig(xitorch.LinearOperator.m(H), M=xitorch.LinearOperator.m(S), return_eigenvectors = True) # Has shape = (n_k, N, N)\n",
    "        \n",
    "        occ = torch.tensor([2.0 if i < nelec//2 else 0.0 for i in range(C.shape[-1])], dtype = torch.float64, requires_grad = True, device = device)\n",
    "        P = torch.einsum('n,...in,...jn->ij...', occ, C, C.conj())\n",
    "        rhos.append(P)\n",
    "        \n",
    "        slices = torch.split(P, split_idx, dim=0)\n",
    "        blocks = [torch.split(slice_, split_idx, dim=1) for slice_ in slices]\n",
    "        blocks_flat = [block for sublist in blocks for block in sublist]\n",
    "        \n",
    "        if needed:\n",
    "            squared_blocks = []\n",
    "            for block in blocks_flat:\n",
    "                pad_size = (0, max_dim - block.size(1), 0, max_dim - block.size(0))\n",
    "                squared_block = torch.nn.functional.pad(block, pad_size, \"constant\", 0)\n",
    "                squared_blocks.append(squared_block)\n",
    "            blocks_flat = squared_blocks\n",
    "\n",
    "\n",
    "        ard_.append(torch.stack(blocks_flat).norm(dim=(1,2)))\n",
    "        eig.append(eigvals)\n",
    "        Cs.append(C)\n",
    "\n",
    "    try:\n",
    "        ard_ = torch.stack(ard_)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        eig = torch.stack(eig)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        rhos = torch.stack(rhos)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        Cs = torch.stack(Cs)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return eig, ard_, Cs, rhos\n",
    "\n",
    "def compute_dipole_moment(frames, fock_predictions, overlaps, basis = 'sto-3g'):\n",
    "    assert (\n",
    "        len(frames) == len(fock_predictions) == len(overlaps)\n",
    "    ), \"Length of frames, fock_predictions, and overlaps must be the same\"\n",
    "    dipoles = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        mol = _instantiate_pyscf_mol(frame, basis = basis)\n",
    "        mf = hf.SCF(mol)\n",
    "        fock = torch.autograd.Variable(\n",
    "            fock_predictions[i].type(torch.float64), requires_grad=True\n",
    "        )\n",
    "\n",
    "        mo_energy, mo_coeff = mf.eig(fock, overlaps[i])\n",
    "        mo_occ = mf.get_occ(mo_energy)  # get_occ returns a numpy array\n",
    "        mo_occ = ops.convert_to_tensor(mo_occ)\n",
    "        dm1 = mf.make_rdm1(mo_coeff, mo_occ)\n",
    "        dip = mf.dip_moment(dm=dm1)\n",
    "        dipoles.append(dip)\n",
    "    return torch.stack(dipoles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "06d52ddf-8bd5-4385-802a-c533ac4f1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orbitals = {\n",
    "    'sto-3g': {1: [[1,0,0]], \n",
    "               5: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               6: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               7: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]],\n",
    "               8: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]]\n",
    "               }, \n",
    "    \n",
    "    'def2svp': {1: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]],\n",
    "                6: [[1,0,0],[2,0,0],[3,0,0],[2,1,1], [2,1,-1],[2,1,0], [3,1,1], [3,1,-1],[3,1,0], [3,2,-2], [3,2,-1],[3,2,0], [3,2,1],[3,2,2]],\n",
    "                8: [[1,0,0],[2,0,0],[3,0,0],[2,1,1], [2,1,-1],[2,1,0], [3,1,1], [3,1,-1],[3,1,0], [3,2,-2], [3,2,-1],[3,2,0], [3,2,1],[3,2,2]]\n",
    "                },\n",
    "}\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "4998d7ce-9947-4561-8ed0-5e1aa203aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_radial  = 12\n",
    "max_angular = 6\n",
    "atomic_gaussian_width = 0.3\n",
    "cutoff = 3.5\n",
    "\n",
    "hypers_pair = {'cutoff': cutoff,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': atomic_gaussian_width,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}}}\n",
    "\n",
    "hypers_atom = {'cutoff': cutoff,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': 0.5,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}}}\n",
    "\n",
    "\n",
    "return_rho0ij = False\n",
    "both_centers = False\n",
    "LCUT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61d5bfc3-c835-45de-b318-c96150bfafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = '../examples/data/water_1000'\n",
    "every = 50\n",
    "frames = read(f'{workdir}/water_1000.xyz', f'::{every}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6d217-5003-47a7-bfd8-73acd29687cc",
   "metadata": {},
   "source": [
    "For the moment, we need to create multiple QMDataset (analogous to MoleculeDataset), one for the large basis, one for the small one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e3b2cb6-7c1f-416c-aaf7-7f3ebdaabcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fock = torch.from_numpy(np.load(f'{workdir}/def2svp/focks.npy', allow_pickle = True)[::every].astype(np.float64))\n",
    "over = torch.from_numpy(np.load(f'{workdir}/def2svp/overlaps.npy', allow_pickle=True)[::every].astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "7188d839-579f-4b16-bab2-e5f78cc033ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmdata = QMDataset.from_file(frames_path = f'{workdir}/water_1000.xyz', fock_realspace_path = f'{workdir}/def2svp/focks.npy', overlap_realspace_path = f'{workdir}/def2svp/overlaps.npy',\n",
    "                             dimension = 0, device = 'cpu', orbs_name='def2svp', orbs=orbitals['def2svp'], frame_slice = f'::{every}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be02c4fe-273e-46b5-9c04-7dae558b62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fock_small = hickle.load(f'{workdir}/sto-3g/fock.hickle')[::every]\n",
    "over_small = hickle.load(f'{workdir}/sto-3g/overlap.hickle')[::every]\n",
    "\n",
    "qmdata_sto3G = QMDataset(frames = frames, \n",
    "                         dimension = 0,\n",
    "                         fock_realspace=fock_small.clone(),\n",
    "                         overlap_realspace=over_small.clone(),\n",
    "                         device = device, \n",
    "                         orbs = orbitals['sto-3g'], \n",
    "                         orbs_name = 'sto-3g'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "af9ffb89-465f-44a3-806a-dd0b244aeba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.data.mldataset import MLDataset\n",
    "mldata = MLDataset(qmdata, \n",
    "                   item_names = ['fock_blocks', 'fock_realspace', 'overlap_realspace', 'eigenvalues', 'atom_resolved_density'],\n",
    "                   features = './features', #mldata.features,\n",
    "                   cutoff = hypers_pair['cutoff'],\n",
    "                   hypers_atom = hypers_atom,\n",
    "                   hypers_pair = hypers_pair,\n",
    "                   lcut = 3,\n",
    "                   orbitals_to_properties = True,\n",
    "                   train_frac = 1,\n",
    "                   shuffle = False,\n",
    "                   model_basis = orbitals['sto-3g'],\n",
    "                   fix_p_orbital_order=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93531df-4e44-4dd8-8136-0d3595f342c5",
   "metadata": {},
   "source": [
    "The following cell is required to compute the metadata used to initialize the model (i.e., the model's submodels info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f3b0e4-6227-46ae-b476-0b9ddc6db7c6",
   "metadata": {},
   "source": [
    "Instantiate the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0391ccbe-8be6-4084-991f-e1ab64a0f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(mldata.train_dataset, batch_size = 10, collate_fn = mldata.group_and_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dca2cf-2183-4b8e-820e-5a10e9e5a8b5",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abada533-7c2b-423c-bd6c-46516fc8fe74",
   "metadata": {},
   "source": [
    "Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "db4873e8-95f2-4fe6-8524-a135e1f2d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "092f9fed-8da2-4a2d-a300-6c9d54c0b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.models.equivariant_nonlinear_model import EquivariantNonLinearity\n",
    "seed = 0\n",
    "set_random_seed(seed)\n",
    "\n",
    "model = EquivariantNonlinearModel(mldata, device = device, nhidden = 4, nlayers = 1, activation = 'SiLU', apply_norm = True)\n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6ab9aa9e-f265-45b1-95c9-1c1b942c0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "set_random_seed(0)\n",
    "\n",
    "model_old = LinearModelPeriodic(twocfeat = mldata.features, \n",
    "                            target_blocks = mldata.model_metadata,\n",
    "                            frames = mldata.structures, \n",
    "                            orbitals = mldata.model_basis, \n",
    "                            device = device,\n",
    "                            nhidden = 4, \n",
    "                            nlayers = 1,\n",
    "                            activation = 'SiLU',\n",
    "                           apply_norm = True\n",
    "                           )\n",
    "model_old = model_old.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "7e305f15-2a36-4897-9dd6-d4293e64be48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlelec.models.equivariant_nonlinear_lightning import LitEquivariantNonlinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "002d5e14-e65d-488c-8155-2f9107ec2222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "from mlelec.models.equivariant_nonlinear_lightning import LitEquivariantNonlinearModel, MSELoss\n",
    "from mlelec.models.equivariant_nonlinear_lightning import MLDatasetDataModule\n",
    "\n",
    "# Assuming you have `train_dataset`, `val_dataset`, and `test_dataset`\n",
    "data_module = MLDatasetDataModule(mldata, batch_size=16)\n",
    "\n",
    "# Initialize with custom loss function and keyword arguments for derived predictions\n",
    "\n",
    "model = LitEquivariantNonlinearModel(\n",
    "    mldata=mldata,\n",
    "    nhidden=16,\n",
    "    nlayers=2,\n",
    "    activation='SiLU',\n",
    "    apply_norm=True,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=MSELoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "5c57a4e8-f7e2-41ab-be0c-d40cda602dde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                      | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | model | EquivariantNonlinearModel | 509 K  | train\n",
      "------------------------------------------------------------\n",
      "509 K     Trainable params\n",
      "0         Non-trainable params\n",
      "509 K     Total params\n",
      "2.037     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baae2eb27f51482da86101c218fbaa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "could not find blocks matching the selection (block_type=0, inversion_sigma=1, species_center=8, species_neighbor=8, spherical_harmonics_l=4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[296], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model, data_module)\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1028\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1028\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1057\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1057\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 311\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    314\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/models/equivariant_nonlinear_lightning.py:92\u001b[0m, in \u001b[0;36mLitEquivariantNonlinearModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     90\u001b[0m features \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[1;32m     91\u001b[0m targets \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mfock_blocks\n\u001b[0;32m---> 92\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m derived_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_derived_predictions(predictions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mderived_pred_kwargs)\n\u001b[1;32m     94\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn\u001b[38;5;241m.\u001b[39mcompute(predictions, targets, derived_predictions\u001b[38;5;241m=\u001b[39mderived_predictions)\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/models/equivariant_nonlinear_lightning.py:78\u001b[0m, in \u001b[0;36mLitEquivariantNonlinearModel.forward\u001b[0;34m(self, features, target_blocks, return_matrix)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features, target_blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, return_matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_matrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/models/equivariant_nonlinear_model.py:245\u001b[0m, in \u001b[0;36mEquivariantNonlinearModel.forward\u001b[0;34m(self, features, target_blocks, return_matrix)\u001b[0m\n\u001b[1;32m    243\u001b[0m feat_blocks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[0;32m--> 245\u001b[0m     feat_blocks\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmap_targetkeys_to_featkeys_integrated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    246\u001b[0m feat_map \u001b[38;5;241m=\u001b[39m mts\u001b[38;5;241m.\u001b[39mTensorMap(keys, feat_blocks)\n\u001b[1;32m    247\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(feat_map)\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/utils/twocenter_utils.py:1106\u001b[0m, in \u001b[0;36mmap_targetkeys_to_featkeys_integrated\u001b[0;34m(features, key, cell_shift, return_key)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_key:\n\u001b[1;32m   1085\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m labels_where(\n\u001b[1;32m   1086\u001b[0m             features\u001b[38;5;241m.\u001b[39mkeys,\n\u001b[1;32m   1087\u001b[0m             Labels(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1104\u001b[0m             ),\n\u001b[1;32m   1105\u001b[0m         )\n\u001b[0;32m-> 1106\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspherical_harmonics_l\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43minversion_sigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minversion_sigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspecies_center\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecies_center\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspecies_neighbor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecies_neighbor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m block\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: could not find blocks matching the selection (block_type=0, inversion_sigma=1, species_center=8, species_neighbor=8, spherical_harmonics_l=4)"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10, accelerator='cpu')\n",
    "trainer.fit(model, data_module)\n",
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5c6f697a-aa24-45bd-a7ca-01f3f211f9bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for old_param, new_param in zip(model.parameters(), model_old.parameters()):\n",
    "        print(torch.equal(old_param, new_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "df7bf460-5521-4e35-bcba-6569d0bb13cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/Software/mlelec/src/mlelec/models/equivariant_nonlinear_model.py:300: UserWarning: Using training target_blocks; provide test target_blocks for inference.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mts.equal(model.forward(mldata.features), model_old.forward(mldata.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "548f5a3a-1139-49a5-9a32-1f57881a720a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.73 ms ± 289 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a=model.forward(mldata.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "beb2187a-ee13-422d-b569-f8dfd08d4f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 ms ± 442 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "b=model_old.forward(mldata.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9b675a52-d827-434f-8bbc-797befc39cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-3, weight_decay = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2058f772-1313-49cd-bb3d-24b8f5bd7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N. of eigenvalues to match (this needs to be adapted for diverse datasets\n",
    "n_eig_to_match = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6c1b28a8-9604-455f-a645-f5754a1a9bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/Software/mlelec/src/mlelec/models/equivariant_nonlinear_model.py:191: UserWarning: Using training target_blocks; provide test target_blocks for inference.\n",
      "  warnings.warn('Using training target_blocks; provide test target_blocks for inference.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss  219.7553532973; rmse_eig=0.3267259950 rmse_evec=0.0874602325 rmse_ard=0.2984171424 \n",
      "Epoch      10, train loss  188.0561662894; rmse_eig=0.2846495583 rmse_evec=0.0864570868 rmse_ard=0.2853555375 \n",
      "Epoch      20, train loss  165.6647154412; rmse_eig=0.2652878039 rmse_evec=0.0841932332 rmse_ard=0.2683434521 \n",
      "Epoch      30, train loss  145.4441962106; rmse_eig=0.2439117446 rmse_evec=0.0824518196 rmse_ard=0.2533267582 \n",
      "Epoch      40, train loss  127.1075292883; rmse_eig=0.2171281662 rmse_evec=0.0811576440 rmse_ard=0.2416990873 \n",
      "Epoch      50, train loss  110.7902560254; rmse_eig=0.1891645476 rmse_evec=0.0793942670 rmse_ard=0.2314221064 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[220], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m epoch_loss_ard \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     14\u001b[0m eig_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mib\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/data/mldataset.py:679\u001b[0m, in \u001b[0;36mMLDataset.group_and_join\u001b[0;34m(self, batch, fields_to_join, join_kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m fields_to_join:  \u001b[38;5;66;03m# Join tensors if requested\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(field[\u001b[38;5;241m0\u001b[39m], torch\u001b[38;5;241m.\u001b[39mScriptObject) \u001b[38;5;129;01mand\u001b[39;00m field[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_has_method(\n\u001b[1;32m    677\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeys_to_properties\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    678\u001b[0m     ):  \u001b[38;5;66;03m# inferred metatensor.torch.TensorMap type\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msamples\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjoin_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(field[\u001b[38;5;241m0\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor):  \u001b[38;5;66;03m# torch.Tensor type\u001b[39;00m\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nepoch = 1000\n",
    "nevery = 10\n",
    "losses = []\n",
    "losses_e = []\n",
    "losses_ard = []\n",
    "losses_evec = []\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_e = 0\n",
    "    epoch_loss_evec = 0\n",
    "    epoch_loss_ard = 0\n",
    "    eig_sum = 0\n",
    "    \n",
    "\n",
    "    for ib, batch in enumerate(dl):\n",
    "        \n",
    "        model.train(True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model.forward_mts(batch.features)\n",
    "\n",
    "        HT = blocks_to_matrix(pred, qmdata_sto3G, detach = False)\n",
    "        HT = [HT[i][0,0,0] for i in batch.sample_id] # Required for now\n",
    "\n",
    "        pred_eigvals, pred_ard, pred_C, _ = compute_ard_vec(qmdata_sto3G, batch, HT, device, overlap = [qmdata_sto3G.overlap_realspace[i] for i in batch.sample_id])\n",
    "\n",
    "        loss_e = Eigval_loss(pred_eigvals[:, :n_eig_to_match], batch.eigenvalues[:, :n_eig_to_match])\n",
    "        \n",
    "        loss_ard = torch.sum((pred_ard - batch.atom_resolved_density)**2)\n",
    "        \n",
    "        pred_ev_0 = torch.norm(pred_C[:, :, :n_eig_to_match], dim = (1))\n",
    "        targ_ev_0 = torch.norm(batch.eigenvectors[:, :, :n_eig_to_match], dim = (1))\n",
    "        loss_evec = torch.sum((pred_ev_0 - targ_ev_0)**2)\n",
    "        \n",
    "        loss = loss_ard + loss_e + loss_evec\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss_e += loss_e.item()\n",
    "        epoch_loss_evec += loss_evec.item()\n",
    "        epoch_loss_ard += loss_ard.item()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    scheduler.step(epoch_loss)\n",
    "    losses.append(epoch_loss)\n",
    "    losses_e.append(epoch_loss_e)\n",
    "    losses_evec.append(epoch_loss_evec)\n",
    "    losses_ard.append(epoch_loss_ard)\n",
    "    \n",
    "    if epoch % nevery == 0:\n",
    "        print(f\"Epoch {epoch:>7d}, train loss {epoch_loss:>15.10f}; rmse_eig={np.sqrt(epoch_loss_e/5/160):>12.10f} rmse_evec={np.sqrt(epoch_loss_evec/5/160):>12.10f} rmse_ard={np.sqrt(epoch_loss_ard/160/9):>12.10f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "966b60fd-d959-4820-b030-96042b1f5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.forward(batch.features, mldata.model_metadata)\n",
    "HT = blocks_to_matrix(pred, qmdata_sto3G, detach = False)\n",
    "HT = [HT[i][0,0,0] for i in batch.sample_id] # Required for now\n",
    "\n",
    "pred_eigvals, pred_ard, pred_C, _ = compute_ard_vec(qmdata_sto3G, batch, HT, device, overlap = [qmdata_sto3G.overlap_realspace[i] for i in batch.sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d42b83a1-a15b-48ee-8a26-519e37636e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 9])\n",
      "torch.Size([10, 9])\n"
     ]
    }
   ],
   "source": [
    "print(batch.atom_resolved_density.shape)\n",
    "print(pred_ard.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58876e0-efb2-4174-8380-331772bcf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(mldata.test_dataset, batch_size = len(mldata.test_dataset), collate_fn=mldata.group_and_join)\n",
    "train_dl = DataLoader(mldata.train_dataset, batch_size = len(mldata.train_dataset), collate_fn=mldata.group_and_join)\n",
    "\n",
    "fig_e, ax_e = plt.subplots()\n",
    "fig_a, ax_a = plt.subplots()\n",
    "fig_evec, ax_evec = plt.subplots()\n",
    "\n",
    "data = {}\n",
    "for dl_, lbl in zip([train_dl, test_dl], ['train', 'test']):\n",
    "    batch = next(iter(dl_))\n",
    "    pred = model(batch.features, mldata.model_metadata)\n",
    "\n",
    "    HT = blocks_to_matrix(pred, qmdata, detach = True)\n",
    "    HT = [HT[i][0,0,0] for i in batch.sample_id]\n",
    "    \n",
    "    pred_eigvals, pred_ard, pred_eigvec, pred_rho = compute_ard_vec(qmdata, batch, HT, device, overlap = [qmdata.overlap_realspace[i] for i in batch.sample_id])\n",
    "\n",
    "    ax_e.plot(batch.eigenvalues[:,:n_eig_to_match].flatten(), pred_eigvals[:,:n_eig_to_match].detach().flatten(), '.', label = lbl)\n",
    "    ax_e.plot([-21, 2], [-21, 2], 'k')\n",
    "    ax_e.set_title('Eigenvalues')\n",
    "    ax_e.legend()\n",
    "\n",
    "    ax_a.plot(batch.atom_resolved_density.flatten(), pred_ard.detach().flatten(), '.', label = lbl)\n",
    "    ax_a.plot([0,7], [0,7], 'k')\n",
    "    ax_a.set_title('Mayer bond charges')\n",
    "    ax_a.legend()\n",
    "\n",
    "    pred_evn, targ_evn = torch.norm(pred_eigvec[:, :, :n_eig_to_match], dim = (1)), torch.norm(batch.eigenvectors[:, :, :n_eig_to_match], dim = (1))\n",
    "    \n",
    "    ax_evec.plot(targ_evn.flatten(), pred_evn.detach().flatten(), '.', label = lbl)\n",
    "    xmin, xmax = ax_evec.get_xlim()\n",
    "    ymin, ymax = ax_evec.get_ylim()\n",
    "    xmin = np.min([xmin,ymin])\n",
    "    xmax = np.max([xmax,ymax])\n",
    "    ax_evec.plot([xmin,xmax], [xmin,xmax], 'k')\n",
    "    ax_evec.set_title('evec')\n",
    "    ax_evec.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1617430a-3515-4204-aa44-c9b8d0029e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dl))\n",
    "print(batch.sample_id)\n",
    "dl_frames = [qmdata.structures[A] for A in batch.sample_id]\n",
    "\n",
    "pred = model(batch.features, mldata.model_metadata)\n",
    "\n",
    "HT = blocks_to_matrix(pred, qmdata, detach = True)\n",
    "HT = [HT[i][0,0,0] for i in batch.sample_id]\n",
    "\n",
    "fock_predictions = torch.stack(HT)\n",
    "\n",
    "fock_predictions = unfix_orbital_order(\n",
    "    fock_predictions,\n",
    "    dl_frames,\n",
    "    qmdata_sto3G.basis,\n",
    ")\n",
    "\n",
    "fock_targets = unfix_orbital_order(\n",
    "    batch.fock_realspace,\n",
    "    dl_frames,\n",
    "    mldata.qmdata.basis,\n",
    ")\n",
    "\n",
    "fock_sto3g = unfix_orbital_order(\n",
    "    qmdata_sto3G.fock_realspace,\n",
    "    dl_frames,\n",
    "    qmdata_sto3G.basis,\n",
    ")\n",
    "\n",
    "over_large = unfix_orbital_order(\n",
    "    batch.overlap_realspace,\n",
    "    dl_frames,\n",
    "    mldata.qmdata.basis,\n",
    ")\n",
    "\n",
    "over_small = unfix_orbital_order(\n",
    "    torch.stack([qmdata_sto3G.overlap_realspace[i] for i in batch.sample_id]),\n",
    "    dl_frames,\n",
    "    qmdata_sto3G.basis,\n",
    ")\n",
    "\n",
    "with ipy_io.capture_output():\n",
    "    dipole_targets = compute_dipole_moment(\n",
    "        dl_frames,\n",
    "        fock_targets,\n",
    "        over_large,\n",
    "        qmdata.basis_name\n",
    "    )\n",
    "\n",
    "with ipy_io.capture_output():\n",
    "    dipole_predictions = compute_dipole_moment(\n",
    "        dl_frames,\n",
    "        fock_predictions,\n",
    "        over_small,\n",
    "        qmdata_sto3G.basis_name\n",
    "    )\n",
    "\n",
    "with ipy_io.capture_output():\n",
    "    dipole_sto3g = compute_dipole_moment(\n",
    "        dl_frames,\n",
    "        fock_sto3g,\n",
    "        over_small,\n",
    "        qmdata_sto3G.basis_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60168fff-f183-4a70-b20c-daa9987beee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.units import Bohr, Debye\n",
    "au_to_debye = Bohr/Debye\n",
    "\n",
    "ms = 12\n",
    "mew = .7\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = dipole_targets.flatten().detach().cpu() * au_to_debye\n",
    "y_sto3g = dipole_sto3g.flatten().detach().cpu() * au_to_debye\n",
    "y_ml = dipole_predictions.flatten().detach().cpu() * au_to_debye\n",
    "\n",
    "rmse_sto3g = np.sqrt(torch.mean(y_sto3g - x)**2)\n",
    "rmse_ml = np.sqrt(torch.mean(y_ml - x)**2)\n",
    "\n",
    "ax.plot(x, y_sto3g, 'v', \n",
    "         markeredgewidth = mew,\n",
    "         markeredgecolor = 'k',\n",
    "         markersize = ms, \n",
    "         label = 'STO-3G', \n",
    "         alpha = 1)\n",
    "\n",
    "ax.plot(x, y_ml, 'o',\n",
    "         markeredgewidth = mew,\n",
    "         markeredgecolor = 'k',\n",
    "         markersize = ms, \n",
    "         label = 'ML', \n",
    "         alpha = 1)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "xm, xM = ax.get_xlim()\n",
    "ym, yM = ax.get_ylim()\n",
    "m = np.min([xm,ym])\n",
    "M = np.max([xM,yM])\n",
    "ax.plot([m,M], [m,M], '--k')\n",
    "ax.set_xlim(m, M)\n",
    "ax.set_ylim(m, M)\n",
    "\n",
    "ax.text(0.6, 0.3, fr'$\\mathrm{{RMSE_{{\\text{{STO-3G}}}}}}={rmse_sto3g:.3f}\\,$D', transform = ax.transAxes, ha = 'left')\n",
    "ax.text(0.6, 0.25, f'$\\mathrm{{RMSE_{{ML}}}}={rmse_ml:.3f}\\,$D', transform = ax.transAxes, ha = 'left')\n",
    "\n",
    "ax.set_xlabel('Target dipoles (D)')\n",
    "ax.set_ylabel('Predicted dipoles (D)')\n",
    "\n",
    "# ax.set_title('Indirect training from def2-svp to a STO-3G-like model.\\nTargets: eigenvalues; ARD; eigenvector norms over AOs\\nTest set contains 50 water molecules')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
