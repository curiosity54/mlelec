{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "162d0473-d1cf-4306-9a2b-d03218bbfe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c52587-7c8f-4b08-8508-8d45b56d9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle\n",
    "\n",
    "from ase.io import read\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from mlelec.data.qmdataset import QMDataset\n",
    "from mlelec.data.mldataset import MLDataset\n",
    "from mlelec.models.linear_integrated import LinearModelPeriodic\n",
    "from mlelec.utils.pbc_utils import blocks_to_matrix\n",
    "from mlelec.utils.twocenter_utils import _to_uncoupled_basis, unfix_orbital_order\n",
    "from mlelec.metrics import Eigval_loss, L2_loss_meanzero, L2_loss\n",
    "import metatensor.torch as mts\n",
    "from metatensor.learn import DataLoader\n",
    "\n",
    "import os\n",
    "os.environ[\"PYSCFAD_BACKEND\"] = \"torch\"\n",
    "from pyscf import gto\n",
    "from pyscfad import numpy as pynp\n",
    "from pyscfad import ops\n",
    "from pyscfad.ml.scf import hf\n",
    "import pyscf.pbc.tools.pyscf_ase as pyscf_ase\n",
    "from mlelec.data.pyscf_calculator import _instantiate_pyscf_mol\n",
    "\n",
    "import xitorch\n",
    "from xitorch.linalg import symeig\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.utils import io as ipy_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aaccef1-cad2-4284-9537-af28fa5b67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigval_vec(dataset, batch, Hk, return_rho = False, return_eigenvectors = False):\n",
    "    eig = []\n",
    "    rho = []\n",
    "    eigvec = []\n",
    "    for A, H, S in zip(batch.sample_id, Hk, batch.overlap_realspace):\n",
    "        # Compute eigenvalues and eigenvectors\n",
    "        # eigvals, eigvecs = symeig(Ax, M = Mx)\n",
    "        eigvals, C = symeig(xitorch.LinearOperator.m(H), M = xitorch.LinearOperator.m(S))\n",
    "        if return_rho:\n",
    "            frame = dataset.structures[A]\n",
    "            natm = len(frame)\n",
    "            ncore = sum(dataset.ncore[s] for s in frame.numbers)\n",
    "            nelec = sum(frame.numbers) - ncore\n",
    "            occ = torch.tensor([2.0 if i < nelec//2 else 0.0 for i in range(C.shape[-1])], dtype = torch.float64, requires_grad = True, device = device)\n",
    "            # occ = torch.tensor([2.0 for i in range(C.shape[-1])], dtype = torch.float64, requires_grad = True, device = device)\n",
    "            rho.append(torch.einsum('n,...in,...jn->ij...', occ, C, C.conj()))\n",
    "        eig.append(eigvals)\n",
    "        if return_eigenvectors:\n",
    "            eigvec.append(C)\n",
    "\n",
    "    to_return = [eig]\n",
    "    if return_rho:\n",
    "        try:\n",
    "            rho = torch.stack(rho)\n",
    "        except:\n",
    "            pass\n",
    "        to_return.append(rho)\n",
    "    if return_eigenvectors:\n",
    "        try:\n",
    "            eigvec = torch.stack(eigvec)\n",
    "        except:\n",
    "            pass\n",
    "        to_return.append(eigvec)\n",
    "    return tuple(to_return)\n",
    "\n",
    "def compute_ard_vec(dataset, batch, HT, device, overlap = None):\n",
    "    basis = dataset.basis\n",
    "    ard_ = []\n",
    "    eig = []\n",
    "    Cs = []\n",
    "    rhos = []\n",
    "\n",
    "    overlap = batch.overlap_realspace if overlap is None else overlap\n",
    "    \n",
    "    for A, H, S in zip(batch.sample_id, HT, overlap):\n",
    "        frame = dataset.structures[A]\n",
    "        natm = len(frame)\n",
    "        ncore = sum(dataset.ncore[s] for s in frame.numbers)\n",
    "        nelec = sum(frame.numbers) - ncore\n",
    "        split_idx = [len(basis[s]) for s in frame.numbers]\n",
    "        needed = True if len(np.unique(split_idx)) > 1 else False\n",
    "        \n",
    "        max_dim = np.max(split_idx)\n",
    "        \n",
    "        eigvals, C = symeig(xitorch.LinearOperator.m(H), M=xitorch.LinearOperator.m(S), return_eigenvectors = True) # Has shape = (n_k, N, N)\n",
    "        \n",
    "        occ = torch.tensor([2.0 if i < nelec//2 else 0.0 for i in range(C.shape[-1])], dtype = torch.float64, requires_grad = True, device = device)\n",
    "        P = torch.einsum('n,...in,...jn->ij...', occ, C, C.conj())\n",
    "        rhos.append(P)\n",
    "        \n",
    "        slices = torch.split(P, split_idx, dim=0)\n",
    "        blocks = [torch.split(slice_, split_idx, dim=1) for slice_ in slices]\n",
    "        blocks_flat = [block for sublist in blocks for block in sublist]\n",
    "        \n",
    "        if needed:\n",
    "            squared_blocks = []\n",
    "            for block in blocks_flat:\n",
    "                pad_size = (0, max_dim - block.size(1), 0, max_dim - block.size(0))\n",
    "                squared_block = torch.nn.functional.pad(block, pad_size, \"constant\", 0)\n",
    "                squared_blocks.append(squared_block)\n",
    "            blocks_flat = squared_blocks\n",
    "\n",
    "\n",
    "        ard_.append(torch.stack(blocks_flat).norm(dim=(1,2)))\n",
    "        eig.append(eigvals)\n",
    "        Cs.append(C)\n",
    "\n",
    "    try:\n",
    "        ard_ = torch.stack(ard_)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        eig = torch.stack(eig)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        rhos = torch.stack(rhos)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        Cs = torch.stack(Cs)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return eig, ard_, Cs, rhos\n",
    "\n",
    "def compute_dipole_moment(frames, fock_predictions, overlaps, basis = 'sto-3g'):\n",
    "    assert (\n",
    "        len(frames) == len(fock_predictions) == len(overlaps)\n",
    "    ), \"Length of frames, fock_predictions, and overlaps must be the same\"\n",
    "    dipoles = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        mol = _instantiate_pyscf_mol(frame, basis = basis)\n",
    "        mf = hf.SCF(mol)\n",
    "        fock = torch.autograd.Variable(\n",
    "            fock_predictions[i].type(torch.float64), requires_grad=True\n",
    "        )\n",
    "\n",
    "        mo_energy, mo_coeff = mf.eig(fock, overlaps[i])\n",
    "        mo_occ = mf.get_occ(mo_energy)  # get_occ returns a numpy array\n",
    "        mo_occ = ops.convert_to_tensor(mo_occ)\n",
    "        dm1 = mf.make_rdm1(mo_coeff, mo_occ)\n",
    "        dip = mf.dip_moment(dm=dm1)\n",
    "        dipoles.append(dip)\n",
    "    return torch.stack(dipoles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d52ddf-8bd5-4385-802a-c533ac4f1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orbitals = {\n",
    "    'sto-3g': {1: [[1,0,0]], \n",
    "               5: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               6: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               7: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]],\n",
    "               8: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]]\n",
    "               }, \n",
    "    \n",
    "    'def2svp': {1: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]],\n",
    "                6: [[1,0,0],[2,0,0],[3,0,0],[2,1,1], [2,1,-1],[2,1,0], [3,1,1], [3,1,-1],[3,1,0], [3,2,-2], [3,2,-1],[3,2,0], [3,2,1],[3,2,2]],\n",
    "                8: [[1,0,0],[2,0,0],[3,0,0],[2,1,1], [2,1,-1],[2,1,0], [3,1,1], [3,1,-1],[3,1,0], [3,2,-2], [3,2,-1],[3,2,0], [3,2,1],[3,2,2]]\n",
    "                },\n",
    "}\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4998d7ce-9947-4561-8ed0-5e1aa203aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_radial  = 12\n",
    "max_angular = 6\n",
    "atomic_gaussian_width = 0.3\n",
    "cutoff = 3.5\n",
    "\n",
    "hypers_pair = {'cutoff': cutoff,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': atomic_gaussian_width,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}}}\n",
    "\n",
    "hypers_atom = {'cutoff': cutoff,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': 0.5,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}}}\n",
    "\n",
    "\n",
    "return_rho0ij = False\n",
    "both_centers = False\n",
    "LCUT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d5bfc3-c835-45de-b318-c96150bfafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = '../examples/data/water_1000'\n",
    "every = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6d217-5003-47a7-bfd8-73acd29687cc",
   "metadata": {},
   "source": [
    "For the moment, we need to create multiple QMDataset (analogous to MoleculeDataset), one for the large basis, one for the small one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7188d839-579f-4b16-bab2-e5f78cc033ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmdata = QMDataset.from_file(frames_path = f'{workdir}/water_1000.xyz', \n",
    "                             fock_realspace_path = f'{workdir}/def2svp/focks.npy', \n",
    "                             overlap_realspace_path = f'{workdir}/def2svp/overlaps.npy',\n",
    "                             dimension = 0, \n",
    "                             device = 'cpu', \n",
    "                             orbs_name='def2svp', \n",
    "                             orbs=orbitals['def2svp'], \n",
    "                             frame_slice = f'::{every}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be02c4fe-273e-46b5-9c04-7dae558b62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fock_small = hickle.load(f'{workdir}/sto-3g/fock.hickle')[::every]\n",
    "over_small = hickle.load(f'{workdir}/sto-3g/overlap.hickle')[::every]\n",
    "\n",
    "qmdata_sto3G = QMDataset(frames = read(f'{workdir}/water_1000.xyz', index = f'::{every}'), \n",
    "                         dimension = 0,\n",
    "                         fock_realspace=fock_small.clone(),\n",
    "                         overlap_realspace=over_small.clone(),\n",
    "                         device = device, \n",
    "                         orbs = orbitals['sto-3g'], \n",
    "                         orbs_name = 'sto-3g'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af9ffb89-465f-44a3-806a-dd0b244aeba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu pair features\n",
      "cpu single center features\n",
      "cpu single center features\n"
     ]
    }
   ],
   "source": [
    "from mlelec.data.mldataset import MLDataset\n",
    "mldata = MLDataset(qmdata_sto3G, \n",
    "                   item_names = ['fock_blocks', 'fock_realspace', 'overlap_realspace', 'eigenvalues', 'atom_resolved_density'],\n",
    "                #    features = './features', #mldata.features,\n",
    "                   cutoff = hypers_pair['cutoff'],\n",
    "                   hypers_atom = hypers_atom,\n",
    "                   hypers_pair = hypers_pair,\n",
    "                   lcut = 3,\n",
    "                   orbitals_to_properties = True,\n",
    "                   train_frac = 1,\n",
    "                   shuffle = False,\n",
    "                   model_basis = orbitals['sto-3g'],\n",
    "                   fix_p_orbital_order=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dca2cf-2183-4b8e-820e-5a10e9e5a8b5",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abada533-7c2b-423c-bd6c-46516fc8fe74",
   "metadata": {},
   "source": [
    "Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db4873e8-95f2-4fe6-8524-a135e1f2d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "092f9fed-8da2-4a2d-a300-6c9d54c0b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.models.equivariant_nonlinear_model import EquivariantNonlinearModel\n",
    "seed = 0\n",
    "set_random_seed(seed)\n",
    "\n",
    "model = EquivariantNonlinearModel(mldata, device = device, nhidden = 4, nlayers = 1, activation = 'SiLU', apply_norm = True)\n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ab9aa9e-f265-45b1-95c9-1c1b942c0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "set_random_seed(0)\n",
    "\n",
    "model_old = LinearModelPeriodic(twocfeat = mldata.features, \n",
    "                            target_blocks = mldata.model_metadata,\n",
    "                            frames = mldata.structures, \n",
    "                            orbitals = mldata.model_basis, \n",
    "                            device = device,\n",
    "                            nhidden = 4, \n",
    "                            nlayers = 1,\n",
    "                            activation = 'SiLU',\n",
    "                           apply_norm = True\n",
    "                           )\n",
    "model_old = model_old.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "002d5e14-e65d-488c-8155-2f9107ec2222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "from mlelec.models.equivariant_nonlinear_lightning import LitEquivariantNonlinearModel, MSELoss\n",
    "from mlelec.models.equivariant_nonlinear_lightning import MLDatasetDataModule\n",
    "\n",
    "# Assuming you have `train_dataset`, `val_dataset`, and `test_dataset`\n",
    "data_module = MLDatasetDataModule(mldata, batch_size=16)\n",
    "\n",
    "# Initialize with custom loss function and keyword arguments for derived predictions\n",
    "\n",
    "model = LitEquivariantNonlinearModel(\n",
    "    mldata=mldata,\n",
    "    nhidden=16,\n",
    "    nlayers=2,\n",
    "    activation='SiLU',\n",
    "    apply_norm=True,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=MSELoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c03e8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maccelerator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAccelerator\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstrategy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_nodes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprecision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transformer-engine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transformer-engine-float16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'16-true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'16-mixed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bf16-true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bf16-mixed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'32-true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'64-true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bf16'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogger\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfast_dev_run\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_epochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_epochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlimit_train_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlimit_val_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlimit_test_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlimit_predict_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moverfit_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mval_check_interval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcheck_val_every_n_epoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_sanity_val_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlog_every_n_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menable_checkpointing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menable_progress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menable_model_summary\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maccumulate_grad_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgradient_clip_val\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgradient_clip_algorithm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbenchmark\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minference_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_distributed_sampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprofiler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofilers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProfiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdetect_anomaly\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbarebones\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mplugins\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClusterEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_sync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerSync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClusterEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_sync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerSync\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msync_batchnorm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreload_dataloaders_every_n_epochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdefault_root_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m      <no docstring>\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Customize every aspect of training via flags.\n",
      "\n",
      "Args:\n",
      "    accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"hpu\", \"mps\", \"auto\")\n",
      "        as well as custom accelerator instances.\n",
      "\n",
      "    strategy: Supports different training strategies with aliases as well custom strategies.\n",
      "        Default: ``\"auto\"``.\n",
      "\n",
      "    devices: The devices to use. Can be set to a positive number (int or str), a sequence of device indices\n",
      "        (list or str), the value ``-1`` to indicate all available devices should be used, or ``\"auto\"`` for\n",
      "        automatic selection based on the chosen accelerator. Default: ``\"auto\"``.\n",
      "\n",
      "    num_nodes: Number of GPU nodes for distributed training.\n",
      "        Default: ``1``.\n",
      "\n",
      "    precision: Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),\n",
      "        16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').\n",
      "        Can be used on CPU, GPU, TPUs, or HPUs.\n",
      "        Default: ``'32-true'``.\n",
      "\n",
      "    logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n",
      "        the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.\n",
      "        ``False`` will disable logging. If multiple loggers are provided, local files\n",
      "        (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of the first logger.\n",
      "        Default: ``True``.\n",
      "\n",
      "    callbacks: Add a callback or list of callbacks.\n",
      "        Default: ``None``.\n",
      "\n",
      "    fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
      "        of train, val and test to find any bugs (ie: a sort of unit test).\n",
      "        Default: ``False``.\n",
      "\n",
      "    max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
      "        If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\n",
      "        To enable infinite training, set ``max_epochs = -1``.\n",
      "\n",
      "    min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
      "\n",
      "    max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\n",
      "        and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\n",
      "        ``max_epochs`` to ``-1``.\n",
      "\n",
      "    min_steps: Force training for at least these number of steps. Disabled by default (``None``).\n",
      "\n",
      "    max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\n",
      "        The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\n",
      "        :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\n",
      "        :class:`datetime.timedelta`.\n",
      "\n",
      "    limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\n",
      "        Default: ``1.0``.\n",
      "\n",
      "    limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\n",
      "        Default: ``1.0``.\n",
      "\n",
      "    limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\n",
      "        Default: ``1.0``.\n",
      "\n",
      "    limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\n",
      "        Default: ``1.0``.\n",
      "\n",
      "    overfit_batches: Overfit a fraction of training/validation data (float) or a set number of batches (int).\n",
      "        Default: ``0.0``.\n",
      "\n",
      "    val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\n",
      "        after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\n",
      "        batches. An ``int`` value can only be higher than the number of training batches when\n",
      "        ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches\n",
      "        across epochs or during iteration-based training.\n",
      "        Default: ``1.0``.\n",
      "\n",
      "    check_val_every_n_epoch: Perform a validation loop every after every `N` training epochs. If ``None``,\n",
      "        validation will be done solely based on the number of training batches, requiring ``val_check_interval``\n",
      "        to be an integer value.\n",
      "        Default: ``1``.\n",
      "\n",
      "    num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
      "        Set it to `-1` to run all batches in all validation dataloaders.\n",
      "        Default: ``2``.\n",
      "\n",
      "    log_every_n_steps: How often to log within steps.\n",
      "        Default: ``50``.\n",
      "\n",
      "    enable_checkpointing: If ``True``, enable checkpointing.\n",
      "        It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
      "        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.callbacks`.\n",
      "        Default: ``True``.\n",
      "\n",
      "    enable_progress_bar: Whether to enable to progress bar by default.\n",
      "        Default: ``True``.\n",
      "\n",
      "    enable_model_summary: Whether to enable model summarization by default.\n",
      "        Default: ``True``.\n",
      "\n",
      "    accumulate_grad_batches: Accumulates gradients over k batches before stepping the optimizer.\n",
      "        Default: 1.\n",
      "\n",
      "    gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\n",
      "        gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\n",
      "        Default: ``None``.\n",
      "\n",
      "    gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\n",
      "        to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\n",
      "        be set to ``\"norm\"``.\n",
      "\n",
      "    deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\n",
      "        Set to ``\"warn\"`` to use deterministic algorithms whenever possible, throwing warnings on operations\n",
      "        that don't support deterministic mode. If not set, defaults to ``False``. Default: ``None``.\n",
      "\n",
      "    benchmark: The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.\n",
      "        The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used\n",
      "        (``False`` if not manually set). If :paramref:`~lightning.pytorch.trainer.trainer.Trainer.deterministic`\n",
      "        is set to ``True``, this will default to ``False``. Override to manually set a different value.\n",
      "        Default: ``None``.\n",
      "\n",
      "    inference_mode: Whether to use :func:`torch.inference_mode` or :func:`torch.no_grad` during\n",
      "        evaluation (``validate``/``test``/``predict``).\n",
      "\n",
      "    use_distributed_sampler: Whether to wrap the DataLoader's sampler with\n",
      "        :class:`torch.utils.data.DistributedSampler`. If not specified this is toggled automatically for\n",
      "        strategies that require it. By default, it will add ``shuffle=True`` for the train sampler and\n",
      "        ``shuffle=False`` for validation/test/predict samplers. If you want to disable this logic, you can pass\n",
      "        ``False`` and add your own distributed sampler in the dataloader hooks. If ``True`` and a distributed\n",
      "        sampler was already added, Lightning will not replace the existing one. For iterable-style datasets,\n",
      "        we don't do this automatically.\n",
      "\n",
      "    profiler: To profile individual steps during training and assist in identifying bottlenecks.\n",
      "        Default: ``None``.\n",
      "\n",
      "    detect_anomaly: Enable anomaly detection for the autograd engine.\n",
      "        Default: ``False``.\n",
      "\n",
      "    barebones: Whether to run in \"barebones mode\", where all features that may impact raw speed are\n",
      "        disabled. This is meant for analyzing the Trainer overhead and is discouraged during regular training\n",
      "        runs. The following features are deactivated:\n",
      "        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_checkpointing`,\n",
      "        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.logger`,\n",
      "        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_progress_bar`,\n",
      "        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.log_every_n_steps`,\n",
      "        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_model_summary`,\n",
      "        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.num_sanity_val_steps`,\n",
      "        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.fast_dev_run`,\n",
      "        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.detect_anomaly`,\n",
      "        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.profiler`,\n",
      "        :meth:`~lightning.pytorch.core.LightningModule.log`,\n",
      "        :meth:`~lightning.pytorch.core.LightningModule.log_dict`.\n",
      "    plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
      "        Default: ``None``.\n",
      "\n",
      "    sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
      "        Default: ``False``.\n",
      "\n",
      "    reload_dataloaders_every_n_epochs: Set to a positive integer to reload dataloaders every n epochs.\n",
      "        Default: ``0``.\n",
      "\n",
      "    default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
      "        Default: ``os.getcwd()``.\n",
      "        Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      "\n",
      "Raises:\n",
      "    TypeError:\n",
      "        If ``gradient_clip_val`` is not an int or float.\n",
      "\n",
      "    MisconfigurationException:\n",
      "        If ``gradient_clip_algorithm`` is invalid.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "pl.Trainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c57a4e8-f7e2-41ab-be0c-d40cda602dde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name  | Type                      | Params | Mode\n",
      "-----------------------------------------------------------\n",
      "0 | model | EquivariantNonlinearModel | 509 K  | eval\n",
      "-----------------------------------------------------------\n",
      "509 K     Trainable params\n",
      "0         Non-trainable params\n",
      "509 K     Total params\n",
      "2.037     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f97e7206c04c19a09a1eaedd69ef99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=20). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f17dd1f30f94a16a0ce1a98dcd8f54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6356a440f01b48a8852693393d58747d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f12b1c586624b64ba1c32941d40f5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec46192fe4642f89e90ee0c73bd50a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a166c20d52244a3b8466ba19df6c69fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3ba90448a348f587f7cde99a321322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397fd505edb94c4cb4ac1856df444abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed9f45a32674c2fbc62b46986a4bd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275828dfba33490b86cf3062c8281ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a1aad31661491b94090a034fa3316a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420ea7c61c3d47d89e085042b16e098d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5d7fbdfd2f4eae94cb9d00c4003ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55097471a86f47e9b17b32ce55a2fed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134528abc1ad46e48e1af5b0d4660128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09c39db4e81414dab413072586c9cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56307c8d895343f09f912457d8573328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34b912b7864422c8ec4e87687b049b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1000, accelerator='cpu', log_every_n_steps = 20, check_val_every_n_epoch=60)\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f14631e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e69a6662f6e4845b9d18de0afe5ca63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">        Test metric        </span><span style=\"font-weight: bold\">       DataLoader 0        </span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span><span style=\"color: #800080; text-decoration-color: #800080\">     1.373575576085556     </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       "\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m    1.373575576085556    \u001b[0m\u001b[35m \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.373575576085556}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b8110df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))\n",
    "pred = model.forward(batch.features, mldata.model_metadata)\n",
    "MSELoss().compute(pred, batch.fock_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b675a52-d827-434f-8bbc-797befc39cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-3, weight_decay = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2058f772-1313-49cd-bb3d-24b8f5bd7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N. of eigenvalues to match (this needs to be adapted for diverse datasets\n",
    "n_eig_to_match = 5\n",
    "dl = mts.learn.DataLoader(mldata.train_dataset, batch_size = 16, collate_fn=mldata.group_and_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c1b28a8-9604-455f-a645-f5754a1a9bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch\u001b[38;5;241m.\u001b[39mfeatures, mldata\u001b[38;5;241m.\u001b[39mmodel_metadata)\n\u001b[0;32m---> 24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mL2_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfock_blocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# HT = blocks_to_matrix(pred, qmdata_sto3G, detach = False)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# HT = [HT[i][0,0,0] for i in batch.sample_id] # Required for now\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# loss = loss_ard + loss_e + loss_evec\u001b[39;00m\n\u001b[1;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Software/mlelec/src/mlelec/metrics.py:100\u001b[0m, in \u001b[0;36mL2_loss\u001b[0;34m(pred, target, loss_per_block, norm)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     98\u001b[0m         block\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m==\u001b[39m targetblock\u001b[38;5;241m.\u001b[39msamples\n\u001b[1;32m     99\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction and target must have the same samples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 100\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargetblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m norm)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# losses.append(torch.sum((block.values - targetblock.values)*(block.values - targetblock.values).conj()))\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# losses.append(torch.sum((block.values - targetblock.values) ** 2))\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, requires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(block\u001b[38;5;241m.\u001b[39mvalues))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "nepoch = 1000\n",
    "nevery = 10\n",
    "losses = []\n",
    "losses_e = []\n",
    "losses_ard = []\n",
    "losses_evec = []\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_e = 0\n",
    "    epoch_loss_evec = 0\n",
    "    epoch_loss_ard = 0\n",
    "    eig_sum = 0\n",
    "    \n",
    "\n",
    "    for ib, batch in enumerate(dl):\n",
    "        \n",
    "        model.train(True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model.forward(batch.features, mldata.model_metadata)\n",
    "\n",
    "        loss = L2_loss(pred,)\n",
    "        # HT = blocks_to_matrix(pred, qmdata_sto3G, detach = False)\n",
    "        # HT = [HT[i][0,0,0] for i in batch.sample_id] # Required for now\n",
    "\n",
    "        # pred_eigvals, pred_ard, pred_C, _ = compute_ard_vec(qmdata_sto3G, batch, HT, device, overlap = [qmdata_sto3G.overlap_realspace[i] for i in batch.sample_id])\n",
    "\n",
    "        # loss_e = Eigval_loss(pred_eigvals[:, :n_eig_to_match], batch.eigenvalues[:, :n_eig_to_match])\n",
    "        \n",
    "        # loss_ard = torch.sum((pred_ard - batch.atom_resolved_density)**2)\n",
    "        \n",
    "        # pred_ev_0 = torch.norm(pred_C[:, :, :n_eig_to_match], dim = (1))\n",
    "        # targ_ev_0 = torch.norm(batch.eigenvectors[:, :, :n_eig_to_match], dim = (1))\n",
    "        # loss_evec = torch.sum((pred_ev_0 - targ_ev_0)**2)\n",
    "        \n",
    "        # loss = loss_ard + loss_e + loss_evec\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss_e += loss_e.item()\n",
    "        epoch_loss_evec += loss_evec.item()\n",
    "        epoch_loss_ard += loss_ard.item()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    scheduler.step(epoch_loss)\n",
    "    losses.append(epoch_loss)\n",
    "    losses_e.append(epoch_loss_e)\n",
    "    losses_evec.append(epoch_loss_evec)\n",
    "    losses_ard.append(epoch_loss_ard)\n",
    "    \n",
    "    if epoch % nevery == 0:\n",
    "        print(f\"Epoch {epoch:>7d}, train loss {epoch_loss:>15.10f}; rmse_eig={np.sqrt(epoch_loss_e/5/160):>12.10f} rmse_evec={np.sqrt(epoch_loss_evec/5/160):>12.10f} rmse_ard={np.sqrt(epoch_loss_ard/160/9):>12.10f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "966b60fd-d959-4820-b030-96042b1f5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.forward(batch.features, mldata.model_metadata)\n",
    "HT = blocks_to_matrix(pred, qmdata_sto3G, detach = False)\n",
    "HT = [HT[i][0,0,0] for i in batch.sample_id] # Required for now\n",
    "\n",
    "pred_eigvals, pred_ard, pred_C, _ = compute_ard_vec(qmdata_sto3G, batch, HT, device, overlap = [qmdata_sto3G.overlap_realspace[i] for i in batch.sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d8db17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 7 blocks\n",
       "keys: block_type  species_i  species_j  L  inversion_sigma\n",
       "          0           1          1      0         1\n",
       "          0           8          8      0         1\n",
       "          0           8          8      1         1\n",
       "          0           8          8      2         1\n",
       "          1           1          1      0         1\n",
       "          2           1          8      0         1\n",
       "          2           1          8      1         1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_old(batch.features, mldata.model_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bf9b9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 7 blocks\n",
       "keys: block_type  species_i  species_j  L  inversion_sigma\n",
       "          0           1          1      0         1\n",
       "          0           8          8      0         1\n",
       "          0           8          8      1         1\n",
       "          0           8          8      2         1\n",
       "          1           1          1      0         1\n",
       "          2           1          8      0         1\n",
       "          2           1          8      1         1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be46d13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 22 blocks\n",
       "keys: block_type  species_i  species_j  L  inversion_sigma\n",
       "          -1          1          1      0         1\n",
       "          -1          1          1      1        -1\n",
       "          -1          1          1      1         1\n",
       "          0           1          1      0         1\n",
       "          0           1          1      1         1\n",
       "          0           1          1      2         1\n",
       "          0           8          8      0         1\n",
       "          0           8          8      1        -1\n",
       "          0           8          8      1         1\n",
       "          0           8          8      2        -1\n",
       "          0           8          8      2         1\n",
       "          0           8          8      3         1\n",
       "          0           8          8      4         1\n",
       "          1           1          1      0         1\n",
       "          1           1          1      1         1\n",
       "          1           1          1      2         1\n",
       "          2           1          8      0         1\n",
       "          2           1          8      1        -1\n",
       "          2           1          8      1         1\n",
       "          2           1          8      2        -1\n",
       "          2           1          8      2         1\n",
       "          2           1          8      3         1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.fock_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d42b83a1-a15b-48ee-8a26-519e37636e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 9])\n",
      "torch.Size([10, 9])\n"
     ]
    }
   ],
   "source": [
    "print(batch.atom_resolved_density.shape)\n",
    "print(pred_ard.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58876e0-efb2-4174-8380-331772bcf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(mldata.test_dataset, batch_size = len(mldata.test_dataset), collate_fn=mldata.group_and_join)\n",
    "train_dl = DataLoader(mldata.train_dataset, batch_size = len(mldata.train_dataset), collate_fn=mldata.group_and_join)\n",
    "\n",
    "fig_e, ax_e = plt.subplots()\n",
    "fig_a, ax_a = plt.subplots()\n",
    "fig_evec, ax_evec = plt.subplots()\n",
    "\n",
    "data = {}\n",
    "for dl_, lbl in zip([train_dl, test_dl], ['train', 'test']):\n",
    "    batch = next(iter(dl_))\n",
    "    pred = model(batch.features, mldata.model_metadata)\n",
    "\n",
    "    HT = blocks_to_matrix(pred, qmdata, detach = True)\n",
    "    HT = [HT[i][0,0,0] for i in batch.sample_id]\n",
    "    \n",
    "    pred_eigvals, pred_ard, pred_eigvec, pred_rho = compute_ard_vec(qmdata, batch, HT, device, overlap = [qmdata.overlap_realspace[i] for i in batch.sample_id])\n",
    "\n",
    "    ax_e.plot(batch.eigenvalues[:,:n_eig_to_match].flatten(), pred_eigvals[:,:n_eig_to_match].detach().flatten(), '.', label = lbl)\n",
    "    ax_e.plot([-21, 2], [-21, 2], 'k')\n",
    "    ax_e.set_title('Eigenvalues')\n",
    "    ax_e.legend()\n",
    "\n",
    "    ax_a.plot(batch.atom_resolved_density.flatten(), pred_ard.detach().flatten(), '.', label = lbl)\n",
    "    ax_a.plot([0,7], [0,7], 'k')\n",
    "    ax_a.set_title('Mayer bond charges')\n",
    "    ax_a.legend()\n",
    "\n",
    "    pred_evn, targ_evn = torch.norm(pred_eigvec[:, :, :n_eig_to_match], dim = (1)), torch.norm(batch.eigenvectors[:, :, :n_eig_to_match], dim = (1))\n",
    "    \n",
    "    ax_evec.plot(targ_evn.flatten(), pred_evn.detach().flatten(), '.', label = lbl)\n",
    "    xmin, xmax = ax_evec.get_xlim()\n",
    "    ymin, ymax = ax_evec.get_ylim()\n",
    "    xmin = np.min([xmin,ymin])\n",
    "    xmax = np.max([xmax,ymax])\n",
    "    ax_evec.plot([xmin,xmax], [xmin,xmax], 'k')\n",
    "    ax_evec.set_title('evec')\n",
    "    ax_evec.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1617430a-3515-4204-aa44-c9b8d0029e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dl))\n",
    "print(batch.sample_id)\n",
    "dl_frames = [qmdata.structures[A] for A in batch.sample_id]\n",
    "\n",
    "pred = model(batch.features, mldata.model_metadata)\n",
    "\n",
    "HT = blocks_to_matrix(pred, qmdata, detach = True)\n",
    "HT = [HT[i][0,0,0] for i in batch.sample_id]\n",
    "\n",
    "fock_predictions = torch.stack(HT)\n",
    "\n",
    "fock_predictions = unfix_orbital_order(\n",
    "    fock_predictions,\n",
    "    dl_frames,\n",
    "    qmdata_sto3G.basis,\n",
    ")\n",
    "\n",
    "fock_targets = unfix_orbital_order(\n",
    "    batch.fock_realspace,\n",
    "    dl_frames,\n",
    "    mldata.qmdata.basis,\n",
    ")\n",
    "\n",
    "fock_sto3g = unfix_orbital_order(\n",
    "    qmdata_sto3G.fock_realspace,\n",
    "    dl_frames,\n",
    "    qmdata_sto3G.basis,\n",
    ")\n",
    "\n",
    "over_large = unfix_orbital_order(\n",
    "    batch.overlap_realspace,\n",
    "    dl_frames,\n",
    "    mldata.qmdata.basis,\n",
    ")\n",
    "\n",
    "over_small = unfix_orbital_order(\n",
    "    torch.stack([qmdata_sto3G.overlap_realspace[i] for i in batch.sample_id]),\n",
    "    dl_frames,\n",
    "    qmdata_sto3G.basis,\n",
    ")\n",
    "\n",
    "with ipy_io.capture_output():\n",
    "    dipole_targets = compute_dipole_moment(\n",
    "        dl_frames,\n",
    "        fock_targets,\n",
    "        over_large,\n",
    "        qmdata.basis_name\n",
    "    )\n",
    "\n",
    "with ipy_io.capture_output():\n",
    "    dipole_predictions = compute_dipole_moment(\n",
    "        dl_frames,\n",
    "        fock_predictions,\n",
    "        over_small,\n",
    "        qmdata_sto3G.basis_name\n",
    "    )\n",
    "\n",
    "with ipy_io.capture_output():\n",
    "    dipole_sto3g = compute_dipole_moment(\n",
    "        dl_frames,\n",
    "        fock_sto3g,\n",
    "        over_small,\n",
    "        qmdata_sto3G.basis_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60168fff-f183-4a70-b20c-daa9987beee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.units import Bohr, Debye\n",
    "au_to_debye = Bohr/Debye\n",
    "\n",
    "ms = 12\n",
    "mew = .7\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = dipole_targets.flatten().detach().cpu() * au_to_debye\n",
    "y_sto3g = dipole_sto3g.flatten().detach().cpu() * au_to_debye\n",
    "y_ml = dipole_predictions.flatten().detach().cpu() * au_to_debye\n",
    "\n",
    "rmse_sto3g = np.sqrt(torch.mean(y_sto3g - x)**2)\n",
    "rmse_ml = np.sqrt(torch.mean(y_ml - x)**2)\n",
    "\n",
    "ax.plot(x, y_sto3g, 'v', \n",
    "         markeredgewidth = mew,\n",
    "         markeredgecolor = 'k',\n",
    "         markersize = ms, \n",
    "         label = 'STO-3G', \n",
    "         alpha = 1)\n",
    "\n",
    "ax.plot(x, y_ml, 'o',\n",
    "         markeredgewidth = mew,\n",
    "         markeredgecolor = 'k',\n",
    "         markersize = ms, \n",
    "         label = 'ML', \n",
    "         alpha = 1)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "xm, xM = ax.get_xlim()\n",
    "ym, yM = ax.get_ylim()\n",
    "m = np.min([xm,ym])\n",
    "M = np.max([xM,yM])\n",
    "ax.plot([m,M], [m,M], '--k')\n",
    "ax.set_xlim(m, M)\n",
    "ax.set_ylim(m, M)\n",
    "\n",
    "ax.text(0.6, 0.3, fr'$\\mathrm{{RMSE_{{\\text{{STO-3G}}}}}}={rmse_sto3g:.3f}\\,$D', transform = ax.transAxes, ha = 'left')\n",
    "ax.text(0.6, 0.25, f'$\\mathrm{{RMSE_{{ML}}}}={rmse_ml:.3f}\\,$D', transform = ax.transAxes, ha = 'left')\n",
    "\n",
    "ax.set_xlabel('Target dipoles (D)')\n",
    "ax.set_ylabel('Predicted dipoles (D)')\n",
    "\n",
    "# ax.set_title('Indirect training from def2-svp to a STO-3G-like model.\\nTargets: eigenvalues; ARD; eigenvector norms over AOs\\nTest set contains 50 water molecules')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
