{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78f2d86-1a50-4563-a7e3-5030f90b9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab92a44-4e57-4112-bcdf-7bdba9f8888f",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff84274a-981b-4bb7-86f0-eadf1e6ae409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/pyscf/dft/libxc.py:771: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, corresponding to the original definition by Stephens et al. (issue 1480) and the same as the B3LYP functional in Gaussian. To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read\n",
    "from ase.visualize import view\n",
    "import numpy as np \n",
    "import torch \n",
    "import metatensor \n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from metatensor import Labels, TensorBlock, TensorMap\n",
    "from mlelec.data.dataset import QMDataset\n",
    "\n",
    "from mlelec.utils.plot_utils import print_matrix, matrix_norm, block_matrix_norm\n",
    "\n",
    "from metatensor import load \n",
    "from mlelec.utils.twocenter_utils import _to_coupled_basis_OLD, _to_coupled_basis\n",
    "from mlelec.utils.pbc_utils import matrix_to_blocks_OLD, matrix_to_blocks\n",
    "\n",
    "def get_targets(dataset, device =\"cpu\", cutoff = None, target='fock', all_pairs= True):\n",
    "    \n",
    "    blocks = matrix_to_blocks(dataset, device = device, cutoff = cutoff, all_pairs = all_pairs, target = target)\n",
    "    coupled_blocks = _to_coupled_basis(blocks, skip_symmetry = True, device = device, translations = True)\n",
    "\n",
    "    blocks = blocks.to(arrays='numpy')\n",
    "    blocks = sort(blocks)\n",
    "    blocks = blocks.to(arrays='torch')\n",
    "    \n",
    "    coupled_blocks = coupled_blocks.to(arrays='numpy')\n",
    "    coupled_blocks = sort(coupled_blocks)\n",
    "    coupled_blocks = coupled_blocks.to(arrays='torch')\n",
    "    \n",
    "    return blocks, coupled_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aedad6a8-e3dc-491f-90e7-bb4b4c9198c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f1b77e-0109-411c-81d0-286f339638ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "orbitals = {\n",
    "    'sto-3g': {6: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]]}, \n",
    "    'def2svp': {6: [[1,0,0],[2,0,0],[3,0,0],[2,1,1], [2,1,-1],[2,1,0], [3,1,1], [3,1,-1],[3,1,0], [3,2,-2], [3,2,-1],[3,2,0], [3,2,1],[3,2,2]]},\n",
    "    'gthszvmolopt': {6: [[2, 0, 0], [2, 1, -1], [2, 1, 0], [2, 1, 1]]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf9ebbd-09ac-446b-922a-ca052a4070fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/Software/mlelec/src/mlelec/data/dataset.py:872: UserWarning: Overlap matrices not provided\n",
      "  warnings.warn(\"Overlap matrices not provided\")\n"
     ]
    }
   ],
   "source": [
    "workdir = './'\n",
    "root = f'{workdir}/examples/data/periodic'\n",
    "ORBS = 'sto-3g'\n",
    "# ORBS = 'gthszvmolopt'\n",
    "START = 0\n",
    "STOP = 2\n",
    "frames = read(f'{root}/c2/C2_174.extxyz', slice(START, STOP))\n",
    "for f in frames: \n",
    "    f.pbc = [True, True, True] \n",
    "    f.wrap(center=(0.,0.,0.))\n",
    "\n",
    "kmesh = [12,12,1]\n",
    "\n",
    "kfock = np.load(f\"{root}/c2/aims/H_k.npz\")\n",
    "kfock = [kfock[arr] for arr in kfock.files[slice(START, STOP)]]\n",
    "\n",
    "# from copy import deepcopy\n",
    "# kfock_ = deepcopy(kfock)\n",
    "\n",
    "# Ts = np.load(f\"{root}/c2/aims/cell_idx_list.npz\")\n",
    "# rfock = np.load(f\"{root}/c2/aims/realspaceH_nonsym.npz\")\n",
    "# rfock = [{tuple(int(t) for t in T): rf for T, rf in zip(Ts[arr], rfock[arr])} for arr in rfock.files[slice(START, STOP)]]\n",
    "# rover = np.load(f\"{root}/c2/aims/realspaceOvl_nonsym.npz\")\n",
    "# rover = [{tuple(int(t) for t in T): rf for T, rf in zip(Ts[arr], rover[arr])} for arr in rover.files[slice(START, STOP)]]\n",
    "\n",
    "dataset = QMDataset(frames = frames, \n",
    "                               kmesh = kmesh, \n",
    "                               dimension = 2, \n",
    "                               fix_p_orbital_order = False,\n",
    "                               apply_condon_shortley = True,\n",
    "                               # fock_realspace = rfock,\n",
    "                               # overlap_realspace = rover,\n",
    "                               fock_kspace = kfock,\n",
    "                               # overlap_kspace = kover,\n",
    "                               device = device, \n",
    "                               orbs = orbitals[ORBS], \n",
    "                               orbs_name = ORBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97646a42-ed95-47a6-aedf-df03b14aad5a",
   "metadata": {},
   "source": [
    "# Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "254c2686-e4c1-48e8-b2f8-1d72b18b54f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from metatensor import sort\n",
    "cutoff = 6\n",
    "target_blocks, target_coupled_blocks = get_targets(dataset, cutoff = cutoff, device = device, all_pairs = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da1204a4-bf68-421e-85a9-15cafdafde0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlelec.utils.pbc_utils import kmatrix_to_blocks\n",
    "k_target_blocks = kmatrix_to_blocks(dataset, cutoff = cutoff, all_pairs = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39d934b-f83b-4970-b8dc-016f1cbd879b",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86584b1a-5b62-4af9-aed2-2148efb29db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.features.acdc import pair_features, single_center_features, twocenter_features_periodic_NH, twocenter_hermitian_features\n",
    "import rascaline\n",
    "max_radial  = 8\n",
    "max_angular = 5\n",
    "atomic_gaussian_width = 0.3\n",
    "spline_basis = False\n",
    "##---splined basis \n",
    "if spline_basis :\n",
    "    spliner = rascaline.utils.SoapSpliner(\n",
    "        cutoff=cutoff,\n",
    "        max_radial=max_radial,\n",
    "        max_angular=max_angular,\n",
    "        basis=rascaline.utils.SphericalBesselBasis(\n",
    "            cutoff=cutoff, max_radial=max_radial, max_angular=max_angular\n",
    "        ),\n",
    "        density=rascaline.utils.GaussianDensity(atomic_gaussian_width=atomic_gaussian_width),\n",
    "        accuracy=1e-5,\n",
    "    )\n",
    "    splined_basis = spliner.compute()\n",
    "##-------\n",
    "hypers_pair = {'cutoff': cutoff,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': atomic_gaussian_width,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": splined_basis if spline_basis else {\"Gto\": {}},\n",
    "               # \"radial_basis\": splined_basis,\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}}}\n",
    "\n",
    "hypers_atom = {'cutoff': 4,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': 0.3,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}}}\n",
    "\n",
    "return_rho0ij = False\n",
    "both_centers = False\n",
    "all_pairs = False\n",
    "LCUT = 3\n",
    "\n",
    "def compute_features(dataset):\n",
    "\n",
    "    rhoij = pair_features(dataset.structures, hypers_atom, hypers_pair, order_nu = 1, all_pairs = all_pairs, both_centers = both_centers, mic = False,\n",
    "                          kmesh = dataset.kmesh, device = device, lcut = LCUT, return_rho0ij = return_rho0ij, counter = None, \n",
    "                          T_dict = None)  \n",
    "    \n",
    "    if both_centers and not return_rho0ij:\n",
    "        NU = 3\n",
    "    else:\n",
    "        NU = 2\n",
    "    rhonui = single_center_features(dataset.structures, hypers_atom, order_nu = NU, lcut = LCUT, device = device,\n",
    "                                    feature_names = rhoij.property_names)\n",
    "    \n",
    "    hfeat = twocenter_features_periodic_NH(single_center = rhonui, pair = rhoij, all_pairs = all_pairs)\n",
    "\n",
    "    return hfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ab8f766-88a6-4ef0-be83-544502d401fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfeat = compute_features(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e983ab9c-321e-49ab-beea-6ba58e86f63d",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b55f6-8880-445c-9c44-794cda37ebc7",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bc464ec-c7a4-463b-8f8d-6b625aa25a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.models.linear import LinearModelPeriodic\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "493afa2c-6ba5-4c8f-a554-f9aeb50403b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ridge(model, target_blocks, set_bias=False, alphas = None, kernel_ridge = False, alpha = None):\n",
    "    block_losses = {}\n",
    "    loss = 0\n",
    "    pred, ridges = model.fit_ridge_analytical(return_matrix = False, set_bias = set_bias, kernel_ridge = kernel_ridge, alphas = alphas, alpha = alpha)\n",
    "\n",
    "    for (key, block) in pred.items():\n",
    "        block_loss=torch.norm(block.values - target_blocks[key].values)**2\n",
    "        loss += block_loss\n",
    "        \n",
    "        block_losses[tuple(key.values)] = block_loss\n",
    "\n",
    "    # print(np.sum(list(block_losses.values())))\n",
    "    return loss, pred, ridges, block_losses#, kernels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "900116f8-66a7-4efe-bd3c-44d4beeb31f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0980e-05)\n"
     ]
    }
   ],
   "source": [
    "model_ridge = LinearModelPeriodic(twocfeat = hfeat, \n",
    "                                  target_blocks = target_coupled_blocks,\n",
    "                                  frames = dataset.structures, orbitals = dataset.basis, \n",
    "                                  device = device)\n",
    "\n",
    "model_ridge = model_ridge.double()\n",
    "loss_ridge_bias, pred_ridge_bias, ridges_bias, loss_blocks = train_ridge(model_ridge, \n",
    "                                                                         target_coupled_blocks,\n",
    "                                                                         set_bias = True,\n",
    "                                                                         kernel_ridge = False,\n",
    "                                                                         alphas = np.logspace(-10, -2, 100),\n",
    "                                                                         # alphas = np.logspace(-30, -6, 100),\n",
    "                                                                         # alpha = 1e-4\n",
    "                                                                        )\n",
    "print(loss_ridge_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486cb39c-aa20-451d-a1ad-e78291f6c419",
   "metadata": {},
   "source": [
    "Compute the feature covariance matrix and diagonalize it. Project the weights onto the covariance eigenstates at each training iteration (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3f59d-0717-4063-a73f-df08b980acdb",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "756af992-e3a3-4104-8b53-bee3925a94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.metrics import L2_loss, L2_kspace_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67409ac9-7aa9-4c84-bbd3-ac205b2528d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.models.linear import LinearModelPeriodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d49a77a-ca50-446b-9e3a-3d72844eeb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.utils.pbc_utils import blocks_to_matrix\n",
    "from mlelec.utils.symmetry import ClebschGordanReal\n",
    "target_kspace = dataset.fock_kspace #orch.stack([kfock[ifr][:1] for ifr in range(3)])\n",
    "CG = ClebschGordanReal(lmax = 3, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6737eca-e2db-4c68-9302-8c11612cb010",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = LinearModelPeriodic(twocfeat = hfeat, \n",
    "                            target_blocks = target_coupled_blocks,\n",
    "                            frames = dataset.structures, orbitals = dataset.basis, \n",
    "                            device = device,\n",
    "                            bias = True,\n",
    "                            nhidden = 512, \n",
    "                            nlayers = 1,\n",
    "                            train_kspace = False,\n",
    "                            activation = 'SiLU',\n",
    "                            apply_norm = True\n",
    "                           )\n",
    "\n",
    "model = model.double()\n",
    "# for p, (_, b) in zip(model.parameters(), target_coupled_blocks.items()):\n",
    "#     p.data = torch.ones_like(p.data)\n",
    "\n",
    "losses = {}\n",
    "para = {}\n",
    "grad = {}\n",
    "learning_rates = {}\n",
    "last_layer_kernel = {}\n",
    "nepoch = 500\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.6, patience=50, verbose=True)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200, 400, 600, 800], gamma=1e-1)\n",
    "optimizers = []\n",
    "schedulers = []\n",
    "for i, key in enumerate(model.model):\n",
    "    # optimizers.append(torch.optim.Adam(model.model[key].parameters(), lr = 1e-1)) #, betas = (0.8, 0.9)))\n",
    "    optimizers.append(torch.optim.AdamW(model.model[key].parameters(), lr = 1e-3, betas = (0.8, 0.9)))\n",
    "    schedulers.append(torch.optim.lr_scheduler.ReduceLROnPlateau(optimizers[-1], factor = 0.8, patience = 30, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fb08d8c-d731-4f32-933a-cb33f544cc50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss real  472.3191662686, train loss k  437.9705932919, train loss per prediction 3.32186e-03\n",
      "Epoch      10, train loss real  145.2168755370, train loss k  144.6484815456, train loss per prediction 1.90905e-03\n",
      "Epoch      20, train loss real    0.3794703421, train loss k    0.2756040143, train loss per prediction 8.33302e-05\n",
      "Epoch      30, train loss real    0.6459917496, train loss k    0.5581449209, train loss per prediction 1.18586e-04\n",
      "Epoch      40, train loss real    0.2836925200, train loss k    0.2481200235, train loss per prediction 7.90661e-05\n",
      "Epoch      50, train loss real    0.2107573787, train loss k    0.2004642786, train loss per prediction 7.10686e-05\n",
      "Epoch      60, train loss real    0.1115143833, train loss k    0.1017572025, train loss per prediction 5.06340e-05\n",
      "Epoch      70, train loss real    0.1547327027, train loss k    0.1400800979, train loss per prediction 5.94084e-05\n",
      "Epoch      80, train loss real    0.1140014904, train loss k    0.1091734618, train loss per prediction 5.24467e-05\n",
      "Epoch      90, train loss real    0.1108331970, train loss k    0.1021504264, train loss per prediction 5.07317e-05\n",
      "Epoch     100, train loss real    0.1389418730, train loss k    0.1241306571, train loss per prediction 5.59241e-05\n",
      "Epoch     110, train loss real    0.1058094903, train loss k    0.0996492423, train loss per prediction 5.01068e-05\n",
      "Epoch     120, train loss real    0.1006706809, train loss k    0.0938931716, train loss per prediction 4.86381e-05\n",
      "Epoch     130, train loss real    0.0830207936, train loss k    0.0740099606, train loss per prediction 4.31822e-05\n",
      "Epoch     140, train loss real    0.0792017538, train loss k    0.0749433467, train loss per prediction 4.34536e-05\n",
      "Epoch     150, train loss real    0.0676858641, train loss k    0.0642153589, train loss per prediction 4.02234e-05\n",
      "Epoch     160, train loss real    0.0763711180, train loss k    0.0731801223, train loss per prediction 4.29394e-05\n",
      "Epoch     170, train loss real    0.0573562457, train loss k    0.0546844250, train loss per prediction 3.71186e-05\n",
      "Epoch     180, train loss real    0.0757357487, train loss k    0.0700933495, train loss per prediction 4.20240e-05\n",
      "Epoch     190, train loss real    0.0505679764, train loss k    0.0489417037, train loss per prediction 3.51155e-05\n",
      "Epoch     200, train loss real    0.0773426565, train loss k    0.0699600126, train loss per prediction 4.19841e-05\n",
      "Epoch     210, train loss real    0.0545236045, train loss k    0.0513323058, train loss per prediction 3.59629e-05\n",
      "Epoch     220, train loss real    0.0832398976, train loss k    0.0751826644, train loss per prediction 4.35229e-05\n",
      "Epoch     230, train loss real    0.0630944877, train loss k    0.0570389015, train loss per prediction 3.79092e-05\n",
      "Epoch     240, train loss real    0.0643655095, train loss k    0.0597335795, train loss per prediction 3.87944e-05\n",
      "Epoch     250, train loss real    0.0501848727, train loss k    0.0461161086, train loss per prediction 3.40868e-05\n",
      "Epoch     260, train loss real    0.0499005363, train loss k    0.0480914062, train loss per prediction 3.48091e-05\n",
      "Epoch     270, train loss real    0.0476895066, train loss k    0.0449397014, train loss per prediction 3.36492e-05\n",
      "Epoch     280, train loss real    0.0468307417, train loss k    0.0453434914, train loss per prediction 3.38000e-05\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00289: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch     290, train loss real    0.0183897479, train loss k    0.0175801730, train loss per prediction 2.10461e-05\n",
      "Epoch     300, train loss real    0.0337993081, train loss k    0.0317557364, train loss per prediction 2.82859e-05\n",
      "Epoch     310, train loss real    0.0239457726, train loss k    0.0240859337, train loss per prediction 2.46344e-05\n",
      "Epoch     320, train loss real    0.0446244733, train loss k    0.0402105478, train loss per prediction 3.18295e-05\n",
      "Epoch     330, train loss real    0.0310677634, train loss k    0.0303774942, train loss per prediction 2.76653e-05\n",
      "Epoch     340, train loss real    0.0281947243, train loss k    0.0278936253, train loss per prediction 2.65101e-05\n",
      "Epoch     350, train loss real    0.0235344953, train loss k    0.0211142662, train loss per prediction 2.30647e-05\n",
      "Epoch     360, train loss real    0.0312299097, train loss k    0.0295020931, train loss per prediction 2.72638e-05\n",
      "Epoch     370, train loss real    0.0240527414, train loss k    0.0234476607, train loss per prediction 2.43058e-05\n",
      "Epoch     380, train loss real    0.0271353831, train loss k    0.0256405391, train loss per prediction 2.54169e-05\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch     390, train loss real    0.0098522208, train loss k    0.0103993980, train loss per prediction 1.61869e-05\n",
      "Epoch     400, train loss real    0.0206416658, train loss k    0.0198632355, train loss per prediction 2.23710e-05\n",
      "Epoch     410, train loss real    0.0183247997, train loss k    0.0186564487, train loss per prediction 2.16807e-05\n",
      "Epoch     420, train loss real    0.0195829938, train loss k    0.0192902034, train loss per prediction 2.20459e-05\n",
      "Epoch     430, train loss real    0.0153447317, train loss k    0.0150884033, train loss per prediction 1.94976e-05\n",
      "Epoch     440, train loss real    0.0182027702, train loss k    0.0184066981, train loss per prediction 2.15351e-05\n",
      "Epoch     450, train loss real    0.0152543792, train loss k    0.0154768712, train loss per prediction 1.97470e-05\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00455: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch     460, train loss real    0.0101047616, train loss k    0.0092676272, train loss per prediction 1.52807e-05\n",
      "Epoch     470, train loss real    0.0122468270, train loss k    0.0127394771, train loss per prediction 1.79158e-05\n",
      "Epoch     480, train loss real    0.0109373125, train loss k    0.0118795583, train loss per prediction 1.73005e-05\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch     490, train loss real    0.0030826207, train loss k    0.0042036773, train loss per prediction 1.02914e-05\n"
     ]
    }
   ],
   "source": [
    "from mlelec.utils.twocenter_utils import _to_uncoupled_basis, map_targetkeys_to_featkeys\n",
    "from mlelec.utils.pbc_utils import precompute_phase, TMap_bloch_sums\n",
    "\n",
    "phase, indices = precompute_phase(target_blocks, dataset, cutoff = cutoff)\n",
    "\n",
    "loss_real = []\n",
    "loss_k = []\n",
    "\n",
    "new_sched = True\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    model.train(True)\n",
    "\n",
    "    for ik, key in enumerate(model.model):\n",
    "        optimizers[ik].zero_grad()\n",
    "    \n",
    "    pred = model()\n",
    "    unc_pred = _to_uncoupled_basis(pred, cg = CG)\n",
    "    pred_kspace = TMap_bloch_sums(unc_pred, phase, indices)\n",
    "    \n",
    "    n_predictions = sum([np.prod(b.values.shape) for _, b in pred.items()])\n",
    "\n",
    "    if epoch < 0:\n",
    "\n",
    "        # Compute the loss for each block\n",
    "        all_losses, epoch_loss = L2_loss(pred, target_coupled_blocks, loss_per_block = True)\n",
    "\n",
    "        # Total loss\n",
    "        epoch_loss = epoch_loss.item()\n",
    "        \n",
    "        # Append the values of the loss to a list\n",
    "        loss_k.append(L2_loss(pred_kspace, k_target_blocks, norm = 2*12*12).item())\n",
    "        loss_real.append(epoch_loss)\n",
    "\n",
    "        # Loop through submodels and backpropagate\n",
    "        for ik, (loss, key) in enumerate(zip(all_losses, model.model)):\n",
    "            loss.backward(retain_graph = False)\n",
    "            torch.nn.utils.clip_grad_norm_(model.model[key].parameters(), 1)\n",
    "            optimizers[ik].step()\n",
    "            schedulers[ik].step(loss)\n",
    "            \n",
    "            if key not in losses:\n",
    "                losses[key] = [loss.item()]\n",
    "                learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "            else:\n",
    "                losses[key].append(loss.item())\n",
    "                learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "                \n",
    "    else:\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = L2_loss(pred_kspace, k_target_blocks, norm = 2*12*12)\n",
    "\n",
    "        # Total loss \n",
    "        epoch_loss = loss.item()\n",
    "        \n",
    "        # Append the values of the loss to a list\n",
    "        loss_real.append(L2_loss(pred, target_coupled_blocks).item())\n",
    "        loss_k.append(epoch_loss)\n",
    "\n",
    "        loss.backward(retain_graph = True)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        for ik, key in enumerate(model.model):\n",
    "            optimizers[ik].step()\n",
    "            schedulers[ik].step(epoch_loss/len(model.model))\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss on all blocks {epoch_loss:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        print(f\"Epoch {epoch:>7d}, train loss real {loss_real[-1]:>15.10f}, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81430533-b638-4abd-a770-72d328513ad4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss on all blocks  472.3191662686, train loss per prediction 3.44967e-03\n",
      "Epoch      10, train loss on all blocks  144.9112405630, train loss per prediction 1.91078e-03\n",
      "Epoch      20, train loss on all blocks    0.3937676111, train loss per prediction 9.96046e-05\n",
      "Epoch      30, train loss on all blocks    0.4370468295, train loss per prediction 1.04936e-04\n",
      "Epoch 00032: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00035: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00037: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00037: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch      40, train loss on all blocks    1.9360444597, train loss per prediction 2.20860e-04\n",
      "Epoch      50, train loss on all blocks    0.5686732944, train loss per prediction 1.19699e-04\n",
      "Epoch 00058: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch      60, train loss on all blocks    0.3829309893, train loss per prediction 9.82245e-05\n",
      "Epoch 00063: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00064: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00071: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00071: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00071: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch      70, train loss on all blocks    0.3233899506, train loss per prediction 9.02657e-05\n",
      "Epoch 00073: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00075: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00075: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00075: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00077: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00077: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch      80, train loss on all blocks    0.0724650750, train loss per prediction 4.27291e-05\n",
      "Epoch 00085: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00087: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00089: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00090: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00091: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch      90, train loss on all blocks    0.1206365630, train loss per prediction 5.51314e-05\n",
      "Epoch 00093: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00094: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00095: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00097: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00098: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00098: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00100: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00101: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch     100, train loss on all blocks  424.0013460379, train loss per prediction 3.26846e-03\n",
      "Epoch 00104: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00106: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00108: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00108: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00110: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00111: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch     110, train loss on all blocks  423.9975152807, train loss per prediction 3.26844e-03\n",
      "Epoch 00112: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00113: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00115: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00116: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00117: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00118: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00118: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00120: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00121: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00121: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00121: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00121: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch     120, train loss on all blocks  423.9982452707, train loss per prediction 3.26845e-03\n",
      "Epoch 00122: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00123: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 00124: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00124: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00125: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00126: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00126: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00128: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00129: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00129: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00130: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00131: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00131: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00131: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch     130, train loss on all blocks  423.9978482904, train loss per prediction 3.26845e-03\n",
      "Epoch 00132: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00135: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00137: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00139: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00139: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00141: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch     140, train loss on all blocks  423.9988308608, train loss per prediction 3.26845e-03\n",
      "Epoch 00142: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00143: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00144: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00146: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00147: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00148: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00149: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00149: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00151: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch     150, train loss on all blocks  424.0046280472, train loss per prediction 3.26847e-03\n",
      "Epoch 00152: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00152: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00152: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00152: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00153: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00154: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00155: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00155: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00156: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00157: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00157: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00159: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00160: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00160: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00161: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch     160, train loss on all blocks  423.9987492071, train loss per prediction 3.26845e-03\n",
      "Epoch 00162: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00162: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00162: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00163: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00166: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00168: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00170: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00170: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch     170, train loss on all blocks  423.9989255825, train loss per prediction 3.26845e-03\n",
      "Epoch 00172: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00173: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00174: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00175: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00177: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00178: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00179: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00180: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00180: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch     180, train loss on all blocks  423.9980935677, train loss per prediction 3.26845e-03\n",
      "Epoch 00182: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00183: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00183: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00183: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00183: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00184: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00185: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00186: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00186: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00187: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00188: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00188: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00190: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00191: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00191: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch     190, train loss on all blocks  423.9972243670, train loss per prediction 3.26844e-03\n",
      "Epoch 00192: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00193: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00193: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00193: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00194: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00197: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00199: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00201: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00201: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch     200, train loss on all blocks  423.9969406856, train loss per prediction 3.26844e-03\n",
      "Epoch 00203: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00204: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00205: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00206: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00208: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00209: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00210: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00211: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00211: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch     210, train loss on all blocks  423.9970049695, train loss per prediction 3.26844e-03\n",
      "Epoch 00213: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00214: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00214: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00214: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00214: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00215: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00216: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00217: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00217: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00218: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00219: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00219: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00221: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch     220, train loss on all blocks  423.9966827962, train loss per prediction 3.26844e-03\n",
      "Epoch 00222: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00222: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00223: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00224: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00224: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00224: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00225: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00228: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00230: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch     230, train loss on all blocks  423.9966339551, train loss per prediction 3.26844e-03\n",
      "Epoch 00232: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00232: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00234: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00235: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00236: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00237: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00239: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00240: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00241: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch     240, train loss on all blocks  423.9966281887, train loss per prediction 3.26844e-03\n",
      "Epoch 00242: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00242: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00244: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00245: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00245: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00245: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00245: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00246: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00247: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00248: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00248: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00249: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00250: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00250: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch     250, train loss on all blocks  423.9967701191, train loss per prediction 3.26844e-03\n",
      "Epoch 00252: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00253: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00253: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00254: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00255: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00255: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00255: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00256: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00259: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00261: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch     260, train loss on all blocks  423.9964682181, train loss per prediction 3.26844e-03\n",
      "Epoch 00263: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00263: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00265: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00266: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00267: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00268: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00270: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00271: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch     270, train loss on all blocks  423.9966843944, train loss per prediction 3.26844e-03\n",
      "Epoch 00272: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00273: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00273: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00275: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00276: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00276: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00276: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00276: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00277: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00278: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00279: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00279: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00280: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00281: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00281: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch     280, train loss on all blocks  423.9964950786, train loss per prediction 3.26844e-03\n",
      "Epoch 00283: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00284: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00284: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00285: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00286: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00286: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00286: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00287: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00290: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch     290, train loss on all blocks  423.9964005563, train loss per prediction 3.26844e-03\n",
      "Epoch 00292: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00294: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00294: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00296: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00297: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00298: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00299: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00301: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch     300, train loss on all blocks  423.9964205994, train loss per prediction 3.26844e-03\n",
      "Epoch 00302: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00303: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00304: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00304: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00306: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00307: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00307: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00307: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00307: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00308: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00309: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00310: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00310: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch     310, train loss on all blocks  423.9964373010, train loss per prediction 3.26844e-03\n",
      "Epoch 00312: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00312: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00314: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00315: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00315: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00316: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00317: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00317: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00317: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00318: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00321: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch     320, train loss on all blocks  423.9963608560, train loss per prediction 3.26844e-03\n",
      "Epoch 00323: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00325: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00325: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00327: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00328: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00329: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00330: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch     330, train loss on all blocks  423.9963552102, train loss per prediction 3.26844e-03\n",
      "Epoch 00332: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00333: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00334: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00335: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00335: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00337: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00338: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00338: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00338: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00338: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00339: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00340: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00341: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00341: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch     340, train loss on all blocks  423.9963790536, train loss per prediction 3.26844e-03\n",
      "Epoch 00342: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00343: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00343: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00345: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00346: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00346: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00347: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00348: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00348: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00348: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00349: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch     350, train loss on all blocks  423.9963915030, train loss per prediction 3.26844e-03\n",
      "Epoch 00352: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00354: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00356: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00356: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00358: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00359: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00360: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00361: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch     360, train loss on all blocks  423.9963576669, train loss per prediction 3.26844e-03\n",
      "Epoch 00363: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00364: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00365: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00366: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00366: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00368: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00369: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00369: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00369: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00369: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00370: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00371: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch     370, train loss on all blocks  423.9963395467, train loss per prediction 3.26844e-03\n",
      "Epoch 00372: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00372: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00373: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00374: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00374: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00376: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00377: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00377: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00378: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00379: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00379: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00379: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00380: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch     380, train loss on all blocks  423.9963246376, train loss per prediction 3.26844e-03\n",
      "Epoch 00383: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00385: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00387: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00387: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00389: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00390: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00391: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch     390, train loss on all blocks  423.9963304806, train loss per prediction 3.26844e-03\n",
      "Epoch 00392: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00394: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00395: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00396: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00397: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00397: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00399: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00400: reducing learning rate of group 0 to 6.8719e-05.\n",
      "Epoch 00400: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00400: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00400: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00401: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch     400, train loss on all blocks  423.9963663188, train loss per prediction 3.26844e-03\n",
      "Epoch 00402: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00403: reducing learning rate of group 0 to 6.8719e-05.\n",
      "Epoch 00403: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00404: reducing learning rate of group 0 to 6.8719e-05.\n",
      "Epoch 00405: reducing learning rate of group 0 to 6.8719e-05.\n",
      "Epoch 00405: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00407: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00408: reducing learning rate of group 0 to 6.8719e-05.\n",
      "Epoch 00408: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00409: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00410: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00410: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00410: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00411: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch     410, train loss on all blocks  423.9963181470, train loss per prediction 3.26844e-03\n",
      "Epoch 00414: reducing learning rate of group 0 to 6.8719e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from mlelec.utils.twocenter_utils import _to_uncoupled_basis, map_targetkeys_to_featkeys\n",
    "# from mlelec.utils.pbc_utils import precompute_phase, TMap_bloch_sums\n",
    "# # kpts_ = torch.from_numpy(np.array(kpts_train)).to(device)\n",
    "\n",
    "# phase, where_inv = precompute_phase(target_blocks, dataset, cutoff = cutoff)\n",
    "\n",
    "# loss_real = []\n",
    "# loss_k = []\n",
    "\n",
    "# new_sched = True\n",
    "# for epoch in range(nepoch):\n",
    "\n",
    "#     model.train(True)\n",
    "\n",
    "#     for ik, key in enumerate(model.model):\n",
    "#         optimizers[ik].zero_grad()\n",
    "    \n",
    "#     pred = model()\n",
    "#     n_predictions = sum([np.prod(b.values.shape) for _, b in pred.items()])\n",
    "\n",
    "#     if epoch < 100:\n",
    "#         all_losses, epoch_loss = L2_loss(pred, target_coupled_blocks, loss_per_block = True)\n",
    "#         epoch_loss = epoch_loss.item()\n",
    "#         for ik, (loss, key) in enumerate(zip(all_losses, model.model)):\n",
    "#             loss.backward(retain_graph = False)\n",
    "#             torch.nn.utils.clip_grad_norm_(model.model[key].parameters(), 1)\n",
    "#             optimizers[ik].step()\n",
    "#             # schedulers[ik].step()\n",
    "#             schedulers[ik].step(loss)\n",
    "            \n",
    "#             if key not in losses:\n",
    "#                 losses[key] = [loss.item()]\n",
    "#                 learning_rates[key] = [schedulers[ik].state_dict()['_last_lr'][0]]\n",
    "#             else:\n",
    "#                 losses[key].append(loss.item())\n",
    "#                 learning_rates[key].append(schedulers[ik].state_dict()['_last_lr'][0])\n",
    "            \n",
    "#         unc_pred = _to_uncoupled_basis(pred, cg = CG)\n",
    "#         pred_kspace = TMap_bloch_sums(unc_pred, phase, where_inv)\n",
    "#         loss_k.append(L2_loss(pred_kspace, k_target_blocks, norm = 2*144).item())\n",
    "#         loss_real.append(epoch_loss)\n",
    "#         # for ik, (key, loss) in enumerate(model.model):\n",
    "        \n",
    "#     else:\n",
    "#         # if new_sched:\n",
    "#         #     pred_before_k = pred.copy()\n",
    "\n",
    "#         #     optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)\n",
    "#         #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.5, patience = 200, verbose=True)\n",
    "#         #     # for s, o in zip(schedulers, optimizers):\n",
    "#         #     #     s = torch.optim.lr_scheduler.ReduceLROnPlateau(o, factor = 0.5, patience = 100, verbose=True)\n",
    "#         #     new_sched = False\n",
    "\n",
    "#         #     _loss = L2_kspace_loss(pred, \n",
    "#         #                       target_kspace, \n",
    "#         #                       dataset, \n",
    "#         #                       cg = CG, \n",
    "#         #                       kpts = kpts_,\n",
    "#         #                       norm = 1\n",
    "#         #                       )\n",
    "#         #     print(\"Initial k-space loss = \", _loss.item())\n",
    "\n",
    "#         unc_pred = _to_uncoupled_basis(pred, cg = CG)\n",
    "#         pred_kspace = TMap_bloch_sums(unc_pred, phase, where_inv)\n",
    "#         loss = L2_loss(pred_kspace, k_target_blocks, norm = 2*144)\n",
    "#         loss_real.append(L2_loss(pred, target_coupled_blocks).item())\n",
    "#         # pred_kspace = None # Allow to deallocate\n",
    "#         # loss = L2_kspace_loss(pred, \n",
    "#         #               target_kspace, \n",
    "#         #               dataset, \n",
    "#         #               cg = CG, \n",
    "#         #               kpts = kpts_,\n",
    "#         #               norm = 1\n",
    "#         #               ) \n",
    "\n",
    "#         loss.backward(retain_graph = False)\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "#         epoch_loss = loss.item()\n",
    "\n",
    "#         loss_k.append(epoch_loss)\n",
    "#         # for key in model.model:\n",
    "#         #     if key not in losses:\n",
    "#         #         losses[key] = [epoch_loss/len(model.model)]\n",
    "#         #     else:\n",
    "#         #         losses[key].append(epoch_loss/len(model.model))\n",
    "    \n",
    "#         for ik, key in enumerate(model.model):\n",
    "#             optimizers[ik].step()\n",
    "#             schedulers[ik].step(epoch_loss)\n",
    "        \n",
    "#         # optimizer.step()\n",
    "#         # scheduler.step(epoch_loss)\n",
    "\n",
    "#     # for i, key in enumerate(model.model):\n",
    "#     #     if key not in para:\n",
    "#     #         para[key] = {}\n",
    "#     #         # grad[key] = []\n",
    "#     #         # optimizers[0].param_groups[0]['lr']\n",
    "#     #     for i, param in enumerate(model.model[key].parameters()):\n",
    "#     #         # grad[key].append(param.grad.norm().item())\n",
    "#     #         if i not in para[key]:\n",
    "#     #             para[key][i] = []\n",
    "#     #         para[key][i].append(torch.clone(param).detach())\n",
    "\n",
    "#     # for k in model.target_blocks.keys:\n",
    "#     #     kl = tuple(k.values)\n",
    "#     #     if kl not in last_layer_kernel:\n",
    "#     #         last_layer_kernel[kl] = []\n",
    "#     #     feat = map_targetkeys_to_featkeys(hfeat, k)\n",
    "#     #     m = model.model[str(kl)].mlp[:-1].forward(feat.values).detach()\n",
    "#     #     # if epoch == 0:\n",
    "#     #     #     print(kl, m.shape)\n",
    "#     #     # m = m.reshape(-1, m.shape[-1])\n",
    "#     #     mTm = torch.einsum('scf,Scf->csS', m, m)\n",
    "#     #     # mTm = torch.einsum('scf,scg->cfg', m, m)\n",
    "#     #     last_layer_kernel[kl].append(mTm)\n",
    "    \n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f\"Epoch {epoch:>7d}, train loss on all blocks {epoch_loss:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "\n",
    "#         # for i, key in enumerate(model.model):\n",
    "#         #     if key not in para:\n",
    "#         #         para[key] = []\n",
    "#         #         grad[key] = []\n",
    "#         #         # optimizers[0].param_groups[0]['lr']\n",
    "#         #     for param in model.model[key].parameters():\n",
    "#         #         grad[key].append(param.grad.norm().item())\n",
    "#         #         para[key].append(param.norm().item())\n",
    "\n",
    "#     #     # validate \n",
    "#     #     valpred = model.predict(hfeat_test, target_test)\n",
    "#     #     if loss_func == 'kspace':\n",
    "#     #         val_loss = L2_kspace_loss(valpred, target_test, dataset)\n",
    "#     #     else:\n",
    "#     #         val_loss = L2_loss(valpred, target_test)\n",
    "        \n",
    "#     #     print(f\"Epoch {epoch} val loss {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7306f6d6-4c49-40d9-8a37-d4e4fbef09c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423.99631527445854"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unc_pred = _to_uncoupled_basis(pred, cg = CG)\n",
    "pred_kspace = TMap_bloch_sums(unc_pred, phase, where_inv)\n",
    "L2_loss(pred_kspace, k_target_blocks, norm = 2*144).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8feea00-ce1d-41e2-ae20-9f354f268df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(loss_k, label='k')\n",
    "plt.loglog(loss_real, label='real')\n",
    "# plt.axhline(np.mean(np.array(loss_k)/np.array(loss_real)), label = f'avg ratio={np.mean(np.array(loss_k)/np.array(loss_real))}', ls = '--', color = 'k')\n",
    "# plt.loglog(np.array(loss_k)/np.array(loss_real), label='ratio', color = 'gold')\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "plt.title('AIMS')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a2b91-ddcc-47fb-a14d-f56efb305649",
   "metadata": {},
   "source": [
    "# Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9888ce5-7977-4be9-aff7-cf8dd372e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(pred, dataset, cutoff):\n",
    "    from mlelec.utils.pbc_utils import inverse_bloch_sum\n",
    "    reconstructed_H_ = blocks_to_matrix(pred, dataset)\n",
    "    reconstructed_H = []\n",
    "    reconstructed_S = []\n",
    "    S = [inverse_bloch_sum(dataset, dataset.overlap_kspace[ifr], ifr, cutoff) for ifr in range(len(dataset.structures))]\n",
    "    for ifr in range(len(reconstructed_H_)):\n",
    "        reconstructed_H.append({})\n",
    "        reconstructed_S.append({})\n",
    "        for T in reconstructed_H_[ifr]:\n",
    "            if T in S[ifr]:\n",
    "                reconstructed_S[ifr][T] = S[ifr][T]\n",
    "                reconstructed_H[ifr][T] = reconstructed_H_[ifr][T].cpu().detach().numpy()\n",
    "            elif tuple(-np.array(T)) in S[ifr]:\n",
    "                T = tuple(-np.array(T))\n",
    "                reconstructed_S[ifr][T] = S[ifr][T]\n",
    "                reconstructed_H[ifr][T] = reconstructed_H_[ifr][T].cpu().detach().numpy()\n",
    "            else:\n",
    "                print(T)\n",
    "                raise ValueError()\n",
    "    return reconstructed_H, reconstructed_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45fb22-59e7-459a-a588-2ca8e15684cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rH_k, rS_k = reconstruct(pred, dataset, cutoff = cutoff)\n",
    "\n",
    "_, true_target_coupled_blocks = get_targets(dataset, cutoff = 8.35, device = device)\n",
    "rH_true, rS_true = reconstruct(true_target_coupled_blocks, dataset, cutoff = 8.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908ddd6-256b-421e-8d51-81658106bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "special_points = {'G': [0.0, 0.0, 0.0], 'K': [0.3333333333333333, 0.3333333333333333, 0.0], 'M': [0.5, 0.0, 0.0]}\n",
    "\n",
    "from mlelec.utils.plot_utils import plot_bands_frame_\n",
    "import matplotlib\n",
    "\n",
    "ifr = 2\n",
    "frame = dataset.structures[ifr]\n",
    "pyscf_cell = dataset.cells[ifr]\n",
    "kmesh = dataset.kmesh[ifr]\n",
    "kpath = frame.cell.bandpath('GMKG', 500, special_points = special_points, pbc = [True, True, False])\n",
    "realfock = np.asarray(list(rH_true[ifr].values())) #np.asarray(list(dataset.fock_realspace[0].values()))\n",
    "realover = np.asarray(list(rS_true[ifr].values())) #np.asarray(list(dataset.overlap_realspace[0].values()))\n",
    "R_vec_rel = [list(T) for T in rH_true[ifr]]\n",
    "\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "ax, b, handle = plot_bands_frame_(frame, realfock, realover, pyscf_cell, kmesh, special_symm = 'hexagonal', \n",
    "                                  factor = 1,\n",
    "                                  kpath = kpath, ax = ax, color = 'k', R_vec_rel_in = R_vec_rel, lw = 3)\n",
    "handles.append(tuple(handle))\n",
    "labels.append('Ref.')\n",
    "\n",
    "for H, S, c, ls, factor, label in zip([rH_k], [rS_k], ['red'], ['--'], [1], ['after kspace']): #np.sqrt(np.prod(kmesh)/len(H[0]))]):\n",
    "    # if label == 'only real space':\n",
    "        # continue\n",
    "    realfock_cutoff = np.asarray(list(H[ifr].values()))\n",
    "    realover_cutoff = np.asarray(list(S[ifr].values()))\n",
    "    R_vec_rel = [list(T) for T in H[ifr]]\n",
    "    ax, b, handle = plot_bands_frame_(frame, realfock_cutoff, realover_cutoff, pyscf_cell, kmesh, \n",
    "                                      factor = factor,\n",
    "                                      lw = 2,\n",
    "                                      R_vec_rel_in = R_vec_rel, special_symm = 'hexagonal', kpath = kpath, ax = ax, color = c, ls = ls)\n",
    "    handles.append(tuple(handle))\n",
    "    labels.append(label)\n",
    "\n",
    "ax.legend(handles, labels)\n",
    "# ax.set_ylim(-35, 43)\n",
    "# nmax = 50\n",
    "# for ifr in range(0, nmax):\n",
    "#     frame = dataset.structures[ifr]\n",
    "#     pyscf_cell = dataset.cells[ifr]\n",
    "#     kmesh = dataset.kmesh[ifr]\n",
    "#     kpath = frame.cell.bandpath('GMKG', 500, special_points = special_points, pbc = [True, True, False])\n",
    "#     realfock = np.asarray(list(dataset.fock_realspace[ifr].values()))\n",
    "#     realover = np.asarray(list(dataset.overlap_realspace[ifr].values()))\n",
    "#     realfock_cutoff = np.asarray(list(reconstructed_H[ifr].values()))\n",
    "#     realover_cutoff = np.asarray(list(reconstructed_S[ifr].values()))\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax, b = plot_bands_frame_(frame, realfock, realover, pyscf_cell, kmesh = [8,8,1], special_symm = 'hexagonal' , \n",
    "#                               kpath = kpath, ax=ax,color = cmap((ifr+1)/nmax))\n",
    "#     ax, b = plot_bands_frame_(frame, realfock_cutoff, realover_cutoff, pyscf_cell, kmesh = [8,8,1], special_symm = 'hexagonal' , \n",
    "#                               kpath = kpath, ax = ax, color = 'k', ls = ':')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73177d16-25a9-47ea-9e5b-f3831dd63a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.utils.plot_utils import plot_block_errors\n",
    "fig,ax,ax_loss=plot_block_errors(target_coupled_blocks, pred, plot_loss = True)\n",
    "ax_loss.set_ylim(1e-5,)\n",
    "ax.set_ylim(1e-5,)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
