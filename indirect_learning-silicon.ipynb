{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3d8f12c-62ee-4d46-b766-c3b49d817fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635a399b-c69b-4438-bac6-4ba0712d0d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegolo/micromamba/envs/sci/lib/python3.11/site-packages/pyscf/dft/libxc.py:771: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, corresponding to the original definition by Stephens et al. (issue 1480) and the same as the B3LYP functional in Gaussian. To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read\n",
    "from ase.visualize import view\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import torch \n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import rascaline.torch\n",
    "\n",
    "import metatensor.torch as mts\n",
    "\n",
    "from metatensor.torch import Labels, TensorBlock, TensorMap\n",
    "from metatensor.learn import IndexedDataset, DataLoader\n",
    "from metatensor.learn.data import group as mts_group, group_and_join as group_and_join_mts\n",
    "\n",
    "from mlelec.data.dataset import QMDataset, split_by_Aij_mts\n",
    "from mlelec.utils.twocenter_utils import _to_coupled_basis\n",
    "from mlelec.utils.pbc_utils import matrix_to_blocks, kmatrix_to_blocks, TMap_bloch_sums, precompute_phase, kblocks_to_matrix, kmatrix_to_blocks, blocks_to_matrix, matrix_to_blocks\n",
    "from mlelec.utils.plot_utils import print_matrix, matrix_norm, block_matrix_norm, plot_block_errors\n",
    "from mlelec.features.acdc import compute_features\n",
    "from mlelec.utils.target_utils import get_targets\n",
    "from mlelec.models.linear_integrated import LinearModelPeriodic\n",
    "from mlelec.metrics import L2_loss, L2_loss_meanzero\n",
    "from mlelec.data.mldataset import MLDataset\n",
    "\n",
    "import xitorch\n",
    "from xitorch.linalg import symeig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0131f6a-72a2-445e-9384-0ba95406c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "orbitals = {\n",
    "    'sto-3g': {14: [[1,0,0], [2,0,0], [3,0,0], [2,1,-1], [2,1,0], [2,1,1], [3,1,-1], [3,1,0], [3,1,1]]}, \n",
    "    'def2svp': {6: [[1,0,0],[2,0,0],[3,0,0],[2,1,1], [2,1,-1],[2,1,0], [3,1,1], [3,1,-1],[3,1,0], [3,2,-2], [3,2,-1],[3,2,0], [3,2,1],[3,2,2]]},\n",
    "    'gthdzvp': {14: [[2,0,0], [3,0,0], [2,1,1], [2,1,-1], [2,1,0], [3,1,1], [3,1,-1], [3,1,0], [3,2,-2], [3,2,-1], [3,2,0], [3,2,1], [3,2,2]]},\n",
    "    'gthszv':  {14: [[3,0,0], [3,1,1], [3,1,-1], [3,1,0]]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61756a8d-5fbe-4fd2-9521-4a4604762d6d",
   "metadata": {},
   "source": [
    "# QM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0a40d-b9f9-4cb6-b501-683d7d25b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.build import bulk\n",
    "from ase import Atoms\n",
    "frames = []\n",
    "frames.append(bulk('NaCl', 'rocksalt', a = 5.7, cubic = True))\n",
    "frames.append(bulk('Si', 'diamond', a = 6, cubic = True))\n",
    "frames.append(bulk('MgO', 'zincblende', a = 4, cubic = True))\n",
    "kfock = []\n",
    "kover = []\n",
    "kmesh = [[1,2,3],[4,5,6],[1,3,5]]\n",
    "orbitals = {\n",
    "               11: [[1,0,0],[2,0,0],[3,0,0],[2,1,-1], [2,1,0],[2,1,1]], \n",
    "               17: [[1,0,0],[2,0,0],[3,0,0],[2,1,-1], [2,1,0],[2,1,1], [3,1,-1],[3,1,0],[3,1,1]], \n",
    "               14: [[1,0,0],[2,0,0],[3,0,0],[2,1,-1], [2,1,0],[2,1,1], [3,1,-1],[3,1,0],[3,1,1]],\n",
    "               12: [[1,0,0],[2,0,0],[3,0,0],[2,1,-1], [2,1,0],[2,1,1]],\n",
    "               8: [[1,0,0],[2,0,0],[2,1,-1], [2,1,0],[2,1,1]],\n",
    "               }\n",
    "for ifr, f in enumerate(frames):\n",
    "    nao = np.sum([len(orbitals[i]) for i in f.numbers])\n",
    "    a = torch.randn(np.prod(kmesh[ifr]), nao,nao, dtype = torch.complex128)\n",
    "    a = a+a.transpose(2,1).conj()\n",
    "    kfock.append(a)\n",
    "    b =torch.randn(np.prod(kmesh[ifr]), nao,nao, dtype = torch.complex128)\n",
    "    b = b+b.transpose(2,1)\n",
    "    kover.append(torch.einsum('aij, akj ->aik', b,b.conj()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb8d851-377b-4bd5-a632-397f666731e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING!\n",
      "  Very diffused basis functions are found in the basis set. They may lead to severe\n",
      "  linear dependence and numerical instability.  You can set  cell.exp_to_discard=0.1\n",
      "  to remove the diffused Gaussians whose exponents are less than 0.1.\n",
      "\n",
      "WARNING!\n",
      "  Very diffused basis functions are found in the basis set. They may lead to severe\n",
      "  linear dependence and numerical instability.  You can set  cell.exp_to_discard=0.1\n",
      "  to remove the diffused Gaussians whose exponents are less than 0.1.\n",
      "\n",
      "WARNING!\n",
      "  Very diffused basis functions are found in the basis set. They may lead to severe\n",
      "  linear dependence and numerical instability.  You can set  cell.exp_to_discard=0.1\n",
      "  to remove the diffused Gaussians whose exponents are less than 0.1.\n",
      "\n",
      "WARNING!\n",
      "  Very diffused basis functions are found in the basis set. They may lead to severe\n",
      "  linear dependence and numerical instability.  You can set  cell.exp_to_discard=0.1\n",
      "  to remove the diffused Gaussians whose exponents are less than 0.1.\n",
      "\n",
      "WARNING!\n",
      "  Very diffused basis functions are found in the basis set. They may lead to severe\n",
      "  linear dependence and numerical instability.  You can set  cell.exp_to_discard=0.1\n",
      "  to remove the diffused Gaussians whose exponents are less than 0.1.\n",
      "\n",
      "WARNING!\n",
      "  Very diffused basis functions are found in the basis set. They may lead to severe\n",
      "  linear dependence and numerical instability.  You can set  cell.exp_to_discard=0.1\n",
      "  to remove the diffused Gaussians whose exponents are less than 0.1.\n",
      "\n",
      "WARNING!\n",
      "  Very diffused basis functions are found in the basis set. They may lead to severe\n",
      "  linear dependence and numerical instability.  You can set  cell.exp_to_discard=0.1\n",
      "  to remove the diffused Gaussians whose exponents are less than 0.1.\n",
      "\n",
      "WARNING!\n",
      "  Very diffused basis functions are found in the basis set. They may lead to severe\n",
      "  linear dependence and numerical instability.  You can set  cell.exp_to_discard=0.1\n",
      "  to remove the diffused Gaussians whose exponents are less than 0.1.\n",
      "\n",
      "WARNING!\n",
      "  Very diffused basis functions are found in the basis set. They may lead to severe\n",
      "  linear dependence and numerical instability.  You can set  cell.exp_to_discard=0.1\n",
      "  to remove the diffused Gaussians whose exponents are less than 0.1.\n",
      "\n",
      "WARNING!\n",
      "  Very diffused basis functions are found in the basis set. They may lead to severe\n",
      "  linear dependence and numerical instability.  You can set  cell.exp_to_discard=0.1\n",
      "  to remove the diffused Gaussians whose exponents are less than 0.1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "workdir = '/scratch/pegolo/silicon_deringer'\n",
    "START = 0 \n",
    "STOP = 10\n",
    "ORBS = 'gthszv'\n",
    "baseline = False\n",
    "\n",
    "root = f'{workdir}/{ORBS}'\n",
    "data_dir = root\n",
    "\n",
    "indices = range(START,STOP)\n",
    "\n",
    "frames = [read(f'{data_dir}/{i}/cell_{i}.xyz') for i in indices]\n",
    "rfock = [np.load(f\"{data_dir}/{i}/realfock_{i}.npy\", allow_pickle = True).item() for i in indices]\n",
    "# rfock0 = [np.load(f\"{data_dir}/nscf/realfock_{i}.npy\", allow_pickle = True).item() for i in range(START, STOP)]\n",
    "rover = [np.load(f\"{data_dir}/{i}/realoverlap_{i}.npy\", allow_pickle = True).item() for i in indices]\n",
    "kmesh = [np.loadtxt(f'{data_dir}/{i}/kmesh_{i}.dat', dtype = np.int32).tolist() for i in indices]\n",
    "\n",
    "if baseline:\n",
    "    for H, H0 in zip(rfock, rfock0):\n",
    "        for T in H:\n",
    "            H[T] -= H0[T]\n",
    "\n",
    "dataset = QMDataset(frames = frames, \n",
    "                   kmesh = kmesh, \n",
    "                   dimension = 3,\n",
    "                   fock_realspace = rfock, \n",
    "                   overlap_realspace = rover, \n",
    "                   device = device, \n",
    "                   orbs = orbitals[ORBS],\n",
    "                   orbs_name = ORBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d7fe85-f60f-47b3-a0c1-90b94b346309",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_radial  = 6\n",
    "max_angular = 4\n",
    "atomic_gaussian_width = 0.3\n",
    "cutoff = 6\n",
    "\n",
    "hypers_pair = {'cutoff': cutoff,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': atomic_gaussian_width,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}}}\n",
    "\n",
    "\n",
    "hypers_atom = {'cutoff': 4,\n",
    "               'max_radial': max_radial,\n",
    "               'max_angular': max_angular,\n",
    "               'atomic_gaussian_width': 0.5,\n",
    "               'center_atom_weight': 1,\n",
    "               \"radial_basis\": {\"Gto\": {}},\n",
    "               \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}}}\n",
    "\n",
    "\n",
    "return_rho0ij = False\n",
    "both_centers = False\n",
    "LCUT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6bf1939-37c5-4116-ba50-deb4e0b68b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu pair features\n",
      "cpu single center features\n",
      "cpu single center features\n",
      "Features set\n",
      "Items set\n"
     ]
    }
   ],
   "source": [
    "mldata = MLDataset(dataset, item_names = ['fock_blocks', 'fock_kspace', 'overlap_kspace', 'eigenvalues'],\n",
    "                   cutoff = hypers_pair['cutoff'],\n",
    "                   hypers_atom = hypers_atom,\n",
    "                   hypers_pair = hypers_pair,\n",
    "                   lcut = 3,\n",
    "                   orbitals_to_properties = True,\n",
    "                   train_frac = 1.0,\n",
    "                   test_frac = 0.0,\n",
    "                   val_frac = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15009490-bc7a-4693-a437-13f0e3cbb908",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(mldata.train_dataset, batch_size = 2, \n",
    "                collate_fn = lambda x: mldata.group_and_join(x, join_kwargs = {'different_keys': 'union', 'remove_tensor_name': True}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c6f88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = LinearModelPeriodic(twocfeat = mldata.features, \n",
    "                            target_blocks = mldata.model_metadata,\n",
    "                            frames = dataset.structures, \n",
    "                            orbitals = dataset.basis, \n",
    "                            device = device,\n",
    "                            bias = True,\n",
    "                            nhidden = 4, \n",
    "                            nlayers = 1,\n",
    "                            activation = 'SiLU',\n",
    "                            apply_norm = True\n",
    "                           )\n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc2393-765a-4953-bcab-4f3cd78af2b1",
   "metadata": {},
   "source": [
    "### Train k matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "223428a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b434eca5-14db-4fdd-8ef5-e4d5eb31f5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mprecompute_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Software/mlelec/src/mlelec/utils/pbc_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precompute_phase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5513af72-413d-4a3b-8547-69ccf4b241de",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase, _, _ = precompute_phase(mts.join(mldata.items['fockblocks'], axis = 'samples', different_keys =  'union', remove_tensor_name = True), dataset, cutoff = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fd7d78f-13f9-421a-9f89-5615ef533ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mTMap_bloch_sums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget_blocks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkpts_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_dummy_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Software/mlelec/src/mlelec/utils/pbc_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TMap_bloch_sums?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4db227b8-a0b6-4871-b91f-c926f3ef7ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss 2505.3571232538\n",
      "Epoch       1, train loss 2050.6508184933\n",
      "Epoch       2, train loss 1723.6380957907\n",
      "Epoch       3, train loss 1631.7701570154\n",
      "Epoch       4, train loss 1496.8209881533\n",
      "Epoch       5, train loss 1403.7753526382\n",
      "Epoch       6, train loss 1325.7093931115\n",
      "Epoch       7, train loss 1264.0866413915\n",
      "Epoch       8, train loss 1230.9570233616\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m L2_loss(pred_k, targ_k)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# loss = L2_loss(pred_k, batch.fock_blocks)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# loss = L2_loss(HK, batch.fock_kspace)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sci/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nepoch = 5000\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Train against real space targets\n",
    "    for ib, batch in enumerate(dl):\n",
    "        \n",
    "        model.train(True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model.forward(batch.features, mldata.model_metadata)\n",
    "        # pred_k = TMap_bloch_sums(pred, phase)\n",
    "        # targ_k = TMap_bloch_sums(batch.fock_blocks, phase)\n",
    "        HT = blocks_to_matrix(pred, dataset, detach = False)\n",
    "        HK = [dataset.bloch_sum(HT)[ifr] for ifr in batch.sample_id]\n",
    "    \n",
    "        # if len(HK) == 1:\n",
    "        #     overlap = [batch.overlap_kspace]\n",
    "        # else:\n",
    "        #     overlap = batch.overlap_kspace\n",
    "            \n",
    "        # pred_eigvals = [torch.stack([compute_eigval(h, s) for h, s in zip(hk, sk)]) for hk, sk in zip(HK, overlap)]\n",
    "        # pred_eigvals = [torch.stack([compute_eigval(h, s) for h, s in zip(HK, batch.overlap_kspace[ifr])]) for ifr in batch.sample_id]\n",
    "        # pred_eigvals = torch.stack([torch.linalg.eigvalsh(lowdin_orthogonalize(h, s)) for h, s in zip(HK, batch.overlap_kpoints)])\n",
    "\n",
    "        # loss = loss_fn(pred_eigvals, batch.eigenvalues)\n",
    "        loss = L2_loss(pred_k, targ_k)\n",
    "        # loss = L2_loss(pred_k, batch.fock_blocks)\n",
    "        # loss = L2_loss(HK, batch.fock_kspace)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    scheduler.step(epoch_loss)\n",
    "    \n",
    "    if epoch >= 0: #% 10 == 0:\n",
    "        print(f\"Epoch {epoch:>7d}, train loss {epoch_loss:>15.10f}\") #, avg lr = {np.mean(lr_list)}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss {LOSS:>15.10f}, check loss {LOSSc:>15.10f}, delta {delta:>15.10f}, avg lr = {np.mean(lr_list)}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ed65095a-42d8-4d6f-b8e2-d10dd6e76839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.utils.pbc_utils import blocks_to_matrix_chat, blocks_to_matrix_claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f24ba65-b2ee-4df7-afff-0824ce5b3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = mts.join(mldata.items['fockblocks'], axis = 'samples', different_keys =  'union', remove_tensor_name = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a7b053ee-91af-480f-ae03-98197d4e1380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b1 = blocks_to_matrix_claude(blocks, dataset, detach = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "571891d0-eda9-4033-a09b-4b7959a1e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.66 s ± 50.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "b1 = blocks_to_matrix_claude(blocks, dataset, detach = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bdc36abc-5a6c-4f0e-8953-243300d7491f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12 s ± 17.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "b2 = blocks_to_matrix(blocks, dataset, detach = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "95190b59-87b9-4f4d-87e8-cf54ec9c1ce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9309e+00,  3.6302e-04,  3.6302e-04,  3.6302e-04,  1.1282e+00,\n",
       "         -1.9949e-01, -1.9949e-01, -1.9949e-01,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 3.6302e-04,  1.2829e+00,  6.2263e-01,  6.6656e-01,  2.0100e-01,\n",
       "         -3.3485e-01, -1.4470e-01, -2.0998e-01,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 3.6302e-04,  6.2263e-01, -3.7672e-02,  6.2520e-03,  2.0100e-01,\n",
       "         -1.3975e-01,  5.0396e-02, -1.4880e-02,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 3.6302e-04,  6.6656e-01,  6.2520e-03,  5.0176e-02,  2.0100e-01,\n",
       "         -1.9634e-01, -6.1887e-03, -7.1465e-02,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 1.1282e+00,  2.0100e-01,  2.0100e-01,  2.0100e-01,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [-1.9949e-01, -3.3485e-01, -1.3975e-01, -1.9634e-01,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [-1.9949e-01, -1.4470e-01,  5.0396e-02, -6.1887e-03,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [-1.9949e-01, -2.0998e-01, -1.4880e-02, -7.1465e-02,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1[0][0,0,0]-b2[0][0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1b919c64-8dc5-4932-834d-764115726b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = blocks_to_matrix_claude(blocks, dataset, detach = True)\n",
    "b2 = blocks_to_matrix(blocks, dataset, detach = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1f50b149-5ee4-4b8f-8ff8-b17950ff7a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.79 s ± 25.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "b1 = blocks_to_matrix_claude(blocks, dataset, detach = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e511764e-cbdc-4afe-b5ca-181f6fb223f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12 s ± 7.18 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "b2 = blocks_to_matrix(blocks, dataset, detach = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a8f782a0-d233-4bc2-83ee-65d4d3d59ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 2.31562 s\n",
       "File: /home/pegolo/Software/mlelec/src/mlelec/utils/pbc_utils.py\n",
       "Function: blocks_to_matrix_claude at line 1880\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "  1880                                           def blocks_to_matrix_claude(blocks, dataset, device=None, cg=None, all_pairs=False, sort_orbs=True, detach=False, sample_id=None):\n",
       "  1881         1        501.0    501.0      0.0      if device is None:\n",
       "  1882         1        792.0    792.0      0.0          device = dataset.device\n",
       "  1883                                                   \n",
       "  1884         1      95811.0  95811.0      0.0      if \"L\" in blocks.keys.names:\n",
       "  1885         1       2695.0   2695.0      0.0          from mlelec.utils.twocenter_utils import _to_uncoupled_basis\n",
       "  1886         1   36550664.0    4e+07      1.6          blocks = _to_uncoupled_basis(blocks, cg=cg, device=device)\n",
       "  1887                                           \n",
       "  1888         1      11332.0  11332.0      0.0      orbs_tot, orbs_offset = _orbs_offsets(dataset.basis)\n",
       "  1889         1   33042543.0    3e+07      1.4      atom_blocks_idx = _atom_blocks_idx(dataset.structures, orbs_tot)\n",
       "  1890         2     258148.0 129074.0      0.0      orbs_mult = {\n",
       "  1891                                                   species: {tuple(k): v for k, v in zip(*np.unique(np.asarray(dataset.basis[species])[:, :2], axis=0, return_counts=True))}\n",
       "  1892         1       1042.0   1042.0      0.0          for species in dataset.basis\n",
       "  1893                                               }\n",
       "  1894                                           \n",
       "  1895         1        220.0    220.0      0.0      reconstructed_matrices = []\n",
       "  1896                                               \n",
       "  1897         1       6182.0   6182.0      0.0      bt1factor = 1/np.sqrt(2) #1/torch.sqrt(torch.tensor(2.0, device=device))\n",
       "  1898         1        210.0    210.0      0.0      if all_pairs:\n",
       "  1899                                                   bt1factor /= 2\n",
       "  1900                                           \n",
       "  1901         1        511.0    511.0      0.0      bt2_factor_p = 0.5 if all_pairs else 1\n",
       "  1902                                           \n",
       "  1903        11       2342.0    212.9      0.0      for A in range(len(dataset.structures)):\n",
       "  1904        10     158928.0  15892.8      0.0          norbs = sum(orbs_tot[ai] for ai in dataset.structures[A].numbers)\n",
       "  1905        10       6854.0    685.4      0.0          reconstructed_matrices.append({})\n",
       "  1906                                           \n",
       "  1907         9     189407.0  21045.2      0.0      for key, block in blocks.items():\n",
       "  1908         8     437517.0  54689.6      0.0          block_type = key[\"block_type\"]\n",
       "  1909         8      91302.0  11412.8      0.0          ai, ni, li = key[\"species_i\"], key[\"n_i\"], key[\"l_i\"]\n",
       "  1910         8      77557.0   9694.6      0.0          aj, nj, lj = key[\"species_j\"], key[\"n_j\"], key[\"l_j\"]\n",
       "  1911                                                   \n",
       "  1912         8       3629.0    453.6      0.0          if sort_orbs:\n",
       "  1913         8       7735.0    966.9      0.0              fac = 1 if not (ai == aj and ni == nj and li == lj) else 2\n",
       "  1914                                                   else:\n",
       "  1915                                                       fac = 2\n",
       "  1916                                           \n",
       "  1917         8       7586.0    948.2      0.0          orbs_i = orbs_mult[ai]\n",
       "  1918         8       2467.0    308.4      0.0          orbs_j = orbs_mult[aj]\n",
       "  1919                                                   \n",
       "  1920        16      73206.0   4575.4      0.0          shapes = {(k1 + k2): (orbs_i[tuple(k1)], orbs_j[tuple(k2)])\n",
       "  1921         8        984.0    123.0      0.0                    for k1 in orbs_i for k2 in orbs_j}\n",
       "  1922         8       5862.0    732.8      0.0          phioffset = orbs_offset[(ai, ni, li)]\n",
       "  1923         8       2247.0    280.9      0.0          psioffset = orbs_offset[(aj, nj, lj)]\n",
       "  1924                                           \n",
       "  1925         8       5051.0    631.4      0.0          bt0_factor_p = 0.5 if not sort_orbs else (1 if not(ni == nj and li == lj) else 0.5)\n",
       "  1926                                               \n",
       "  1927         8  202190522.0    3e+07      8.7          samples = block.samples.values.tolist()\n",
       "  1928         8     180669.0  22583.6      0.0          blockvalues = block.values.detach() if detach else block.values\n",
       "  1929                                           \n",
       "  1930         8       2536.0    317.0      0.0          old_A = -1\n",
       "  1931     83769  117237718.0   1399.5      5.1          for sample, blockval in zip(samples, blockvalues[:,:,0]):\n",
       "  1932     83761   41018729.0    489.7      1.8              if blockval.numel() == 0:\n",
       "  1933                                                           continue        \n",
       "  1934                                           \n",
       "  1935     83761   20433411.0    243.9      0.9              A, i, j, Tx, Ty, Tz = sample\n",
       "  1936     83761   14041470.0    167.6      0.6              T = (Tx, Ty, Tz)\n",
       "  1937     83761   16487610.0    196.8      0.7              mT = (-Tx, -Ty, -Tz)\n",
       "  1938                                           \n",
       "  1939     83761   14131644.0    168.7      0.6              other_fac = 0.5 if (i == j and T != (0,0,0) and not all_pairs) else 1\n",
       "  1940                                           \n",
       "  1941     83761   14586764.0    174.1      0.6              bt0_factor_m = bt0_factor_p * other_fac\n",
       "  1942     83761    9975871.0    119.1      0.4              bt2_factor_m = bt2_factor_p * other_fac\n",
       "  1943     83761   28631916.0    341.8      1.2              bt1_fact_fin = bt1factor / fac * other_fac\n",
       "  1944                                                       \n",
       "  1945     83761   23234097.0    277.4      1.0              if T not in reconstructed_matrices[A]:\n",
       "  1946       110      28467.0    258.8      0.0                  assert mT not in reconstructed_matrices[A], \"why is mT present but not T?\"\n",
       "  1947       110    2407463.0  21886.0      0.1                  norbs = sum(orbs_tot[ai] for ai in dataset.structures[A].numbers)\n",
       "  1948       110    8265358.0  75139.6      0.4                  reconstructed_matrices[A][T] = torch.zeros(norbs, norbs, device=device)\n",
       "  1949       110    4600464.0  41822.4      0.2                  reconstructed_matrices[A][mT] = torch.zeros(norbs, norbs, device=device)\n",
       "  1950                                           \n",
       "  1951     83761   16950611.0    202.4      0.7              matrix_T = reconstructed_matrices[A][T]\n",
       "  1952     83761   16127758.0    192.5      0.7              matrix_mT = reconstructed_matrices[A][mT]\n",
       "  1953                                           \n",
       "  1954     83761   12333424.0    147.2      0.5              if A != old_A:\n",
       "  1955        80      75667.0    945.8      0.0                  i_start, j_start = atom_blocks_idx[(A, i, j)]\n",
       "  1956        80      74833.0    935.4      0.0                  phi_end = shapes[(ni, li, nj, lj)][0]\n",
       "  1957        80      31717.0    396.5      0.0                  psi_end = shapes[(ni, li, nj, lj)][1]  \n",
       "  1958        80      11901.0    148.8      0.0                  old_A = A\n",
       "  1959                                           \n",
       "  1960    167522 1580192524.0   9432.7     68.2              _process_block(block_type, blockval, matrix_T, matrix_mT, \n",
       "  1961     83761   11759194.0    140.4      0.5                             i_start, j_start, phioffset, psioffset, phi_end, psi_end,\n",
       "  1962     83761    9768653.0    116.6      0.4                             bt0_factor_p, bt0_factor_m, bt2_factor_p, bt2_factor_m, bt1_fact_fin)\n",
       "  1963                                                    \n",
       "  1964        11      11151.0   1013.7      0.0      for A, matrix in enumerate(reconstructed_matrices):\n",
       "  1965        10      31150.0   3115.0      0.0          Ts = list(matrix.keys())\n",
       "  1966       220      86587.0    393.6      0.0          for T in Ts:\n",
       "  1967       210     446395.0   2125.7      0.0              mT = tuple(-t for t in T)\n",
       "  1968       210   79221220.0 377243.9      3.4              assert torch.all(torch.isclose(matrix[T] - reconstructed_matrices[A][mT].T, torch.zeros_like(matrix[T]))), \\\n",
       "  1969                                                              torch.norm(matrix[T] - reconstructed_matrices[A][mT].T).item()\n",
       "  1970                                           \n",
       "  1971         1        160.0    160.0      0.0      return reconstructed_matrices"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f blocks_to_matrix_claude blocks_to_matrix_claude(blocks, dataset, detach = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "311fac1c-93d4-488b-bb44-f69a832767cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for b1_, b2_ in zip(b1, b2):\n",
    "    for T in b1_:\n",
    "        assert (b1_[T]-b2_[T]).norm() < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "2add5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689a7b49-6077-4a9d-a683-938839dfd357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283cb5d-305c-439a-abc2-ec95772802dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "729c7a30-48ee-4e2a-9337-94b8ef9fd442",
   "metadata": {},
   "source": [
    "### Train eigvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "634298e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.metrics import Eigval_loss, L2_loss_meanzero, L2_loss\n",
    "loss_fn = Eigval_loss #L2_loss #_meanzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "23a3d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigval(A, M):\n",
    "    Ax = xitorch.LinearOperator.m(A)\n",
    "    Mx = xitorch.LinearOperator.m(M) if M is not None else None\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigvals, eigvecs = symeig(Ax, M = Mx)\n",
    "    return eigvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "bc34e92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       0, train loss 53597.0918419704\n",
      "Epoch       1, train loss 44175.1499621223\n",
      "Epoch       2, train loss 36248.1691240230\n",
      "Epoch       3, train loss 28370.4101911453\n",
      "Epoch       4, train loss 21438.2034891949\n",
      "Epoch       5, train loss 15807.1415780888\n",
      "Epoch       6, train loss 11985.5146603819\n",
      "Epoch       7, train loss 9774.8678498762\n",
      "Epoch       8, train loss 8501.8504699509\n",
      "Epoch       9, train loss 7450.2254457610\n",
      "Epoch      10, train loss 6335.8788934897\n",
      "Epoch      11, train loss 5493.2822659648\n",
      "Epoch      12, train loss 4935.4310709653\n",
      "Epoch      13, train loss 4454.4759109243\n",
      "Epoch      14, train loss 4122.9358574417\n",
      "Epoch      15, train loss 3884.2145714118\n",
      "Epoch      16, train loss 3698.7704982252\n",
      "Epoch      17, train loss 3556.6680679591\n",
      "Epoch      18, train loss 3441.0080173714\n",
      "Epoch      19, train loss 3336.5085425173\n",
      "Epoch      20, train loss 3242.5222185971\n",
      "Epoch      21, train loss 3149.1740489903\n",
      "Epoch      22, train loss 3068.0809865005\n",
      "Epoch      23, train loss 2994.9293297779\n",
      "Epoch      24, train loss 2925.5166445187\n",
      "Epoch      25, train loss 2859.8777408026\n",
      "Epoch      26, train loss 2797.7472732095\n",
      "Epoch      27, train loss 2737.2040836388\n",
      "Epoch      28, train loss 2677.3626748614\n",
      "Epoch      29, train loss 2619.4714999833\n",
      "Epoch      30, train loss 2566.3979070617\n",
      "Epoch      31, train loss 2520.8453874358\n",
      "Epoch      32, train loss 2482.6480080803\n",
      "Epoch      33, train loss 2450.9720792819\n",
      "Epoch      34, train loss 2424.5108561842\n",
      "Epoch      35, train loss 2402.2666055226\n",
      "Epoch      36, train loss 2383.9361756989\n",
      "Epoch      37, train loss 2368.2894368179\n",
      "Epoch      38, train loss 2354.4577255987\n",
      "Epoch      39, train loss 2342.1560128556\n",
      "Epoch      40, train loss 2331.1092910260\n",
      "Epoch      41, train loss 2321.0289529257\n",
      "Epoch      42, train loss 2311.6837056467\n",
      "Epoch      43, train loss 2302.9798816901\n",
      "Epoch      44, train loss 2294.8773877027\n",
      "Epoch      45, train loss 2287.2978254482\n",
      "Epoch      46, train loss 2280.1557422785\n",
      "Epoch      47, train loss 2273.4256773405\n",
      "Epoch      48, train loss 2267.0527168527\n",
      "Epoch      49, train loss 2260.9500762935\n",
      "Epoch      50, train loss 2255.0477178259\n",
      "Epoch      51, train loss 2249.2587713335\n",
      "Epoch      52, train loss 2243.5350953414\n",
      "Epoch      53, train loss 2237.8467450566\n",
      "Epoch      54, train loss 2232.1918605876\n",
      "Epoch      55, train loss 2226.5963616938\n",
      "Epoch      56, train loss 2221.0889211897\n",
      "Epoch      57, train loss 2215.6848937980\n",
      "Epoch      58, train loss 2210.3841279996\n",
      "Epoch      59, train loss 2205.1798652626\n",
      "Epoch      60, train loss 2200.0595939293\n",
      "Epoch      61, train loss 2195.0151740928\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[481], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# pred_eigvals = [torch.stack([compute_eigval(h, s) for h, s in zip(HK, batch.overlap_kspace[ifr])]) for ifr in batch.sample_id]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# pred_eigvals = torch.stack([torch.linalg.eigvalsh(lowdin_orthogonalize(h, s)) for h, s in zip(HK, batch.overlap_kpoints)])\u001b[39;00m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred_eigvals, batch\u001b[38;5;241m.\u001b[39meigenvalues)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# for i, (l, o) in enumerate(zip(all_losses, optimizers)):\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     l.backward(retain_graph = False)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     o.step()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nepoch = 5000\n",
    "for epoch in range(nepoch):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    # Train against real space targets\n",
    "    for ib, batch in enumerate(dl):\n",
    "        \n",
    "        model.train(True)\n",
    "        optimizer.zero_grad()\n",
    "        # for ik, key in enumerate(model.model):\n",
    "        #     optimizers[ik].zero_grad()\n",
    "        \n",
    "        pred = model.forward(batch.features, mldata.model_metadata)\n",
    "        HT = blocks_to_matrix(pred, dataset, detach = False)\n",
    "        HK = [dataset.bloch_sum(HT)[ifr] for ifr in batch.sample_id] #+ batch.baseline\n",
    "        if len(HK) == 1:\n",
    "            overlap = [batch.overlap_kspace]\n",
    "        else:\n",
    "            overlap = batch.overlap_kspace\n",
    "            \n",
    "        pred_eigvals = [torch.stack([compute_eigval(h, s) for h, s in zip(hk, sk)]) for hk, sk in zip(HK, overlap)]\n",
    "        # pred_eigvals = [torch.stack([compute_eigval(h, s) for h, s in zip(HK, batch.overlap_kspace[ifr])]) for ifr in batch.sample_id]\n",
    "        # pred_eigvals = torch.stack([torch.linalg.eigvalsh(lowdin_orthogonalize(h, s)) for h, s in zip(HK, batch.overlap_kpoints)])\n",
    "\n",
    "        loss = loss_fn(pred_eigvals, batch.eigenvalues)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # for i, (l, o) in enumerate(zip(all_losses, optimizers)):\n",
    "        #     l.backward(retain_graph = False)\n",
    "        #     o.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    scheduler.step(epoch_loss)\n",
    "    # for s in schedulers:\n",
    "    #     s.step(l.item())\n",
    "        # lr_list.append(s.get_last_lr())\n",
    "\n",
    "    \n",
    "        # if scheduler.get_last_lr(), lr):\n",
    "        #     lr = scheduler.get_last_lr()\n",
    "        #     print(f'lr changed to {lr}')\n",
    "\n",
    "    # LOSS_LIST.append(LOSS)\n",
    "    # LOSS_LISTc.append(LOSSc)\n",
    "    if epoch >= 0: #% 10 == 0:\n",
    "        print(f\"Epoch {epoch:>7d}, train loss {epoch_loss:>15.10f}\") #, avg lr = {np.mean(lr_list)}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n",
    "        # print(f\"Epoch {epoch:>7d}, train loss {LOSS:>15.10f}, check loss {LOSSc:>15.10f}, delta {delta:>15.10f}, avg lr = {np.mean(lr_list)}\") #, train loss k {loss_k[-1]:>15.10f}, train loss per prediction {np.sqrt(epoch_loss)/n_predictions:>6.5e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
