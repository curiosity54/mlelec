{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d9b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e5ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ase \n",
    "from ase.units import Bohr \n",
    "import torch\n",
    "import metatensor\n",
    "from metatensor import TensorMap, TensorBlock, Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nigam/miniconda3/lib/python3.11/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read \n",
    "from pyscf.pbc.tools import pyscf_ase \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check supercell $\\Gamma$ and unit cell k-point calc/ generate translated matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.data.pyscf_calculator import get_scell_phase, check_translation_hermiticity, map_gammapoint_to_kpoint, map_gammapoint_to_relativetrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010222087626684012\n",
      "9.813485513377627e-05\n",
      "0.00010211799044343512\n"
     ]
    }
   ],
   "source": [
    "# load unit cell frame, create supercell and phase \n",
    "# filename = \"C2_174_imx\"\n",
    "# framename = \"C2_174.extxyz\"\n",
    "\n",
    "filename = \"C2_rotated\"\n",
    "framename = \"C2_rotated.xyz\"\n",
    "# filename = \"examples/data/periodic/c2/C2_rotated\"\n",
    "# framename = \"examples/data/periodic/c2/C2_rotated.xyz\"\n",
    "frames = read(framename, ':')\n",
    "kmesh = [4, 4, 1]\n",
    "fock_Ls = []\n",
    "error = []\n",
    "for ifr, frame in enumerate(frames[:]):\n",
    "    cell, scell, phase = get_scell_phase(frame, kmesh)\n",
    "    NR, Nk = phase.shape\n",
    "    nao = cell.nao\n",
    "\n",
    "    fock = np.load('results_{}/supercell/supercell_fock_{}.npy'.format(filename, ifr)).reshape(NR, nao, NR, nao)\n",
    "    kkfock = np.load('results_{}/unitcell/unitcell_fock_{}.npy'.format(filename, ifr))\n",
    "\n",
    "    overlap = np.load('results_{}/supercell/supercell_over_{}.npy'.format(filename, ifr)).reshape(NR, nao, NR, nao)\n",
    "    kkoverlap = np.load('results_{}/unitcell/unitcell_over_{}.npy'.format(filename, ifr))\n",
    "\n",
    "    check_translation_hermiticity(fock, NR=NR)\n",
    "    # R_rel = translation_vectors_for_kmesh(cell, kmesh, wrap_around=False, return_rel=True)\n",
    "    fock_trans, weights, phasediff = map_gammapoint_to_relativetrans(fock, phase=phase, cell=cell, kmesh=kmesh)\n",
    "    yy = map_gammapoint_to_kpoint(fock, phase=phase, cell=cell, kmesh=kmesh)\n",
    "    print(np.linalg.norm(yy - kkfock))\n",
    "    error.append(np.linalg.norm(yy - kkfock))\n",
    "    fock_Ls.append(fock_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.001)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "large_err = np.where(np.array(error)>1e-5)[0]\n",
    "print(large_err)\n",
    "plt.plot(error, 'k.')\n",
    "plt.plot(large_err, np.array(error)[large_err], 'r.')\n",
    "plt.ylim(0, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle\n",
    "# hickle.dump(cell.make_kpts(kmesh), 'results_{}/kpts.hkl'.format(filename))\n",
    "# hickle.dump(phasediff, 'results_{}/expkL.hkl'.format(filename))\n",
    "# hickle.dump(weights, 'results_{}/weights.hkl'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focks_translated = {key: np.asarray([dictionary[key] for dictionary in fock_Ls]) for key in fock_Ls[0].keys()}\n",
    "\n",
    "# hickle.dump(focks_translated, 'results_{}/fock_Ls.hkl'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dict = {k: [] for k in fock_Ls[0].keys()} \n",
    "# for k in fock_Ls[0].keys():\n",
    "#     for i in range(len(fock_Ls)):\n",
    "#         merged_dict[k].append(fock_Ls[i][k])\n",
    "# for x in merged_dict.keys():\n",
    "#     no = np.linalg.norm(focks_translated[x] - np.array(merged_dict[x]))\n",
    "#     if no > 1e-5:\n",
    "#         print(x, no)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For one structure explicit calc \n",
    "\n",
    "# frame = frames[0]\n",
    "# cell, scell, phase = get_scell_phase(frame, kmesh)\n",
    "# NR, Nk = phase.shape\n",
    "# nao = cell.nao\n",
    "# fock = np.load('results_C2_174_imx/supercell/supercell_fock_{}.npy'.format(7)).reshape(NR, nao, NR, nao)\n",
    "# kkfock = np.load('results_C2_174_imx/unitcell/unitcell_fock_{}.npy'.format(7))\n",
    "\n",
    "# H = {(i,j): fock[i,:,j,:] for i in range(NR) for j in range(NR)}\n",
    "# R_rel = translation_vectors_for_kmesh(cell, kmesh, wrap_around=False, return_rel=True)\n",
    "# # R_vec_abs = translation_vectors_for_kmesh(cell, kmesh, wrap_around=False)\n",
    "# maps_Ls = defaultdict(list) \n",
    "\n",
    "# for i, (M,N) in enumerate(H.keys()):\n",
    "#     maps_Ls[str(tuple((R_rel[M] - R_rel [N])))].append((M-N, M,N,i))\n",
    "\n",
    "# maps_Ls = {k: v for k, v in maps_Ls.items() if v}\n",
    "\n",
    "# H_maps_Ls = defaultdict(list)\n",
    "# H_Ls = {}\n",
    "# weight_Ls = {}\n",
    "# phase_diff_Ls = {}  \n",
    "# for i, k in enumerate(maps_Ls.keys()):\n",
    "#     for x in maps_Ls[k]:\n",
    "#         M, N = x[1], x[2]\n",
    "#         H_maps_Ls[k].append((H[(M,N)]))\n",
    "    \n",
    "#     try: \n",
    "#         xx = H_maps_Ls[k][0]\n",
    "#         for y in H_maps_Ls[k][1:]:\n",
    "#             # print(np.linalg.norm(xx-y), k)\n",
    "#             if not np.allclose(xx, y):\n",
    "#                 print(k, np.linalg.norm(xx-y))\n",
    "#         H_Ls[k] = xx\n",
    "#         weight_Ls[k] = len(H_maps_Ls[k])\n",
    "#         phase_diff_Ls[k] = np.array(phase[M]/ phase[N])\n",
    "#     except:\n",
    "#         print(k, \"skipped\")\n",
    "# yy = np.zeros((Nk, nao, nao), dtype = np.complex128)\n",
    "\n",
    "# for k in H_Ls.keys():\n",
    "#     for j in range(Nk):\n",
    "#         yy[j] += H_Ls[k] * weight_Ls[k] * phase_diff_Ls[k][j]\n",
    "\n",
    "# for k in range(Nk):\n",
    "#     print(np.linalg.norm(yy[k]/16 - kkfock[k].conj()))\n",
    "# print(np.linalg.norm(yy/16 - kkfock.conj()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = read('C2_rotated.xyz', ':')\n",
    "frames = read('examples/data/periodic/c2/C2_174.extxyz', ':10')\n",
    "for f in frames:\n",
    "    f.pbc = [True, True, True]\n",
    "\n",
    "kmesh = [4, 4, 1]\n",
    "# filename = \"C2_rotated\"\n",
    "filename = \"C2_174\"\n",
    "translated_matrices = hickle.load('examples/data/periodic/c2/results_{}/fock_Ls.hkl'.format(filename))\n",
    "weights = hickle.load('examples/data/periodic/c2/results_{}/weights.hkl'.format(filename))\n",
    "kpts = hickle.load('examples/data/periodic/c2/results_{}/kpts.hkl'.format(filename))  \n",
    "expkL = hickle.load('examples/data/periodic/c2/results_{}/expkL.hkl'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in translated_matrices.keys():\n",
    "    mT = tuple(-np.asarray(T))\n",
    "    if not mT in translated_matrices.keys():\n",
    "        raise ValueError(\"key not found\")\n",
    "\n",
    "    norm = np.linalg.norm(translated_matrices[T] - np.transpose(translated_matrices[mT], axes=(0,2,1)))\n",
    "    if norm> 1e-10:\n",
    "        print(T, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ls = np.asarray([list(k) for k in translated_matrices.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix orbital order \n",
    "from mlelec.utils.twocenter_utils import fix_orbital_order\n",
    "orbs = {6: [[1,0,0],[2,0,0],[2,1,1], [2,1,-1],[2,1,0]]}\n",
    "for T in translated_matrices.keys():\n",
    "    translated_matrices[T] = fix_orbital_order(translated_matrices[T], frames, orbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 49\n"
     ]
    }
   ],
   "source": [
    "positive_Ls = []\n",
    "for L in Ls:\n",
    "    lL  = list(L)\n",
    "    lmL = list(-L)\n",
    "    if not (lL in positive_Ls):\n",
    "        if not (lmL in positive_Ls):\n",
    "            positive_Ls.append(lmL)\n",
    "print(len(positive_Ls), len(Ls)   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of positive translations\n",
    "desired_shifts = [\n",
    "    [0, 0, 0],\n",
    " [1, 0, 0],\n",
    " [0,1,0], \n",
    " [1, 1, 0],\n",
    " [1,-1,0],\n",
    " [2,0,0], \n",
    "[0,2,0], \n",
    "[2,1,0],\n",
    "[3,0,0],\n",
    "[0,3,0],\n",
    " ]\n",
    "\n",
    "# desired_shifts = Ls \n",
    "\n",
    "withnegative_shifts = desired_shifts.copy()\n",
    "for s in withnegative_shifts[:]:\n",
    "    withnegative_shifts.append([-s[0], -s[1], -s[2]])\n",
    "\n",
    "selected_matrices = {}\n",
    "for s in withnegative_shifts:\n",
    "    selected_matrices[tuple(s)] = translated_matrices[tuple(s)]\n",
    "\n",
    "# check hermiticity across translations\n",
    "for s in desired_shifts[:]:\n",
    "    if np.linalg.norm(selected_matrices[tuple(s)] - np.transpose(selected_matrices[tuple([-s[0], -s[1], -s[2]])], axes=(0,2,1)))/np.linalg.norm(selected_matrices[tuple(s)])> 1e-10:\n",
    "        print(s)\n",
    "        print(np.linalg.norm(selected_matrices[tuple(s)] - np.transpose(selected_matrices[tuple([-s[0], -s[1], -s[2]])], axes=(0,2,1)) ))\n",
    "        print(np.linalg.norm(selected_matrices[tuple(s)] - np.transpose(selected_matrices[tuple([-s[0], -s[1], -s[2]])], axes=(0,2,1))) /np.linalg.norm(selected_matrices[str(s)]))\n",
    "    # assert np.allclose(selected_matrices[str(s)], selected_matrices[str([-s[0], -s[1], -s[2]])].T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.753175325980078e-16"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroshift = translated_matrices[(0,0,0)]\n",
    "np.linalg.norm(zeroshift - np.transpose(zeroshift, axes=(0,2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_minus_from_plus(block, shift=[0,0,1], pstarting=0 ):\n",
    "#     if isinstance(shift, list):\n",
    "#         shift = np.asarray(shift)\n",
    "#     if sum(shift)%2 == 0:\n",
    "#         return block\n",
    "#     elif sum(shift)%2 == 0 and 0 in shift:\n",
    "#         return block\n",
    "    \n",
    "#     elif not np.where(shift==0):\n",
    "#         return block\n",
    "    \n",
    "#     else:\n",
    "#         zeroindices = np.where(shift!=0)[0]\n",
    "#         print(zeroindices)\n",
    "#         minusblock = np.copy(block)\n",
    "#         minusblock[:,pstarting+zeroindices] *= -1 # block[zeroindices[::-1]]\n",
    "#         minusblock[pstarting+ zeroindices,:] *= -1\n",
    "#         return minusblock\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proceed now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.utils.twocenter_utils import _to_blocks, _to_matrix, _to_coupled_basis, _to_uncoupled_basis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matrices_sum = {}\n",
    "matrices_diff = {} \n",
    "\n",
    "for s in desired_shifts[:]:\n",
    "    matrices_sum[tuple(s)] = 0.5*(selected_matrices[tuple(s)] + selected_matrices[tuple([-s[0], -s[1], -s[2]])])\n",
    "    matrices_diff[tuple(s)] = 0.5* (selected_matrices[tuple(s)] - selected_matrices[tuple([-s[0], -s[1], -s[2]])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0] 0.0\n",
      "[1, 0, 0] 5.099858342432072e-15\n",
      "[0, 1, 0] 5.770927957773908e-15\n",
      "[1, 1, 0] 5.498412567973009e-15\n",
      "[1, -1, 0] 2.9489205524771403e-16\n",
      "[2, 0, 0] 9.65147161657606e-17\n",
      "[0, 2, 0] 3.2663736944268525e-16\n",
      "[2, 1, 0] 2.9478374749280746e-16\n",
      "[3, 0, 0] 1.416424464350212e-15\n",
      "[0, 3, 0] 5.049725970333229e-15\n"
     ]
    }
   ],
   "source": [
    "for s in desired_shifts[:]:\n",
    "    # print(np.linalg.norm(matrices_sum[tuple(s)] - matrices_sum[tuple(s)].transpose(0,2,1))) # SUM is hermitian  \n",
    "    print(s, np.linalg.norm(matrices_diff[tuple(s)] + matrices_diff[tuple(s)].transpose(0,2,1)))  # DIFF is anti-hermitian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm of Sum matrices\n",
      "54.20150788504924\n",
      "1.4703719767052015\n",
      "4.612980281608163\n",
      "4.675329803806235\n",
      "0.11776329648441179\n",
      "0.032072379274884905\n",
      "0.21727649231106863\n",
      "0.1207116869405447\n",
      "1.470371976779733\n",
      "4.6129802820905494\n",
      "norm of Diff matrices\n",
      "0.0\n",
      "1.5896869020477293\n",
      "4.665629333408246\n",
      "4.725158746807788\n",
      "0.11019695388278204\n",
      "1.1926756280806283e-09\n",
      "6.877182071136917e-10\n",
      "0.11331812258631987\n",
      "1.5896869020270719\n",
      "4.665629333712581\n"
     ]
    }
   ],
   "source": [
    "print('norm of Sum matrices')\n",
    "for m in matrices_sum.values():\n",
    "    print(np.linalg.norm(m))\n",
    "print('norm of Diff matrices')\n",
    "for m in matrices_diff.values():\n",
    "    print(np.linalg.norm(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "57b135fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_blocks_sum = {}\n",
    "target_blocks_minus = {}\n",
    "target_coupled_blocks_sum = {}\n",
    "target_coupled_blocks_diff= {}\n",
    "for s in desired_shifts[:]:\n",
    "    target_blocks_sum[tuple(s)] = _to_blocks(matrices_sum[tuple(s)], frames=frames, orbitals=orbs) # matrix -> uncoupled\n",
    "    target_blocks_minus[tuple(s)] = _to_blocks(matrices_diff[tuple(s)], frames=frames, orbitals=orbs)\n",
    "    target_coupled_blocks_sum[tuple(s)] = _to_coupled_basis(_to_blocks(matrices_sum[tuple(s)], frames=frames, orbitals=orbs), orbs) # uncouple -> coupled\n",
    "    target_coupled_blocks_diff[tuple(s)] = _to_coupled_basis(_to_blocks(matrices_diff[tuple(s)], frames=frames, orbitals=orbs), orbs, skip_symmetry=True) # uncouple -> coupled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsum = {}\n",
    "rdiff = {}\n",
    "rblocks_sum = {}\n",
    "rblocks_minus = {}\n",
    "\n",
    "for s in desired_shifts[:]:\n",
    "    # rblocks_sum[tuple(s)] = _to_uncoupled_basis(target_coupled_blocks_sum[tuple(s)]) # couples-> uncoupled\n",
    "    # rblocks_minus[tuple(s)] = _to_uncoupled_basis(target_coupled_blocks_diff[tuple(s)])\n",
    "\n",
    "    # rsum[tuple(s)] = _to_matrix(target_blocks_sum[tuple(s)], frames = frames, orbitals=orbs) # uncoupled -> matrix\n",
    "    # rdiff[tuple(s)] = _to_matrix(target_blocks_minus[tuple(s)], frames = frames, orbitals=orbs, hermitian=False)   \n",
    "    rsum[tuple(s)] = _to_matrix(_to_uncoupled_basis(target_coupled_blocks_sum[tuple(s)]), frames = frames, orbitals=orbs)\n",
    "    rdiff[tuple(s)] = _to_matrix(_to_uncoupled_basis(target_coupled_blocks_diff[tuple(s)]), frames = frames, orbitals=orbs, hermitian=False)\n",
    "\n",
    "# matrix <-> uncoupled \n",
    "\n",
    "# this is now checking that matrix <-> uncoupled blocks works \n",
    "for s in desired_shifts[:]:\n",
    "    assert torch.allclose(rsum[tuple(s)].cpu(), torch.from_numpy(matrices_sum[tuple(s)]).type(torch.float)),print(\"SUM\", torch.linalg.norm(rsum[tuple(s)].cpu()- torch.from_numpy(matrices_sum[tuple(s)]).type(torch.float)))\n",
    "   \n",
    "    assert torch.allclose(rdiff[tuple(s)].cpu(), torch.from_numpy(matrices_diff[tuple(s)]).type(torch.float)) , print(\"DIFF\",s, torch.linalg.norm(rdiff[tuple(s)].cpu()-torch.from_numpy(matrices_diff[tuple(s)]).type(torch.float)))\n",
    "    # print(torch.linalg.norm(rsum[tuple(s)].cpu()- torch.from_numpy(matrices_sum[tuple(s)]).type(torch.float)))\n",
    "    # print(torch.linalg.norm(rdiff[tuple(s)].cpu()-torch.from_numpy(matrices_diff[tuple(s)]).type(torch.float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coupled -> uncoupled \n",
    "# for s in desired_shifts[:]:\n",
    "#     for key, block in rblocks_sum[tuple(s)].items():\n",
    "#         norm = np.linalg.norm(block.values.cpu().numpy() - target_blocks_sum[tuple(s)][key].values.cpu().numpy())\n",
    "#         if norm> 1e-5:\n",
    "#             print(\"SUM\", s, key.values, norm)\n",
    "\n",
    "# for s in desired_shifts[:]:\n",
    "#     for key, block in rblocks_minus[tuple(s)].items():\n",
    "#         li,lj= key[\"l_i\"], key[\"l_j\"]\n",
    "#         norm = np.linalg.norm(block.values.cpu().numpy() - target_blocks_minus[tuple(s)][key].values.cpu().numpy())\n",
    "#         if norm> 1e-5:\n",
    "#             print(\"DIFF\", s, key.values, norm)\n",
    "#             break\n",
    "#             print(block.values.cpu().numpy()[0])\n",
    "#             print(target_blocks_minus[tuple(s)][key].values.cpu().numpy()[0]\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0] tensor(1.2480e-06, dtype=torch.float64)\n",
      "[1, 0, 0] tensor(9.7420e-08, dtype=torch.float64)\n",
      "[0, 1, 0] tensor(3.3836e-07, dtype=torch.float64)\n",
      "[1, 1, 0] tensor(3.3133e-07, dtype=torch.float64)\n",
      "[1, -1, 0] tensor(1.1270e-08, dtype=torch.float64)\n",
      "[2, 0, 0] tensor(2.1471e-09, dtype=torch.float64)\n",
      "[0, 2, 0] tensor(1.3980e-08, dtype=torch.float64)\n",
      "[2, 1, 0] tensor(1.1029e-08, dtype=torch.float64)\n",
      "[3, 0, 0] tensor(9.6681e-08, dtype=torch.float64)\n",
      "[0, 3, 0] tensor(3.3826e-07, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "reconstructed_matrices = {} #sum/diff -> +T and -T \n",
    "for s in desired_shifts[:]:\n",
    "    reconstructed_matrices[tuple(s)] = rsum[tuple(s)] + rdiff[tuple(s)]\n",
    "    reconstructed_matrices[tuple(-np.asarray(s))] = rsum[tuple(s)] - rdiff[tuple(s)]\n",
    "# reconstructed_matrices = get_predicted_matrices(rsum, rdiff)\n",
    "for s in desired_shifts[:]:\n",
    "    print(s, torch.linalg.norm(reconstructed_matrices[tuple(s)].cpu() - selected_matrices[tuple(s)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nk = len(kpts)\n",
    "nao = 10\n",
    "small_shifts_target = []\n",
    "for ifr, frame in enumerate(frames): \n",
    "    kmatrix = torch.zeros((Nk, nao, nao)).type(torch.complex128)\n",
    "\n",
    "    # shift_indices = [np.where(np.all(Ls==np.asarray(s), axis=1))[0][0] for s in withnegative_shifts]\n",
    "    for key in withnegative_shifts:\n",
    "        key = tuple(key)\n",
    "        for kpt in range(Nk):\n",
    "            kmatrix[kpt] += translated_matrices[key][ifr] * weights[key] * expkL[key][kpt]\n",
    "    kmatrix = kmatrix / Nk\n",
    "    small_shifts_target.append(kmatrix) \n",
    "small_shifts_target = torch.stack(small_shifts_target).swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a32124",
   "metadata": {},
   "source": [
    "### Equivariance check on target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "3c496aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotations = hickle.load('../mlelec/examples/data/water_rotated/rotations.hickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8cc3d035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 24 blocks\n",
       "keys: block_type  species_i  n_i  l_i  species_j  n_j  l_j  L\n",
       "          0           6       1    0       6       1    0   0\n",
       "          0           6       1    0       6       2    0   0\n",
       "                               ...\n",
       "          -1          6       2    1       6       2    1   1\n",
       "          -1          6       2    1       6       2    1   2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c22b5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.utils.metatensor_utils import labels_where\n",
    "from mlelec.utils.symmetry import _wigner_d_real\n",
    "import numpy as np\n",
    "from metatensor import Labels\n",
    "# r1 = target_coupled_blocks_diff\n",
    "r1 = target_coupled_blocks_sum\n",
    "for j, (shift, stmap) in enumerate(r1.items()):\n",
    "    for i, (k,b) in enumerate(stmap.items()): \n",
    "        unrot_idx = labels_where(\n",
    "            b.samples,\n",
    "            selection=Labels([\"structure\"], values=np.asarray([[0]]).reshape(-1, 1)),\n",
    "            return_idx=True,\n",
    "        )[-1]\n",
    "        rot_idx = labels_where(\n",
    "            b.samples,\n",
    "            selection=Labels(\n",
    "                [\"structure\"], values=np.asarray([[1]]).reshape(-1, 1)\n",
    "            ),\n",
    "            return_idx=True,\n",
    "        )[-1]\n",
    "        L = k['L']\n",
    "        # L = k[\"spherical_harmonics_l\"]\n",
    "        wd = _wigner_d_real(L, *rotations[1 -1]).to(b.values)\n",
    "        if torch.linalg.norm(wd @ b.values[unrot_idx] - b.values[rot_idx]) > 1e-7:\n",
    "            print(torch.linalg.norm(wd @ b.values[unrot_idx] - b.values[rot_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO incorporate the plus minus as samples of the same translation and different translations as keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653f11c-56bc-45cf-b2d8-6f0a91d18861",
   "metadata": {},
   "source": [
    "## feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f0c692a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rascaline import SphericalExpansionByPair as PairExpansion\n",
    "from rascaline import SphericalExpansion\n",
    "from mlelec.utils.metatensor_utils import labels_where\n",
    "from metatensor import Labels\n",
    "from mlelec.features.acdc import twocenter_hermitian_features, single_center_features, pair_features, twocenter_hermitian_features_periodic\n",
    "from mlelec.utils.twocenter_utils import map_targetkeys_to_featkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e299cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc.py:114: UserWarning: Using cutoff 2.0 for all pairs feature\n",
      "  else:\n",
      "/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/features/acdc.py:114: UserWarning: Using cutoff 9.0 for all pairs feature\n",
      "  else:\n"
     ]
    }
   ],
   "source": [
    "hyper = {'cutoff': 4.,\n",
    "          'max_radial':8, \n",
    "          'max_angular':4,\n",
    "          'atomic_gaussian_width':0.3,\n",
    "          'center_atom_weight':1,\n",
    "          \"radial_basis\": {\"Gto\": {}},\n",
    "          \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}},\n",
    "}\n",
    "#test_rcut_shift: \n",
    "def test_rcut(frame, hypers, shifts):\n",
    "    hypers_ij = hypers.copy()\n",
    "    r = hypers['cutoff']\n",
    "    cell = frame.cell.copy()\n",
    "    norms = np.linalg.norm(frame.cell, axis=1)\n",
    "    assert isinstance(shifts[0], tuple)\n",
    "    max_shift = tuple([np.max(shifts, axis=(0,1))]*3)\n",
    "    max_disp = np.sqrt(np.dot(max_shift, norms**2))+ frame.get_all_distances().max()**2\n",
    "    if r < max_disp:\n",
    "        hypers_ij['cutoff'] = max_disp    \n",
    "    \n",
    "    return hypers_ij\n",
    "\n",
    "# hypers = test_rcut(frames[100], hyper, [(1,1,1)])\n",
    "hypers = hyper\n",
    "gij = PairExpansion(**hypers)\n",
    "pair = gij.compute(frames)\n",
    "\n",
    "# compute features that will be used for the zero shift\n",
    "single = single_center_features(frames, hypers, 2, lcut=3)#, pca_final = True, npca = 0.9999, slice_samples = 4)\n",
    "pair_zero_shift = pair_features(frames, hypers, order_nu=1,  all_pairs=True, lcut=3, max_shift=[1,1,1], feature_names = single.property_names)\n",
    "\n",
    "# compute features that will be used for the other shifts\n",
    "pair = pair_features(frames, hypers, order_nu=1,  all_pairs=True, lcut=3, max_shift=[3,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "cee9553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pair_feat = pair_zero_shift\n",
    "blocks = []\n",
    "for i, (key, block) in enumerate(pair_feat.items()):\n",
    "    samples_lab, idx = labels_where(block.samples, selection=Labels([\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\"], values=np.asarray([[0,0,0]]).reshape(-1, 3)), return_idx=True)\n",
    "    # print(samples_lab.names)\n",
    "    samples_lab =  Labels(samples_lab.names[:3], values=samples_lab.values[:,:3])\n",
    "    tb = TensorBlock(\n",
    "        samples=samples_lab,\n",
    "        values=block.values[idx],\n",
    "        properties=block.properties,\n",
    "        components = block.components\n",
    "    )\n",
    "    blocks.append(tb)\n",
    "pair_zero_shift = TensorMap(keys = pair_feat.keys, blocks=blocks)\n",
    "\n",
    "zeroshift = twocenter_hermitian_features_periodic(single, pair_zero_shift, shift=[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "68a313d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if desired_shifts is None: \n",
    "# Assume the first block has all the shifts\n",
    "#     shifts = list(zip(pair[0].samples[\"cell_shift_a\"], pair[0].samples[\"cell_shift_b\"], pair[0].samples[\"cell_shift_c\"]))\n",
    "#     unique_shifts= list(set(shifts))\n",
    "#     unique_shifts.sort()\n",
    "#     zeroidx = unique_shifts.index((0,0,0))\n",
    "#     nonneg_shifts = unique_shifts[zeroidx:]\n",
    "#     desired_shifts = [list(x) for x in nonneg_shifts if not len(np.where(np.abs(np.asarray(x))>1)[0])]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifts = list(zip(pair[0].samples[\"cell_shift_a\"], pair[0].samples[\"cell_shift_b\"], pair[0].samples[\"cell_shift_c\"]))\n",
    "shifts = list(set(zip(pair[0].samples[\"cell_shift_a\"], pair[0].samples[\"cell_shift_b\"], pair[0].samples[\"cell_shift_c\"])))\n",
    "\n",
    "pair_sum = {str(x):[] for x in desired_shifts}\n",
    "pair_diff = {str(x):[] for x in desired_shifts}\n",
    "blocks_plus = []\n",
    "blocks_minus = []\n",
    "\n",
    "#treat zero shift separately - we just treat the zero shift samples as blocks_plus in feat_plus later\n",
    "\n",
    "for i, (k,b) in enumerate(pair.items()):\n",
    "    for shift in desired_shifts[1:]:\n",
    "        minus_shift = tuple(-1*np.array(shift))\n",
    "        slab, plusidx = labels_where(b.samples, selection=Labels(names=[\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\"], values=np.array(shift).reshape(1,-1)), return_idx=True)\n",
    "        # print(len(plusidx))\n",
    "        slabm, minusidx = labels_where(b.samples, selection=Labels(names=[\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\"], values=np.array(minus_shift).reshape(1,-1)), return_idx=True)\n",
    "        # print(len(minusidx))\n",
    "\n",
    "        #match samples [\"structure\", \"center\", \"neighbor\"] in blocks_plus and blocks_minus\n",
    "        sidx = []\n",
    "        try:\n",
    "            for i, [structure, center, neighbor] in enumerate(slab.values[:,:3]):   \n",
    "                sidx.append(np.where(np.all(slabm.values[:,:3] == [structure, center, neighbor], axis=1))[0][0])\n",
    "            \n",
    "        except:\n",
    "            print(k, \"skipped\", slab.values.shape, slabm.values.shape)\n",
    "\n",
    "        # print(slab.values.shape, slabm.values.shape)\n",
    "        # print(len(sidx))\n",
    "        if len(sidx) != slab.values.shape[0]:\n",
    "            print(k, \"PROBLEM\", slabm.values, slabm.values)\n",
    "        assert np.allclose(slab.values[:,:3], slabm.values[sidx][:,:3]), (k,slab.values[np.where(slab.values - slabm.values[sidx])[0]][:,:3], slabm.values[sidx][np.where(slab.values - slabm.values)[0]][:,:3]) # so its the same samples under consideration\n",
    "        pvalues = b.values[plusidx] + b.values[minusidx]\n",
    "        mvalues = b.values[plusidx] - b.values[minusidx]\n",
    "        \n",
    "        blocks_plus.append(TensorBlock(values = pvalues,\n",
    "                                   components = b.components,\n",
    "                             samples = Labels(names = pair.sample_names[:-3], values=np.asarray(b.samples.values[plusidx])[:,:-3]),\n",
    "                                   properties = b.properties)\n",
    "                            )\n",
    "        \n",
    "        blocks_minus.append(TensorBlock(values = mvalues,\n",
    "                                   components = b.components,\n",
    "                             samples = Labels(names = pair.sample_names[:-3], values=np.asarray(b.samples.values[plusidx])[:,:-3]),\n",
    "                                   properties = b.properties)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 63\n",
      "Must equal the product of the two values below\n",
      "7 9\n"
     ]
    }
   ],
   "source": [
    "print(len(blocks_plus), len(blocks_minus))\n",
    "print(\"Must equal the product of the two values below\")\n",
    "print(len(pair.keys.values), len(desired_shifts[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "shift_trans = list(itertools.product(pair.keys.values.tolist(), desired_shifts[1:]))\n",
    "shift_trans = [(list(itertools.chain.from_iterable(_))) for _ in shift_trans]\n",
    "shift_trans_names = pair.keys.names  + pair.sample_names[-3:]\n",
    "shift_trans = Labels(shift_trans_names, np.array(shift_trans))\n",
    "pair_plus = TensorMap(shift_trans, blocks_plus)\n",
    "pair_minus = TensorMap(shift_trans, blocks_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pair_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_plus=twocenter_hermitian_features_periodic(single, pair_plus) \n",
    "feat_minus=twocenter_hermitian_features_periodic(single, pair_minus, antisymmetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335d43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "59e8de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 9)\n"
     ]
    }
   ],
   "source": [
    "blocks = []\n",
    "for i, (key, block) in enumerate(zeroshift.items()):\n",
    "    property_names = feat_plus.property_names # feat_plus.property_names\n",
    "    properties = Labels(property_names, values=block.properties.values)\n",
    "    tb = TensorBlock(\n",
    "        samples= block.samples, #Labels(sample_names, values=samp),\n",
    "        values=block.values,\n",
    "        properties=properties,\n",
    "        components = block.components\n",
    "    )\n",
    "    blocks.append(tb)\n",
    "\n",
    "key_names = zeroshift.keys.names[:-1] +[\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\", \"block_type\"]\n",
    "keyvals = np.pad(zeroshift.keys.values, ((0,0),(0,3)))\n",
    "print(keyvals.shape)\n",
    "keyvals[:, [-4, -1]] = keyvals[:, [-1, -4]]\n",
    "# keyvals = np.swapaxes(keyvals, -4,-1) # block typ to the last axis\n",
    "keys = Labels(key_names, keyvals) \n",
    "zeroshift = TensorMap(keys = keys, blocks=blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "d9caa50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 9) 210\n"
     ]
    }
   ],
   "source": [
    "block_plus_wzero = []\n",
    "for k, b in zeroshift.items():\n",
    "    block_plus_wzero.append(b.copy())\n",
    "for k, b in feat_plus.items():\n",
    "    block_plus_wzero.append(b.copy())\n",
    "keyvals = np.concatenate((zeroshift.keys.values, feat_plus.keys.values))\n",
    "print(keyvals.shape, len(block_plus_wzero))\n",
    "keys = Labels(zeroshift.keys.names, keyvals )\n",
    "feat_plus  = TensorMap(keys, block_plus_wzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 189 blocks\n",
       "keys: order_nu  inversion_sigma  spherical_harmonics_l  species_center  species_neighbor  cell_shift_a  cell_shift_b  cell_shift_c  block_type\n",
       "         2             1                   0                  6                6               1             0             0            0\n",
       "         2             1                   0                  6                6               0             1             0            0\n",
       "                                                                        ...\n",
       "         2            -1                   3                  6                6               0             3             0            1\n",
       "         2            -1                   3                  6                6               0             3             0            -1"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_where(feat_plus.keys, selection=Labels(names=[\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\"], values=np.array([1,1,1]).reshape(1,-1)), return_idx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6be876",
   "metadata": {},
   "source": [
    "### Equivariance test on features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "83a69062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3 6 6 0 1 0 0] tensor(1.0044e-07, device='cuda:0')\n",
      "[2 1 2 6 6 1 1 0 1] tensor(1.1173e-07, device='cuda:0')\n",
      "[ 2  1  3  6  6  0  1  0 -1] tensor(1.2971e-07, device='cuda:0')\n",
      "[ 2  1  3  6  6  1  1  0 -1] tensor(1.1374e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from mlelec.utils.metatensor_utils import labels_where\n",
    "from mlelec.utils.symmetry import _wigner_d_real\n",
    "import numpy as np\n",
    "from metatensor import Labels\n",
    "# r1 = feat_plus\n",
    "r1 = feat_minus \n",
    "for i, (k,b) in enumerate(r1.items()): \n",
    "    unrot_idx = labels_where(\n",
    "        b.samples,\n",
    "        selection=Labels([\"structure\"], values=np.asarray([[0]]).reshape(-1, 1)),\n",
    "        return_idx=True,\n",
    "    )[-1]\n",
    "    rot_idx = labels_where(\n",
    "        b.samples,\n",
    "        selection=Labels(\n",
    "            [\"structure\"], values=np.asarray([[1]]).reshape(-1, 1)\n",
    "        ),\n",
    "        return_idx=True,\n",
    "    )[-1]\n",
    "    # L = k['L']\n",
    "    L = k[\"spherical_harmonics_l\"]\n",
    "    wd = _wigner_d_real(L, *rotations[1 -1]).to(b.values)\n",
    "    if torch.linalg.norm(wd @ b.values[unrot_idx] - b.values[rot_idx]) > 1e-7:\n",
    "        print(k.values, torch.linalg.norm(wd @ b.values[unrot_idx] - b.values[rot_idx]))\n",
    "    # if torch.linalg.norm(wd @ b.values[unrot_idx] - b.values[rot_idx]) / torch.linalg.norm(b.values[rot_idx])> 1e-6:\n",
    "    #     print(k, torch.linalg.norm(wd @ b.values[unrot_idx] - b.values[rot_idx])/torch.linalg.norm(b.values[rot_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1f689368",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in desired_shifts[1:]:\n",
    "    for k in target_coupled_blocks_sum[tuple(s)].keys:\n",
    "        blockval = torch.linalg.norm(target_coupled_blocks_sum[tuple(s)][k].values)\n",
    "        feat = map_targetkeys_to_featkeys(feat_plus, k, cell_shift=s)\n",
    "        featval = torch.linalg.norm(feat.values)\n",
    "\n",
    "        # print(s, k.values, blockval, featval)   \n",
    "        if featval < 1e-5 and blockval > 1e-3:\n",
    "            print(s, k.values, blockval, featval)\n",
    "        # print(blockval, featval)   \n",
    "\n",
    "for s in desired_shifts[1:]:\n",
    "    for k in target_coupled_blocks_diff[tuple(s)].keys:\n",
    "        blockval = torch.linalg.norm(target_coupled_blocks_diff[tuple(s)][k].values)\n",
    "        if blockval>1e-5: \n",
    "            feat = map_targetkeys_to_featkeys(feat_minus, k, cell_shift=s)\n",
    "            featval = torch.linalg.norm(feat.values)\n",
    "            if featval < 1e-5 and blockval > 1e-3:\n",
    "                print(s, k.values, blockval, featval)\n",
    "            # print(s, k.values, blockval, featval)   \n",
    "#         # print(blockval, featval)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.models.linear import MLP\n",
    "from mlelec.data.dataset import MLDataset, MoleculeDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MoleculeDataset(mol_name = \"graphene\", frames=frames, target = ['kfock'], target_data={'kfock': small_shifts_target}, orbs = \"sto-3g\")\n",
    "# h_ml = MLDataset(dataset, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K T data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KpointData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/nigam/scratch/MAC/k-hamiltonian/learn_supercell.ipynb Cell 53\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_supercell.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m kdata \u001b[39m=\u001b[39m KpointData(frames, feat_plus, feat_minus,  small_shifts_target, selected_matrices , target_blocks_sum, target_blocks_minus )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_supercell.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# kdata._shuffle(720872)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_supercell.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m kdata\u001b[39m.\u001b[39m_split_indices(\u001b[39m0.7\u001b[39m, \u001b[39m0.2\u001b[39m, \u001b[39m0.1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KpointData' is not defined"
     ]
    }
   ],
   "source": [
    "kdata = KpointData(frames, feat_plus, feat_minus,  small_shifts_target, selected_matrices , target_blocks_sum, target_blocks_minus )\n",
    "# kdata._shuffle(720872)\n",
    "kdata._split_indices(0.7, 0.2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data \n",
    "from typing import Dict, List, Optional, Union\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from metatensor import TensorMap, Labels\n",
    "import metatensor.operations as operations\n",
    "import warnings\n",
    "\n",
    "class KpointData(data.Dataset):\n",
    "    def __init__(self, frames, feat_plus, feat_minus, target_kpoint, target_translation, target_blocks_sum, target_blocks_minus, target=\"target_translation\" , device = \"cuda\"):\n",
    "        self.frames = frames\n",
    "        self.nstructs = len(frames)\n",
    "        self.feat_plus = feat_plus \n",
    "        self.feat_minus = feat_minus\n",
    "        self.features = {'plus': self.feat_plus, 'minus': self.feat_minus}\n",
    "        self.target_kpoint = target_kpoint\n",
    "        self.target_translation = target_translation\n",
    "        if target == \"target_translation\":\n",
    "            self.target = target_translation\n",
    "        else: \n",
    "            self.target = target_kpoint\n",
    "        # warnings.warn('Target set to', target)\n",
    "        self.target_blocks_sum = target_blocks_sum\n",
    "        self.target_blocks_minus = target_blocks_minus\n",
    "        \n",
    "        self.target_blocks = {'plus':self.target_blocks_sum, 'minus':self.target_blocks_minus}\n",
    "        self.device = device\n",
    "        self.indices = self.indices = torch.arange(self.nstructs)\n",
    "\n",
    "    def _shuffle(self, random_seed: int = None):\n",
    "        if random_seed is None:\n",
    "            self.rng = torch.default_generator\n",
    "        else:\n",
    "            self.rng = torch.Generator().manual_seed(random_seed)\n",
    "\n",
    "        self.indices = torch.randperm(self.nstructs, generator=self.rng)\n",
    "\n",
    "\n",
    "    def _get_subset(self, y: TensorMap, indices: torch.tensor):\n",
    "        assert isinstance(y, TensorMap)\n",
    "        # for k, b in y.items():\n",
    "        #     b = b.values.to(device=self.device)\n",
    "        return operations.slice(\n",
    "            y,\n",
    "            axis=\"samples\",\n",
    "            labels=Labels(\n",
    "                names=[\"structure\"], values=np.asarray(indices).reshape(-1, 1)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _split_indices(\n",
    "        self, train_frac: float=None, val_frac: float=None, test_frac: Optional[float] = None\n",
    "    ):\n",
    "        #TODO: handle this smarter  \n",
    "\n",
    "        # overwrite self train/val/test indices\n",
    "        if train_frac is not None:\n",
    "            self.train_frac = train_frac\n",
    "        if val_frac is not None:\n",
    "            self.val_frac = val_frac\n",
    "        if test_frac is None:\n",
    "            test_frac = 1 - (self.train_frac + self.val_frac)\n",
    "            self.test_frac = test_frac\n",
    "            assert self.test_frac > 0\n",
    "        else: \n",
    "            try: \n",
    "                self.test_frac = test_frac\n",
    "                assert np.isclose(self.train_frac + self.val_frac + self.test_frac, 1, rtol=1e-6, atol=1e-5), (\n",
    "            self.train_frac + self.val_frac + self.test_frac, \"Split fractions do not add up to 1\"\n",
    "        )\n",
    "            except:\n",
    "                self.test_frac = 1 - (self.train_frac + self.val_frac)\n",
    "                assert self.test_frac > 0\n",
    "\n",
    "        self.train_idx = self.indices[: int(self.train_frac * self.nstructs)]#.sort()[0]\n",
    "        self.val_idx = self.indices[\n",
    "            int(self.train_frac * self.nstructs) : int(\n",
    "                (self.train_frac + self.val_frac) * self.nstructs\n",
    "            )\n",
    "        ]#.sort()[0]\n",
    "        self.test_idx = self.indices[int((self.train_frac + self.val_frac) * self.nstructs) :]#.sort()[0]\n",
    "        assert (\n",
    "            len(self.test_idx)\n",
    "            > 0  # and len(self.val_idx) > 0 and len(self.train_idx) > 0\n",
    "        ), \"Split indices not generated properly\"\n",
    "        self.target_train = self._get_subset(self.target_blocks, self.train_idx)\n",
    "        self.target_val = self._get_subset(self.target_blocks, self.val_idx)\n",
    "        self.target_test = self._get_subset(self.target_blocks, self.test_idx)\n",
    "\n",
    "        self.train_frames = [self.structures[i] for i in self.train_idx]\n",
    "        self.val_frames = [self.structures[i] for i in self.val_idx]\n",
    "        self.test_frames = [self.structures[i] for i in self.test_idx]\n",
    "       \n",
    "       \n",
    "        self.feature_names = self.features['plus'].keys.values\n",
    "        self.feat_train = self._get_subset(self.features, self.train_idx)\n",
    "        self.feat_val = self._get_subset(self.features, self.val_idx)\n",
    "        self.feat_test = self._get_subset(self.features, self.test_idx)\n",
    "\n",
    "\n",
    "    def _set_model_return(self, model_return: str = \"blocks\"):\n",
    "        ## Helper function to set output in __get_item__ for model training\n",
    "        assert model_return in [\n",
    "            \"blocks\",\n",
    "            \"tensor\",\n",
    "        ], \"model_target must be one of [blocks, tensor]\"\n",
    "        self.model_return = model_return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            # idx = [i.item() for i in idx]\n",
    "            idx = idx.tolist()\n",
    "        if not self.model_type == \"acdc\":\n",
    "            return self.structures[idx], self.target.tensor[idx]\n",
    "        else:\n",
    "            assert (\n",
    "                self.features is not None\n",
    "            ), \"Features not set, call _set_features() first\"\n",
    "            x = operations.slice(\n",
    "                self.features,\n",
    "                axis=\"samples\",\n",
    "                labels=Labels(\n",
    "                    names=[\"structure\"], values=np.asarray([idx]).reshape(-1, 1)\n",
    "                ),\n",
    "            )\n",
    "            if self.model_return == \"blocks\":\n",
    "                y = operations.slice(\n",
    "                    self.target.blocks,\n",
    "                    axis=\"samples\",\n",
    "                    labels=Labels(\n",
    "                        names=[\"structure\"], values=np.asarray([idx]).reshape(-1, 1)\n",
    "                    ),\n",
    "                )\n",
    "            else:\n",
    "                idx = [i.item() for i in idx]\n",
    "                y = self.target.tensor[idx]\n",
    "            # x = metatensor.to(x, \"torch\")\n",
    "            # y = metatensor.to(y, \"torch\")\n",
    "\n",
    "            return x, y, idx\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        x = batch[0][0]\n",
    "        y = batch[0][1]\n",
    "        idx = batch[0][2]\n",
    "        return {\"input\": x, \"output\": y, \"idx\": idx}\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    ml_data: MLDataset,\n",
    "    collate_fn: callable = None,\n",
    "    batch_size: int = 4,\n",
    "    drop_last: bool = False,\n",
    "    selection: Optional[str] = \"all\",\n",
    "    model_return: Optional[str] = None,\n",
    "):\n",
    "    assert selection in [\n",
    "        \"all\",\n",
    "        \"train\",\n",
    "        \"val\",\n",
    "        \"test\",\n",
    "    ], \"selection must be one of [all, train, val, test]\"\n",
    "    if collate_fn is None:\n",
    "        collate_fn = ml_data.collate_fn\n",
    "\n",
    "    assert model_return in [\n",
    "        \"blocks\",\n",
    "        \"tensor\",\n",
    "    ], \"model_target must be one of [blocks, tensor]\"\n",
    "    ml_data._set_model_return(model_return)\n",
    "    train_sampler = data.sampler.SubsetRandomSampler(ml_data.train_idx)\n",
    "    train_sampler = data.sampler.BatchSampler(\n",
    "        train_sampler, batch_size=batch_size, drop_last=drop_last\n",
    "    )\n",
    "\n",
    "    val_sampler = data.sampler.SubsetRandomSampler(ml_data.val_idx)\n",
    "    val_sampler = data.sampler.BatchSampler(\n",
    "        val_sampler, batch_size=batch_size, drop_last=drop_last\n",
    "    )\n",
    "\n",
    "    test_sampler = data.sampler.SubsetRandomSampler(ml_data.test_idx)\n",
    "    test_sampler = data.sampler.BatchSampler(\n",
    "        test_sampler, batch_size=batch_size, drop_last=drop_last\n",
    "    )\n",
    "    train_loader = data.DataLoader(\n",
    "        ml_data,\n",
    "        sampler=train_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    val_loader = data.DataLoader(\n",
    "        ml_data,\n",
    "        sampler=val_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    test_loader = data.DataLoader(\n",
    "        ml_data,\n",
    "        sampler=test_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    if selection.lower() == \"all\":\n",
    "        return train_loader, val_loader, test_loader\n",
    "    elif selection.lower() == \"train\":\n",
    "        return train_loader\n",
    "    elif selection.lower() == \"val\":\n",
    "        return val_loader\n",
    "    elif selection.lower() == \"test\":\n",
    "        return test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LinearModelPeriodic(nn.Module):\n",
    "    def __init__(self, feat_plus, feat_minus, target_blocks_sum, target_blocks_diff, cell_shifts, frames, orbitals, device=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.feat_plus = feat_plus\n",
    "        self.feat_minus = feat_minus\n",
    "        self.target_blocks_sum = target_blocks_sum\n",
    "        self.target_blocks_diff = target_blocks_diff\n",
    "        self.cell_shifts = cell_shifts #Doesnt belong here #TODO extract this better \n",
    "        self.frames = frames\n",
    "        self.orbitals = orbitals\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dummy_property = next(iter(self.target_blocks_sum.values()))[0].properties\n",
    "        self._submodels(**kwargs)\n",
    "\n",
    "    def _submodels(self, **kwargs):\n",
    "        self.blockmodels = {}\n",
    "        for s in self.cell_shifts: \n",
    "            shiftmodels ={}\n",
    "            for k in self.target_blocks_sum[tuple(s)].keys:\n",
    "                blockval = torch.linalg.norm(self.target_blocks_sum[tuple(s)][k].values)\n",
    "                if blockval > 1e-5:\n",
    "                    \n",
    "                    feat = map_targetkeys_to_featkeys(self.feat_plus, k, cell_shift=s)\n",
    "                    shiftmodels[str(tuple(k)+(1,))] = MLP(nin=feat.values.shape[-1], nout=1, nhidden=kwargs.get(\"nhidden\",10), nlayers=kwargs.get(\"nlayers\",2))\n",
    "            if self.target_blocks_diff is not None:\n",
    "                for k in self.target_blocks_diff[tuple(s)].keys:\n",
    "                    blockval = torch.linalg.norm(self.target_blocks_diff[tuple(s)][k].values)\n",
    "                    if blockval > 1e-5:\n",
    "                        feat = map_targetkeys_to_featkeys(self.feat_minus, k, cell_shift=s)\n",
    "                        shiftmodels[str(tuple(k)+(-1,))] = MLP(nin=feat.values.shape[-1], nout=1, nhidden=kwargs.get(\"nhidden\",10), nlayers=kwargs.get(\"nlayers\",2))\n",
    "                    \n",
    "            self.blockmodels[str(s)] = torch.nn.ModuleDict(shiftmodels)\n",
    "        self.model = torch.nn.ModuleDict(self.blockmodels)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def forward(self):\n",
    "        self.recon_sum = {}\n",
    "        if self.target_blocks_diff is not None:\n",
    "            self.recon_diff = {} \n",
    "        else: \n",
    "            self.recon_diff = None\n",
    "        for s in self.cell_shifts: \n",
    "            pred_blocks_sum = []\n",
    "            pred_blocks_diff =[]\n",
    "            for k in self.target_blocks_sum[tuple(s)].keys:\n",
    "                blockval = torch.linalg.norm(self.target_blocks_sum[tuple(s)][k].values)\n",
    "                if blockval > 1e-5:\n",
    "                    feat = map_targetkeys_to_featkeys(self.feat_plus, k, cell_shift=s)\n",
    "                    nsamples, ncomp, nprops = feat.values.shape\n",
    "                    pred = self.blockmodels[str(s)][str(tuple(k)+(1,))](feat.values)\n",
    "                    pred_blocks_sum.append( TensorBlock(\n",
    "                    values=pred.reshape((nsamples, ncomp, 1)),\n",
    "                    samples=self.target_blocks_sum[tuple(s)][k].samples,\n",
    "                    components=self.target_blocks_sum[tuple(s)][k].components, #feat.components,\n",
    "                    properties=self.dummy_property,\n",
    "                    ))\n",
    "                else: \n",
    "                    pred_blocks_sum.append(self.target_blocks_sum[tuple(s)][k].copy())\n",
    "            pred_sum_tmap = TensorMap(self.target_blocks_sum[tuple(s)].keys, pred_blocks_sum)\n",
    "            self.recon_sum[tuple(s)] = _to_matrix(_to_uncoupled_basis(pred_sum_tmap), frames = self.frames, orbitals=self.orbitals)\n",
    "            if self.target_blocks_diff is not None:\n",
    "                for k in self.target_blocks_diff[tuple(s)].keys:\n",
    "                    blockval = torch.linalg.norm(self.target_blocks_diff[tuple(s)][k].values)\n",
    "                    if blockval > 1e-5:\n",
    "                        feat = map_targetkeys_to_featkeys(self.feat_minus, k, cell_shift=s)\n",
    "                        nsamples, ncomp, nprops = feat.values.shape\n",
    "                        pred = self.blockmodels[str(s)][str(tuple(k)+(-1,))](feat.values)\n",
    "                        pred_blocks_diff.append( TensorBlock(\n",
    "                        values=pred.reshape((nsamples, ncomp, 1)),\n",
    "                        samples=self.target_blocks_diff[tuple(s)][k].samples, #feat.samples,\n",
    "                        components=self.target_blocks_diff[tuple(s)][k].components, # feat.components,\n",
    "                        properties=self.dummy_property,\n",
    "                        ))\n",
    "\n",
    "                    else:\n",
    "                        pred_blocks_diff.append(self.target_blocks_diff[tuple(s)][k].copy())\n",
    "            \n",
    "                pred_diff_tmap = TensorMap(self.target_blocks_diff[tuple(s)].keys, pred_blocks_diff)   \n",
    "            \n",
    "            # self.recon_sum[tuple(s)] = _to_matrix(_to_uncoupled_basis(pred_sum_tmap), frames = self.frames, orbitals=self.orbitals)\n",
    "                self.recon_diff[tuple(s)] = _to_matrix(_to_uncoupled_basis(pred_diff_tmap), frames = self.frames, orbitals=self.orbitals, hermitian=False)\n",
    "        \n",
    "        return self.recon_sum, self.recon_diff\n",
    "    \n",
    "    def fit_ridge_analytical(self) -> None:\n",
    "        for shiftkey, model in self.models.item():\n",
    "            for i, (k, submodel) in model.items():\n",
    "                # feat = \n",
    "                feat_block = self.get_feature_block(\n",
    "                    features=features, key=key, core_features=core_features\n",
    "                )\n",
    "                targ_block = targets[key]\n",
    "\n",
    "                x = np.array(feat_block.values.reshape(-1, feat_block.values.shape[2]))\n",
    "                y = np.array(targ_block.values.reshape(-1, 1))\n",
    "\n",
    "                ridge = Ridge(alpha=self.alpha, fit_intercept=self.bias).fit(x, y)\n",
    "\n",
    "                model.layer.weight = torch.nn.Parameter(\n",
    "                    torch.from_numpy(ridge.coef_.copy().astype(np.float64))\n",
    "                )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "def loss_zero_shift(pred, target, device=None):\n",
    "    if device is None: \n",
    "        device = next(iter(pred.values())).device\n",
    "    assert pred.keys() == target.keys()\n",
    "    loss = 0\n",
    "    \n",
    "    loss += torch.sum((pred[tuple(s)]-target[tuple(s)])**2)\n",
    "    return loss\n",
    "\n",
    "def loss_fn_indiv_shift(rsum, rdiff, matrix_plust, matrix_minust, specific_shift_idx:Union[str, List]=None, device=None):\n",
    "    #TODO: loss over particular shifts\n",
    "    if device is None: \n",
    "        device = next(iter(rsum.values())).device\n",
    "    # assert rsum.keys() == rdiff.keys()\n",
    "    # assert rsum.keys() == matrix_plust.keys() # zero shift missing \n",
    "    weight_minus = 1\n",
    "    weight_plus = 1\n",
    "    if matrix_minust is not None:\n",
    "        # assert rsum.keys() == matrix_minust.keys()\n",
    "        if not isinstance(next(iter(matrix_minust.values())), torch.Tensor):\n",
    "            matrix_minust = {k:torch.from_numpy(v).type(torch.float).to(device) for k,v in matrix_minust.items()}\n",
    "    if not isinstance(next(iter(matrix_plust.values())), torch.Tensor):\n",
    "        matrix_plust = {k:torch.from_numpy(v).type(torch.float).to(device) for k,v in matrix_plust.items()}\n",
    "    loss = 0\n",
    "    if isinstance(specific_shift_idx, list):\n",
    "        raise NotImplementedError\n",
    "    elif isinstance(specific_shift_idx, str):\n",
    "        if specific_shift_idx == \"positive\":\n",
    "            weight_minus=0\n",
    "        elif specific_shift_idx == \"negative\":\n",
    "            weight_plus=0\n",
    "           \n",
    "    for s in rsum.keys():\n",
    "        if rdiff is not None:\n",
    "            plust = rsum[tuple(s)] + rdiff[tuple(s)]\n",
    "            minust = rsum[tuple(s)] - rdiff[tuple(s)]\n",
    "        else: \n",
    "            plust = rsum[tuple(s)]\n",
    "            minust = rsum[tuple(s)]\n",
    "        loss += weight_plus*torch.sum((plust-matrix_plust[tuple(s)])**2) + weight_minus*torch.sum((minust-matrix_minust[tuple(s)])**2)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss_fn_combined(rsum, rdiff, expkL:dict, complex_target, device = None):\n",
    "    #TODO : support multiple k points \n",
    "    if device is None: \n",
    "        device = next(iter(rsum.values())).device\n",
    "        complex_target = complex_target.to(device)\n",
    "    assert rsum.keys() == rdiff.keys()\n",
    "    matrix = {}\n",
    "    for s in rsum.keys():\n",
    "        matrix[tuple(s)] = rsum[tuple(s)] + rdiff[tuple(s)]\n",
    "        matrix[tuple(-np.array(s))] = rsum[s] - rdiff[s]\n",
    "\n",
    "    recon_target = torch.zeros_like(complex_target, requires_grad=True, dtype = torch.complex64, device = device)\n",
    "    for s in matrix.keys():\n",
    "         recon_target = recon_target+ matrix[s]*expkL[s]\n",
    "    \n",
    "    loss = torch.tensordot((recon_target-complex_target),torch.conj(recon_target-complex_target)) \n",
    "    # equivalent to torch.linalg.norm((recon_target-complex_target))**2\n",
    "    assert torch.isclose(abs(loss), abs(loss.real))\n",
    "    return loss.real\n",
    "\n",
    "def get_predicted_matrices(rsum, rdiff, device=None):\n",
    "    if device is None: \n",
    "        device = next(iter(rsum.values())).device\n",
    "    assert rsum.keys() == rdiff.keys(), \"rsum and rdiff must have same keys\"\n",
    "    matrix = {}\n",
    "    for s in rsum.keys():\n",
    "        # sint = [int(x) for x in s[1:-1].split(\", \")]\n",
    "        matrix[s] = rsum[s] + rdiff[s]\n",
    "        matrix[tuple(-np.array(s))] = rsum[s] - rdiff[s]\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "93921e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModelPeriodic( zeroshift, None, target_coupled_blocks_sum, None, desired_shifts[:1], frames, orbs, nhidden=16, nlayers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f79c4355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0003953083069063723\n",
      "10 0.0003858382988255471\n",
      "20 0.00037969197728671134\n",
      "30 0.0003737412625923753\n",
      "40 0.0003678331268019974\n",
      "50 0.00036227484815753996\n",
      "60 0.00035727518843486905\n",
      "70 0.00043549900874495506\n",
      "80 0.0004191058105789125\n",
      "90 0.00035224761813879013\n",
      "100 0.000355529657099396\n",
      "110 0.00033384672133252025\n",
      "120 0.00032796338200569153\n",
      "130 0.00032371701672673225\n",
      "140 0.0004698431584984064\n",
      "150 0.0005213679978623986\n",
      "160 0.0005985862808302045\n",
      "170 0.0003761534171644598\n",
      "180 0.00031872763065621257\n",
      "190 0.0003236045013181865\n",
      "200 0.00030117708956822753\n",
      "210 0.0002947057946585119\n",
      "220 0.0002884854329749942\n",
      "230 0.0002843063557520509\n",
      "240 0.00028082949575036764\n",
      "250 0.00027749736909754574\n",
      "260 0.00027424393920227885\n",
      "270 0.0002711306151468307\n",
      "280 0.00026802613865584135\n",
      "290 0.0002666277578100562\n",
      "300 0.0005027313600294292\n",
      "310 0.00038971786852926016\n",
      "320 0.0006213482702150941\n",
      "330 0.0003061413299292326\n",
      "340 0.00026638031704351306\n",
      "350 0.00026559620164334774\n",
      "360 0.00024645542725920677\n",
      "370 0.0002459342358633876\n",
      "380 0.00024139534798450768\n",
      "390 0.00023896756465546787\n",
      "400 0.00023667345521971583\n",
      "410 0.0002342165680602193\n",
      "420 0.00023203162709251046\n",
      "430 0.00022986266412772238\n",
      "440 0.00022773865202907473\n",
      "450 0.00022566693951375782\n",
      "460 0.00022359687136486173\n",
      "470 0.0002215670538134873\n",
      "480 0.0002195902052335441\n",
      "490 0.0002177109126932919\n",
      "500 0.00022491259733214974\n",
      "510 0.00023357852478511631\n",
      "520 0.0009754170896485448\n",
      "530 0.0017329475376755\n",
      "540 0.0006730314344167709\n",
      "550 0.0003432349185459316\n",
      "560 0.0021311030723154545\n",
      "570 0.0008002562681213021\n",
      "580 0.0003718746593222022\n",
      "590 0.0002507234166841954\n",
      "600 0.0002104315790347755\n",
      "610 0.0001986203424166888\n",
      "620 0.0001961856905836612\n",
      "630 0.00019514255109243095\n",
      "640 0.00019317399710416794\n",
      "650 0.00019122418598271906\n",
      "660 0.00018992790137417614\n",
      "670 0.00019170422456227243\n",
      "680 0.0005341029609553516\n",
      "690 0.0002895246143452823\n",
      "700 0.0005964555311948061\n",
      "710 0.0018015834502875805\n",
      "720 0.0002084985317196697\n",
      "730 0.00039266396197490394\n",
      "740 0.00025116850156337023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[287], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1300\u001b[39m):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# loss = closure()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# optimizer.step(closure)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m     rsum, rdiff \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# print(rsum.keys())\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m )\n",
      "Cell \u001b[0;32mIn[280], line 48\u001b[0m, in \u001b[0;36mLinearModelPeriodic.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m pred_blocks_diff \u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_blocks_sum[\u001b[38;5;28mtuple\u001b[39m(s)]\u001b[38;5;241m.\u001b[39mkeys:\n\u001b[0;32m---> 48\u001b[0m     blockval \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_blocks_sum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m blockval \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-5\u001b[39m:\n\u001b[1;32m     50\u001b[0m         feat \u001b[38;5;241m=\u001b[39m map_targetkeys_to_featkeys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_plus, k, cell_shift\u001b[38;5;241m=\u001b[39ms)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/tensor.py:151\u001b[0m, in \u001b[0;36mTensorMap.__getitem__\u001b[0;34m(self, selection)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, selection) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorBlock:\n\u001b[1;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This is equivalent to self.block(selection)\"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselection\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/tensor.py:335\u001b[0m, in \u001b[0;36mTensorMap.block\u001b[0;34m(self, selection, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_by_id(selection)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     selection \u001b[38;5;241m=\u001b[39m \u001b[43m_normalize_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m matching \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks_matching(selection)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matching) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/tensor.py:618\u001b[0m, in \u001b[0;36m_normalize_selection\u001b[0;34m(selection)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m selection\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(selection, LabelsEntry):\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLabels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid type for block selection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(selection)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/labels.py:300\u001b[0m, in \u001b[0;36mLabels.__init__\u001b[0;34m(self, names, values)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels values must be convertible to integers\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib \u001b[38;5;241m=\u001b[39m _get_library()\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labels \u001b[38;5;241m=\u001b[39m \u001b[43m_create_new_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lib\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_names \u001b[38;5;241m=\u001b[39m names\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values \u001b[38;5;241m=\u001b[39m LabelsValues(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/labels.py:997\u001b[0m, in \u001b[0;36m_create_new_labels\u001b[0;34m(lib, names, values)\u001b[0m\n\u001b[1;32m    995\u001b[0m labels\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mctypes\u001b[38;5;241m.\u001b[39mdata_as(ctypes\u001b[38;5;241m.\u001b[39mPOINTER(ctypes\u001b[38;5;241m.\u001b[39mc_int32))\n\u001b[1;32m    996\u001b[0m labels\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 997\u001b[0m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmts_labels_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labels\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/status.py:28\u001b[0m, in \u001b[0;36m_check_status\u001b[0;34m(status)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m LAST_EXCEPTION\n\u001b[1;32m     25\u001b[0m     LAST_EXCEPTION \u001b[38;5;241m=\u001b[39m e\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_status\u001b[39m(status):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m MTS_SUCCESS:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# # optimizer= torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# # optimizer = torch.optim.LBFGS(model.parameters(), lr=1e-4)\n",
    "# # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True)\n",
    "# def closure():\n",
    "#     optimizer.zero_grad()\n",
    "#     rsum, rdiff = model.forward()\n",
    "#     loss = loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx = 'positive' )\n",
    "#     loss.backward()\n",
    "#     return loss\n",
    "\n",
    "# losses = []\n",
    "for i in range(1300):\n",
    "    # loss = closure()\n",
    "    # optimizer.step(closure)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    rsum, rdiff = model.forward()\n",
    "    # print(rsum.keys())\n",
    "    loss = loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx = 'positive' )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step(loss)\n",
    "    losses.append(loss.item())\n",
    "    if i%10 ==0:\n",
    "        print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "be063a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsum_0 = model.forward()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModelPeriodic(feat_plus, feat_minus, target_coupled_blocks_sum, target_coupled_blocks_diff, desired_shifts[1:], frames, orbs, nhidden=16, nlayers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm the matrices before hand? but then how do we recover the actualm matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 68.53742980957031\n",
      "10 48.39392852783203\n",
      "20 28.66879653930664\n",
      "30 15.584948539733887\n",
      "40 8.948714256286621\n",
      "50 4.5619635581970215\n",
      "60 2.0557236671447754\n",
      "70 1.0691601037979126\n",
      "80 0.46487534046173096\n",
      "90 0.2473144829273224\n",
      "100 0.1785847246646881\n",
      "110 0.1247759610414505\n",
      "120 0.10865583270788193\n",
      "130 0.10135360062122345\n",
      "140 0.09514949470758438\n",
      "150 0.0898614302277565\n",
      "160 0.08484591543674469\n",
      "170 0.08012333512306213\n",
      "180 0.07572506368160248\n",
      "190 0.07157675176858902\n",
      "200 0.06765581667423248\n",
      "210 0.06394819170236588\n",
      "220 0.06044171005487442\n",
      "230 0.057127147912979126\n",
      "240 0.05399477109313011\n",
      "250 0.051035840064287186\n",
      "260 0.048241935670375824\n",
      "270 0.045605190098285675\n",
      "280 0.043117962777614594\n",
      "290 0.040773071348667145\n",
      "300 0.03856339305639267\n",
      "310 0.036482252180576324\n",
      "320 0.03452315926551819\n",
      "330 0.03267982602119446\n",
      "340 0.030946258455514908\n",
      "350 0.02931666374206543\n",
      "360 0.02778547629714012\n",
      "370 0.026347298175096512\n",
      "380 0.02499706670641899\n",
      "390 0.023729851469397545\n",
      "400 0.02254086546599865\n",
      "410 0.021425606682896614\n",
      "420 0.020379744470119476\n",
      "430 0.019399166107177734\n",
      "440 0.01847992092370987\n",
      "450 0.017618214711546898\n",
      "460 0.01681048423051834\n",
      "470 0.016053317114710808\n",
      "480 0.015343518927693367\n",
      "490 0.014677969738841057\n",
      "500 0.014053834602236748\n",
      "510 0.01346835121512413\n",
      "520 0.012918924912810326\n",
      "530 0.01240321435034275\n",
      "540 0.01191888190805912\n",
      "550 0.011463815346360207\n",
      "560 0.011035989038646221\n",
      "570 0.01063358224928379\n",
      "580 0.010254831984639168\n",
      "590 0.00989808514714241\n",
      "600 0.009561881422996521\n",
      "610 0.009244757704436779\n",
      "620 0.008945439010858536\n",
      "630 0.00866268202662468\n",
      "640 0.008395353332161903\n",
      "650 0.00814241822808981\n",
      "660 0.007902885787189007\n",
      "670 0.0076758358627557755\n",
      "680 0.007460453547537327\n",
      "690 0.007255963049829006\n",
      "700 0.0070615503937006\n",
      "710 0.0068766591139137745\n",
      "720 0.006700590252876282\n",
      "730 0.006532787811011076\n",
      "740 0.006372721400111914\n",
      "750 0.006219890434294939\n",
      "760 0.006073818542063236\n",
      "770 0.005934093613177538\n",
      "780 0.005800291895866394\n",
      "790 0.005672072991728783\n",
      "800 0.005549068562686443\n",
      "810 0.005430973134934902\n",
      "820 0.005317480303347111\n",
      "830 0.005208312068134546\n",
      "840 0.0051032304763793945\n",
      "850 0.0050019770860672\n",
      "860 0.004904340952634811\n",
      "870 0.00481010414659977\n",
      "880 0.004719087854027748\n",
      "890 0.004631112329661846\n",
      "900 0.004546006675809622\n",
      "910 0.004463993012905121\n",
      "920 0.004383852705359459\n",
      "930 0.004306452348828316\n",
      "940 0.004231339320540428\n",
      "950 0.004158458672463894\n",
      "960 0.004087665118277073\n",
      "970 0.004018864128738642\n",
      "980 0.003951946273446083\n",
      "990 0.0038868349511176348\n",
      "1000 0.0038234347011893988\n",
      "1010 0.0037616833578795195\n",
      "1020 0.0037015473935753107\n",
      "1030 0.0036428351886570454\n",
      "1040 0.0035855770111083984\n",
      "1050 0.0035297214053571224\n",
      "1060 0.003475168952718377\n",
      "1070 0.003421899862587452\n",
      "1080 0.0033698678016662598\n",
      "1090 0.003319007810205221\n",
      "1100 0.0032692605163902044\n",
      "1110 0.003220628947019577\n",
      "1120 0.003173047211021185\n",
      "1130 0.0031264799181371927\n",
      "1140 0.0030809082090854645\n",
      "1150 0.0030362873803824186\n",
      "1160 0.002992618130519986\n",
      "1170 0.0029497873038053513\n",
      "1180 0.0029078416991978884\n",
      "1190 0.0028667496517300606\n",
      "1200 0.002826712792739272\n",
      "1210 0.002786993980407715\n",
      "1220 0.0027482761070132256\n",
      "1230 0.0027103200554847717\n",
      "1240 0.0026731458492577076\n",
      "1250 0.0026366179808974266\n",
      "1260 0.0026007688138633966\n",
      "1270 0.002565619070082903\n",
      "1280 0.002531124744564295\n",
      "1290 0.0024972837418317795\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# optimizer= torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.LBFGS(model.parameters(), lr=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True)\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    rsum, rdiff = model.forward()\n",
    "    loss = loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx = 'positive' )\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "losses = []\n",
    "for i in range(1300):\n",
    "    # loss = closure()\n",
    "    # optimizer.step(closure)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    rsum, rdiff = model.forward()\n",
    "    loss = loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx = 'positive' )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step(loss)\n",
    "    losses.append(loss.item())\n",
    "    if i%10 ==0:\n",
    "        print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "f16b4ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0] tensor(0.0114, dtype=torch.float64)\n",
      "[0, 1, 0] tensor(0.0206, dtype=torch.float64)\n",
      "[1, 1, 0] tensor(0.0149, dtype=torch.float64)\n",
      "[1, -1, 0] tensor(0.0003, dtype=torch.float64)\n",
      "[2, 0, 0] tensor(0.0002, dtype=torch.float64)\n",
      "[0, 2, 0] tensor(0.0008, dtype=torch.float64)\n",
      "[2, 1, 0] tensor(0.0003, dtype=torch.float64)\n",
      "[3, 0, 0] tensor(0.0211, dtype=torch.float64)\n",
      "[0, 3, 0] tensor(0.0353, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "rsum_final_plus, rdiff_final_plus = model.forward()\n",
    "PLUS={}\n",
    "for s in desired_shifts[1:]:\n",
    "    PLUS[tuple(s)] = rsum_final_plus[tuple(s)] - rdiff_final_plus[tuple(s)]\n",
    "    print(s, torch.linalg.norm(PLUS[tuple(s)].detach().cpu() - matrices_sum[tuple(s)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModelPeriodic(feat_plus, feat_minus, target_coupled_blocks_sum, target_coupled_blocks_diff, desired_shifts[1:], frames, orbs, nhidden=16, nlayers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038311250507831573\n",
      "0.036352187395095825\n",
      "0.03450614586472511\n",
      "0.032767415046691895\n",
      "0.031130407005548477\n",
      "0.029589761048555374\n",
      "0.02814016304910183\n",
      "0.026776667684316635\n",
      "0.025494370609521866\n",
      "0.024288680404424667\n",
      "0.023155121132731438\n",
      "0.022089514881372452\n",
      "0.021087804809212685\n",
      "0.020146159455180168\n",
      "0.019260942935943604\n",
      "0.018428726121783257\n",
      "0.01764625310897827\n",
      "0.0169103741645813\n",
      "0.016218237578868866\n",
      "0.015567049384117126\n",
      "0.01495424099266529\n",
      "0.01437731459736824\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[295], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m800\u001b[39m):\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m     rsum, rdiff \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[280], line 62\u001b[0m, in \u001b[0;36mLinearModelPeriodic.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         pred_blocks_sum\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_blocks_sum[\u001b[38;5;28mtuple\u001b[39m(s)][k]\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m     61\u001b[0m pred_sum_tmap \u001b[38;5;241m=\u001b[39m TensorMap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_blocks_sum[\u001b[38;5;28mtuple\u001b[39m(s)]\u001b[38;5;241m.\u001b[39mkeys, pred_blocks_sum)\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecon_sum[\u001b[38;5;28mtuple\u001b[39m(s)] \u001b[38;5;241m=\u001b[39m \u001b[43m_to_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_uncoupled_basis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_sum_tmap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morbitals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morbitals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_blocks_diff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_blocks_diff[\u001b[38;5;28mtuple\u001b[39m(s)]\u001b[38;5;241m.\u001b[39mkeys:\n",
      "File \u001b[0;32m/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/twocenter_utils.py:324\u001b[0m, in \u001b[0;36m_to_matrix\u001b[0;34m(blocks, frames, orbitals, hermitian, device)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_matrix\u001b[39m(\n\u001b[1;32m    313\u001b[0m     blocks: TensorMap,\n\u001b[1;32m    314\u001b[0m     frames: List[ase\u001b[38;5;241m.\u001b[39mAtoms],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m#     return _vectorized_blocks_to_matrix(blocks=blocks, frames=frames, orbs=orbs)\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_blocks_to_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43morbitals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morbitals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhermitian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhermitian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/twocenter_utils.py:407\u001b[0m, in \u001b[0;36m_blocks_to_matrix\u001b[0;34m(blocks, frames, orbitals, device, hermitian)\u001b[0m\n\u001b[1;32m    405\u001b[0m         values \u001b[38;5;241m=\u001b[39m block_data[:, :, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m li \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m lj \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;66;03m# assign values\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m         _fill(\n\u001b[1;32m    408\u001b[0m             block_type,\n\u001b[1;32m    409\u001b[0m             matrix,\n\u001b[1;32m    410\u001b[0m             values,\n\u001b[1;32m    411\u001b[0m             ki_base,\n\u001b[1;32m    412\u001b[0m             kj_base,\n\u001b[1;32m    413\u001b[0m             ki_offset,\n\u001b[1;32m    414\u001b[0m             kj_offset,\n\u001b[1;32m    415\u001b[0m             same_koff,\n\u001b[1;32m    416\u001b[0m             li,\n\u001b[1;32m    417\u001b[0m             lj,\n\u001b[1;32m    418\u001b[0m             hermitian\u001b[38;5;241m=\u001b[39mhermitian,\n\u001b[1;32m    419\u001b[0m         )\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matrices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m matrices[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=1e-1, step_size_up=10, cycle_momentum=False)\n",
    "# losses = []\n",
    "for i in range(800):\n",
    "    optimizer.zero_grad()\n",
    "    rsum, rdiff = model.forward()\n",
    "    loss = loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx = 'negative' )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step(loss)\n",
    "    losses.append(loss.item())\n",
    "    if i%10 ==0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c13cf759",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsum_final_minus, rdiff_final_minus = model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "8e302a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0] tensor(0.0376, dtype=torch.float64)\n",
      "[0, 1, 0] tensor(0.0375, dtype=torch.float64)\n",
      "[1, 1, 0] tensor(0.0689, dtype=torch.float64)\n",
      "[1, -1, 0] tensor(0.0013, dtype=torch.float64)\n",
      "[2, 0, 0] tensor(0.0002, dtype=torch.float64)\n",
      "[0, 2, 0] tensor(0.0002, dtype=torch.float64)\n",
      "[2, 1, 0] tensor(0.0007, dtype=torch.float64)\n",
      "[3, 0, 0] tensor(0.0447, dtype=torch.float64)\n",
      "[0, 3, 0] tensor(0.0672, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for s in desired_shifts[:]:\n",
    "#     print(s, torch.linalg.norm(rsum_final_minus[tuple(s)].detach().cpu() - matrices_sum[tuple(s)]))\n",
    "#     print(s, torch.linalg.norm(rdiff_final_minus[tuple(s)].detach().cpu() - matrices_diff[tuple(s)]))\n",
    "# predicted_matrices = get_predicted_matrices(rsum_final_minus, rdiff_final_minus)\n",
    "MINUS={}\n",
    "for s in desired_shifts[1:]:\n",
    "    MINUS[tuple(s)] = rsum_final_minus[tuple(s)] - rdiff_final_minus[tuple(s)]\n",
    "    print(s, torch.linalg.norm(MINUS[tuple(s)].detach().cpu() - matrices_diff[tuple(s)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "55349761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nigam/miniconda3/lib/python3.11/site-packages/hickle/lookup.py:1491: SerializedWarning: 'Tensor' type not understood, data is serialized:\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hickle.dump(PLUS, \"PLUS.hickle\")\n",
    "hickle.dump(MINUS, \"MINUS.hickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 0) tensor(0.0393, dtype=torch.float64) 2.1654325656602817\n",
      "(-1, 0, 0) tensor(0.0393, dtype=torch.float64) 2.165432565660281\n",
      "(0, 1, 0) tensor(0.0428, dtype=torch.float64) 6.561073399624882\n",
      "(0, -1, 0) tensor(0.0428, dtype=torch.float64) 6.561073399624881\n",
      "(1, 1, 0) tensor(0.0706, dtype=torch.float64) 6.647242582973258\n",
      "(-1, -1, 0) tensor(0.0705, dtype=torch.float64) 6.6472425829732575\n",
      "(1, -1, 0) tensor(0.0013, dtype=torch.float64) 0.1612810052173518\n",
      "(-1, 1, 0) tensor(0.0013, dtype=torch.float64) 0.16128100521735184\n",
      "(2, 0, 0) tensor(0.0003, dtype=torch.float64) 0.03207237927488492\n",
      "(-2, 0, 0) tensor(0.0003, dtype=torch.float64) 0.03207237927488493\n",
      "(0, 2, 0) tensor(0.0008, dtype=torch.float64) 0.2172764923110687\n",
      "(0, -2, 0) tensor(0.0008, dtype=torch.float64) 0.2172764923110686\n",
      "(2, 1, 0) tensor(0.0007, dtype=torch.float64) 0.16556662788895682\n",
      "(-2, -1, 0) tensor(0.0007, dtype=torch.float64) 0.16556662788895682\n",
      "(3, 0, 0) tensor(0.0494, dtype=torch.float64) 2.1654325656957245\n",
      "(-3, 0, 0) tensor(0.0494, dtype=torch.float64) 2.1654325656957245\n",
      "(0, 3, 0) tensor(0.0759, dtype=torch.float64) 6.561073400180453\n",
      "(0, -3, 0) tensor(0.0759, dtype=torch.float64) 6.561073400180453\n",
      "(0, 0, 0) tensor(0.0150, dtype=torch.float64) 54.20150788504924\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reconstruct_FIN = {}\n",
    "for s in desired_shifts[1:]:\n",
    "    reconstruct_FIN[tuple(s)] = PLUS[tuple(s)] + MINUS[tuple(s)] \n",
    "    reconstruct_FIN[tuple(-np.array(s))] = PLUS[tuple(s)] - MINUS[tuple(s)]\n",
    "\n",
    "for s in desired_shifts[:1]:\n",
    "    reconstruct_FIN[tuple(s)] = rsum_0[tuple(s)]\n",
    "\n",
    "for s in reconstruct_FIN.keys():\n",
    "    print(s, torch.linalg.norm(reconstruct_FIN[tuple(s)].detach().cpu() - selected_matrices[tuple(s)]), np.linalg.norm(selected_matrices[tuple(s)]))\n",
    "# reconstruct_FIN = get_predicted_matrices(PLUS, MINUS)\n",
    "# for s in desired_shifts[:]:\n",
    "#     print(s, torch.linalg.norm(reconstruct_FIN[tuple(s)].detach().cpu() - selected_matrices[tuple(s)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3f3c5",
   "metadata": {},
   "source": [
    "## reconstruct k-point target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "e6c0d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recons_kpt_target = []\n",
    "for ifr, frame in enumerate(frames): \n",
    "    kmatrix = torch.zeros((Nk, nao, nao)).type(torch.complex128)\n",
    "\n",
    "    # shift_indices = [np.where(np.all(Ls==np.asarray(s), axis=1))[0][0] for s in withnegative_shifts]\n",
    "    for key in withnegative_shifts:\n",
    "        key = tuple(key)\n",
    "        for kpt in range(Nk):\n",
    "            kmatrix[kpt] += reconstruct_FIN[key][ifr].detach().cpu() * weights[key] * expkL[key][kpt]\n",
    "    kmatrix = kmatrix / Nk\n",
    "    recons_kpt_target.append(kmatrix) \n",
    "recons_kpt_target = torch.stack(recons_kpt_target).swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "b72537d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0651, dtype=torch.float64)\n",
      "1 tensor(0.1141, dtype=torch.float64)\n",
      "2 tensor(0.0581, dtype=torch.float64)\n",
      "3 tensor(0.1141, dtype=torch.float64)\n",
      "4 tensor(0.1347, dtype=torch.float64)\n",
      "5 tensor(0.1178, dtype=torch.float64)\n",
      "6 tensor(0.1090, dtype=torch.float64)\n",
      "7 tensor(0.1183, dtype=torch.float64)\n",
      "8 tensor(0.0658, dtype=torch.float64)\n",
      "9 tensor(0.1248, dtype=torch.float64)\n",
      "10 tensor(0.0615, dtype=torch.float64)\n",
      "11 tensor(0.1248, dtype=torch.float64)\n",
      "12 tensor(0.1347, dtype=torch.float64)\n",
      "13 tensor(0.1183, dtype=torch.float64)\n",
      "14 tensor(0.1090, dtype=torch.float64)\n",
      "15 tensor(0.1178, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for kpt in range(Nk):\n",
    "    print(kpt, torch.linalg.norm(recons_kpt_target[kpt] - small_shifts_target[kpt]))\n",
    "# torch.linalg.norm(recons_kpt_target - small_shifts_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1b99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d02c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29674c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModelPeriodic(feat_plus, feat_minus, target_coupled_blocks_sum, target_coupled_blocks_diff, desired_shifts, frames, orbs, nhidden=16, nlayers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16023802757263184\n",
      "0.031491734087467194\n",
      "0.009251020848751068\n",
      "0.003096561646088958\n",
      "0.0009399798000231385\n",
      "0.0004689812776632607\n",
      "0.00024364719865843654\n",
      "0.00017960922559723258\n",
      "0.00014655283303000033\n",
      "0.00012858735863119364\n",
      "0.0001240893907379359\n",
      "0.00012232616427354515\n",
      "0.00012163409701315686\n",
      "0.00012134024291299284\n",
      "0.00012129032984375954\n",
      "0.00012125116336392239\n",
      "0.00012124008208047599\n",
      "0.00012123619671911001\n",
      "0.00012123472697567195\n",
      "0.0001212342904182151\n",
      "0.00012123407941544428\n",
      "0.00012123402848374099\n",
      "0.00012123401393182576\n",
      "0.00012123399937991053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb Cell 47\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m300\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     rsum, rdiff \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn_combined(rsum, rdiff, expkL_small, small_shifts_target)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32m/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb Cell 47\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     pred_diff_tmap \u001b[39m=\u001b[39m TensorMap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_blocks_diff[\u001b[39mstr\u001b[39m(s)]\u001b[39m.\u001b[39mkeys, pred_blocks_diff)   \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecon_sum[\u001b[39mstr\u001b[39m(s)] \u001b[39m=\u001b[39m _to_matrix(_to_uncoupled_basis(pred_sum_tmap), frames \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframes, orbitals\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morbitals)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecon_diff[\u001b[39mstr\u001b[39m(s)] \u001b[39m=\u001b[39m _to_matrix(_to_uncoupled_basis(pred_diff_tmap), frames \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframes, orbitals\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morbitals, hermitian\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecon_sum, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecon_diff\n",
      "File \u001b[0;32m/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/twocenter_utils.py:544\u001b[0m, in \u001b[0;36m_to_uncoupled_basis\u001b[0;34m(blocks, cg, device)\u001b[0m\n\u001b[1;32m    533\u001b[0m     new_block \u001b[39m=\u001b[39m block_builder\u001b[39m.\u001b[39madd_block(\n\u001b[1;32m    534\u001b[0m         keys\u001b[39m=\u001b[39mblock_idx,\n\u001b[1;32m    535\u001b[0m         properties\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39masarray([[\u001b[39m0\u001b[39m]], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32),\n\u001b[1;32m    536\u001b[0m         components\u001b[39m=\u001b[39m[_components_idx(li), _components_idx(lj)],\n\u001b[1;32m    537\u001b[0m     )\n\u001b[1;32m    538\u001b[0m     new_block\u001b[39m.\u001b[39madd_samples(\n\u001b[1;32m    539\u001b[0m         labels\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39masarray(block\u001b[39m.\u001b[39msamples\u001b[39m.\u001b[39mvalues)\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m    540\u001b[0m             block\u001b[39m.\u001b[39msamples\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    541\u001b[0m         ),\n\u001b[1;32m    542\u001b[0m         data\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mmoveaxis(decoupled, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m    543\u001b[0m     )\n\u001b[0;32m--> 544\u001b[0m \u001b[39mreturn\u001b[39;00m block_builder\u001b[39m.\u001b[39;49mbuild()\n",
      "File \u001b[0;32m/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/metatensor_utils.py:86\u001b[0m, in \u001b[0;36mTensorBuilder.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m     blocks\u001b[39m.\u001b[39mappend(block\u001b[39m.\u001b[39mbuild())\n\u001b[1;32m     85\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(block, TensorBuilderPerSamples):\n\u001b[0;32m---> 86\u001b[0m     blocks\u001b[39m.\u001b[39mappend(block\u001b[39m.\u001b[39;49mbuild())\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid block type\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/metatensor_utils.py:129\u001b[0m, in \u001b[0;36mTensorBuilderPerSamples.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    128\u001b[0m     samples \u001b[39m=\u001b[39m Labels(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_names, np\u001b[39m.\u001b[39mvstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_samples))\n\u001b[0;32m--> 129\u001b[0m     block \u001b[39m=\u001b[39m TensorBlock(\n\u001b[1;32m    130\u001b[0m         values\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mcat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m),\n\u001b[1;32m    131\u001b[0m         samples\u001b[39m=\u001b[39;49msamples,\n\u001b[1;32m    132\u001b[0m         components\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_components,\n\u001b[1;32m    133\u001b[0m         properties\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_properties,\n\u001b[1;32m    134\u001b[0m     )\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_samples \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m         \u001b[39mraise\u001b[39;00m (\u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGradient data not implemented for BlockBuilderSamples\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/metatensor/block.py:72\u001b[0m, in \u001b[0;36mTensorBlock.__init__\u001b[0;34m(self, values, samples, components, properties)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m i, component \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(components):\n\u001b[1;32m     70\u001b[0m     components_array[i] \u001b[39m=\u001b[39m component\u001b[39m.\u001b[39m_as_mts_labels_t()\n\u001b[0;32m---> 72\u001b[0m values \u001b[39m=\u001b[39m ArrayWrapper(values)\n\u001b[1;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actual_ptr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lib\u001b[39m.\u001b[39mmts_block(\n\u001b[1;32m     75\u001b[0m     values\u001b[39m.\u001b[39minto_mts_array(),\n\u001b[1;32m     76\u001b[0m     samples\u001b[39m.\u001b[39m_as_mts_labels_t(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     properties\u001b[39m.\u001b[39m_as_mts_labels_t(),\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     81\u001b[0m _check_pointer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actual_ptr)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/metatensor/data/array.py:78\u001b[0m, in \u001b[0;36mArrayWrapper.__init__\u001b[0;34m(self, array)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m# `mts_array_t::ptr` is a pointer to the PyObject `self`\u001b[39;00m\n\u001b[1;32m     73\u001b[0m mts_array\u001b[39m.\u001b[39mptr \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mcast(\n\u001b[1;32m     74\u001b[0m     ctypes\u001b[39m.\u001b[39mpointer(ctypes\u001b[39m.\u001b[39mpy_object(\u001b[39mself\u001b[39m)), ctypes\u001b[39m.\u001b[39mc_void_p\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m \u001b[39m@catch_exceptions\u001b[39;49m\n\u001b[0;32m---> 78\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39mmts_array_origin\u001b[39;49m(this, origin):\n\u001b[1;32m     79\u001b[0m     origin[\u001b[39m0\u001b[39;49m] \u001b[39m=\u001b[39;49m array_origin\n\u001b[1;32m     81\u001b[0m \u001b[39m# use storage.XXX.__class__ to get the right type for all functions\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/metatensor/utils.py:31\u001b[0m, in \u001b[0;36mcatch_exceptions\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcatch_exceptions\u001b[39m(function):\n\u001b[1;32m     30\u001b[0m     \u001b[39m@functools\u001b[39;49m\u001b[39m.\u001b[39;49mwraps(function)\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mdef\u001b[39;49;00m \u001b[39minner\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs):\n\u001b[1;32m     32\u001b[0m         \u001b[39mtry\u001b[39;49;00m:\n\u001b[1;32m     33\u001b[0m             function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/functools.py:50\u001b[0m, in \u001b[0;36mupdate_wrapper\u001b[0;34m(wrapper, wrapped, assigned, updated)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_wrapper\u001b[39m(wrapper,\n\u001b[1;32m     36\u001b[0m                    wrapped,\n\u001b[1;32m     37\u001b[0m                    assigned \u001b[39m=\u001b[39m WRAPPER_ASSIGNMENTS,\n\u001b[1;32m     38\u001b[0m                    updated \u001b[39m=\u001b[39m WRAPPER_UPDATES):\n\u001b[1;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Update a wrapper function to look like the wrapped function\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m       wrapper is the function to be updated\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m       function (defaults to functools.WRAPPER_UPDATES)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m assigned:\n\u001b[1;32m     51\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m             value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(wrapped, attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True)\n",
    "losses = []\n",
    "for i in range(300):\n",
    "    optimizer.zero_grad()\n",
    "    rsum, rdiff = model.forward()\n",
    "    loss = loss_fn_combined(rsum, rdiff, expkL_small, small_shifts_target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step(loss)\n",
    "    losses.append(loss.item())\n",
    "    if i%10 ==0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
