{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d9b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e5ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ase \n",
    "from ase.units import Bohr \n",
    "import torch\n",
    "import metatensor\n",
    "from metatensor import TensorMap, TensorBlock, Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nigam/miniconda3/lib/python3.11/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read \n",
    "from pyscf.pbc.tools import pyscf_ase \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c2cbc4",
   "metadata": {},
   "source": [
    "## Load and process pyscf data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of loading supercell $\\Gamma$ and unit cell k-point calc. Generate relative translations and identify translated matrices. \n",
    "One doesnt need this for the rest of the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.data.pyscf_calculator import get_scell_phase, check_translation_hermiticity, map_gammapoint_to_kpoint, map_gammapoint_to_relativetrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010222087626684012\n",
      "9.813485513377627e-05\n",
      "0.00010211799044343512\n"
     ]
    }
   ],
   "source": [
    "# load unit cell frame, create supercell and phase \n",
    "# filename = \"C2_174\"\n",
    "# framename = \"C2_174.extxyz\"\n",
    "\n",
    "filename = \"C2_rotated\"\n",
    "framename = \"C2_rotated.xyz\"\n",
    "\n",
    "frames = read('examples/data/periodic/c2/{}'.format(framename), ':')\n",
    "kmesh = [4, 4, 1]\n",
    "fock_Ls = []\n",
    "error = []\n",
    "for ifr, frame in enumerate(frames[:]):\n",
    "    cell, scell, phase = get_scell_phase(frame, kmesh)\n",
    "    NR, Nk = phase.shape\n",
    "    nao = cell.nao\n",
    "\n",
    "    fock = np.load('examples/data/periodic/c2/results_{}/supercell/supercell_fock_{}.npy'.format(filename, ifr)).reshape(NR, nao, NR, nao)\n",
    "    kkfock = np.load('examples/data/periodic/c2/results_{}/unitcell/unitcell_fock_{}.npy'.format(filename, ifr))\n",
    "\n",
    "    overlap = np.load('examples/data/periodic/c2/results_{}/supercell/supercell_over_{}.npy'.format(filename, ifr)).reshape(NR, nao, NR, nao)\n",
    "    kkoverlap = np.load('examples/data/periodic/c2/results_{}/unitcell/unitcell_over_{}.npy'.format(filename, ifr))\n",
    "\n",
    "    check_translation_hermiticity(fock, NR=NR)\n",
    "    # R_rel = translation_vectors_for_kmesh(cell, kmesh, wrap_around=False, return_rel=True)\n",
    "    fock_trans, weights, phasediff = map_gammapoint_to_relativetrans(fock, phase=phase, cell=cell, kmesh=kmesh)\n",
    "    yy = map_gammapoint_to_kpoint(fock, phase=phase, cell=cell, kmesh=kmesh)\n",
    "    print(np.linalg.norm(yy - kkfock))\n",
    "    error.append(np.linalg.norm(yy - kkfock))\n",
    "    fock_Ls.append(fock_trans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = read('C2_rotated.xyz', ':')\n",
    "frames = read('examples/data/periodic/c2/C2_174.extxyz', ':10')\n",
    "# filename = \"C2_rotated\"\n",
    "filename = \"C2_174\"\n",
    "\n",
    "for f in frames:\n",
    "    f.pbc = [True, True, True]\n",
    "\n",
    "kmesh = [4, 4, 1]\n",
    "\n",
    "translated_matrices = hickle.load('examples/data/periodic/c2/results_{}/fock_Ls.hkl'.format(filename)) # dict of translations, {T:H(T)}\n",
    "weights = hickle.load('examples/data/periodic/c2/results_{}/weights.hkl'.format(filename)) # multiplicity of each translation in the supercell gamma, {T:weights}\n",
    "kpts = hickle.load('examples/data/periodic/c2/results_{}/kpts.hkl'.format(filename)) # kpts used in pyscf calc \n",
    "expkL = hickle.load('examples/data/periodic/c2/results_{}/expkL.hkl'.format(filename)) # phase factors of the form {T:[e^{ikT} for T in translations]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7988d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix orbital order (pyscf treats all orbitals except L=1 in the order {m= -L, -L+1, ..., L-1, L}. For L it keeps the elements in the order [x,y,z])\n",
    "#This converts the L=1 to be in the {m=-1, 0, 1} order.\n",
    "from mlelec.utils.twocenter_utils import fix_orbital_order\n",
    "orbs = {6: [[1,0,0],[2,0,0],[2,1,1], [2,1,-1],[2,1,0]]}\n",
    "for T in translated_matrices.keys():\n",
    "    translated_matrices[T] = fix_orbital_order(translated_matrices[T], frames, orbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ls = np.asarray([list(k) for k in translated_matrices.keys()]) # extract the translations from the dictionary keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 49\n"
     ]
    }
   ],
   "source": [
    "# The Ls extracted above can be split into a set positive translations - each of which has a negative counterpart.There is an additional (0,0,0) shift. Here we extract the positive translations (and (0,0,0))).\n",
    "positive_Ls = []\n",
    "for L in Ls:\n",
    "    lL  = list(L)\n",
    "    lmL = list(-L)\n",
    "    if not (lL in positive_Ls):\n",
    "        if not (lmL in positive_Ls):\n",
    "            positive_Ls.append(lmL)\n",
    "print(len(positive_Ls), len(Ls)   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of dealing with all the positive translations above (which we would have to eventually do), for a benchmark, I want to work with a smaller set of translations. Here I selected randomly a subset of the positive translations.\n",
    "# Essentially, I want to see if the code works - and testing it here for a subset of translations. So we are creating a fake target\n",
    "desired_shifts = [\n",
    "    [0, 0, 0],\n",
    " [1, 0, 0],\n",
    " [0,1,0], \n",
    " [1, 1, 0],\n",
    " [1,-1,0],\n",
    " [2,0,0], \n",
    "[0,2,0], \n",
    "[2,1,0],\n",
    "[3,0,0],\n",
    "[0,3,0],\n",
    " ]\n",
    "\n",
    "# desired_shifts = Ls  # Eventually move to this \n",
    "\n",
    "# Add the negative translations of the subset we identified above to maintain the symmetry of the problem\n",
    "withnegative_shifts = desired_shifts.copy()\n",
    "for s in withnegative_shifts[:]:\n",
    "    withnegative_shifts.append([-s[0], -s[1], -s[2]])\n",
    "\n",
    "#collect the matrices for the set of translations identified above\n",
    "selected_matrices = {}\n",
    "for s in withnegative_shifts:\n",
    "    selected_matrices[tuple(s)] = translated_matrices[tuple(s)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian matrix -> blocks  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066eca22",
   "metadata": {},
   "source": [
    "Now that we have {T:H(T)} for the selected translations, we can proceed with the matrix --> uncoupled blocks --> coupled blocks using tensormaps. \n",
    "\n",
    "Below we first symmetrize over the positive and negative translations. H(T) = H(-T)^\\dagger. \n",
    "\n",
    "    for T in positive_shifts \n",
    "\n",
    "        sum[T] = H(T) + H(-T)  is symmetric matrix \n",
    "\n",
    "        diff[T] = H(T) - H(-T) is antisymmetric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.utils.twocenter_utils import _to_blocks, _to_matrix, _to_coupled_basis, _to_uncoupled_basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices_sum = {}\n",
    "matrices_diff = {} \n",
    "\n",
    "for s in desired_shifts[:]:\n",
    "    matrices_sum[tuple(s)] = 0.5*(selected_matrices[tuple(s)] + selected_matrices[tuple([-s[0], -s[1], -s[2]])])\n",
    "    matrices_diff[tuple(s)] = 0.5* (selected_matrices[tuple(s)] - selected_matrices[tuple([-s[0], -s[1], -s[2]])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm of Sum matrices\n",
      "54.20150788504924\n",
      "1.4703719767052015\n",
      "4.612980281608163\n",
      "4.675329803806235\n",
      "0.11776329648441179\n",
      "0.032072379274884905\n",
      "0.21727649231106863\n",
      "0.1207116869405447\n",
      "1.470371976779733\n",
      "4.6129802820905494\n",
      "norm of Diff matrices\n",
      "0.0\n",
      "1.5896869020477293\n",
      "4.665629333408246\n",
      "4.725158746807788\n",
      "0.11019695388278204\n",
      "1.1926756280806283e-09\n",
      "6.877182071136917e-10\n",
      "0.11331812258631987\n",
      "1.5896869020270719\n",
      "4.665629333712581\n"
     ]
    }
   ],
   "source": [
    "# Get an estimate of the norm of the matrices\n",
    "print('norm of Sum matrices')\n",
    "for m in matrices_sum.values():\n",
    "    print(np.linalg.norm(m))\n",
    "print('norm of Diff matrices')\n",
    "for m in matrices_diff.values():\n",
    "    print(np.linalg.norm(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57b135fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nigam/miniconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "target_blocks_sum = {} # for each translation accumulate the TMap of uncoupled blocks of the matrix_sum\n",
    "target_blocks_minus = {} # do the same for matrix_diff\n",
    "target_coupled_blocks_sum = {} # couples the uncoupled blocks of matrix_sum\n",
    "target_coupled_blocks_diff= {}\n",
    "for s in desired_shifts[:]:\n",
    "    target_blocks_sum[tuple(s)] = _to_blocks(matrices_sum[tuple(s)], frames=frames, orbitals=orbs) # matrix -> uncoupled\n",
    "    target_blocks_minus[tuple(s)] = _to_blocks(matrices_diff[tuple(s)], frames=frames, orbitals=orbs)\n",
    "    target_coupled_blocks_sum[tuple(s)] = _to_coupled_basis(_to_blocks(matrices_sum[tuple(s)], frames=frames, orbitals=orbs), orbs) # uncouple -> coupled\n",
    "    target_coupled_blocks_diff[tuple(s)] = _to_coupled_basis(_to_blocks(matrices_diff[tuple(s)], frames=frames, orbitals=orbs), orbs, skip_symmetry=True) # uncouple -> coupled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the small subset of translations we selected above, we create H(k)'s obtained by summing over these translations only (instead of the actual H(k)s which are obtained by summing over all translations). \n",
    "\n",
    "#When we move to learning all translation, this will be replaced by the actual H(k)s\n",
    "\n",
    "Nk = len(kpts)\n",
    "nao = 10\n",
    "small_shifts_target = []\n",
    "for ifr, frame in enumerate(frames): \n",
    "    kmatrix = torch.zeros((Nk, nao, nao)).type(torch.complex128)\n",
    "\n",
    "    # shift_indices = [np.where(np.all(Ls==np.asarray(s), axis=1))[0][0] for s in withnegative_shifts]\n",
    "    for key in withnegative_shifts:\n",
    "        key = tuple(key)\n",
    "        for kpt in range(Nk):\n",
    "            kmatrix[kpt] += translated_matrices[key][ifr] * weights[key] * expkL[key][kpt]\n",
    "    kmatrix = kmatrix / Nk\n",
    "    small_shifts_target.append(kmatrix) \n",
    "small_shifts_target = torch.stack(small_shifts_target).swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653f11c-56bc-45cf-b2d8-6f0a91d18861",
   "metadata": {},
   "source": [
    "## feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0c692a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rascaline import SphericalExpansionByPair as PairExpansion\n",
    "from rascaline import SphericalExpansion\n",
    "from mlelec.utils.metatensor_utils import labels_where\n",
    "from metatensor import Labels\n",
    "from mlelec.features.acdc import twocenter_hermitian_features, single_center_features, pair_features, twocenter_hermitian_features_periodic\n",
    "from mlelec.utils.twocenter_utils import map_targetkeys_to_featkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e299cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/MAC/my_mlelec/src/mlelec/features/acdc.py:114: UserWarning: Using cutoff 2.0 for all pairs feature\n",
      "  warnings.warn(\n",
      "/scratch/MAC/my_mlelec/src/mlelec/features/acdc.py:114: UserWarning: Using cutoff 9.0 for all pairs feature\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hyper = {'cutoff': 4.,\n",
    "          'max_radial':8, \n",
    "          'max_angular':4,\n",
    "          'atomic_gaussian_width':0.3,\n",
    "          'center_atom_weight':1,\n",
    "          \"radial_basis\": {\"Gto\": {}},\n",
    "          \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}},\n",
    "}\n",
    "#test_rcut_shift: \n",
    "def test_rcut(frame, hypers, shifts):\n",
    "    hypers_ij = hypers.copy()\n",
    "    r = hypers['cutoff']\n",
    "    cell = frame.cell.copy()\n",
    "    norms = np.linalg.norm(frame.cell, axis=1)\n",
    "    assert isinstance(shifts[0], tuple)\n",
    "    max_shift = tuple([np.max(shifts, axis=(0,1))]*3)\n",
    "    max_disp = np.sqrt(np.dot(max_shift, norms**2))+ frame.get_all_distances().max()**2\n",
    "    if r < max_disp:\n",
    "        hypers_ij['cutoff'] = max_disp    \n",
    "    \n",
    "    return hypers_ij\n",
    "\n",
    "# hypers = test_rcut(frames[100], hyper, [(1,1,1)])\n",
    "hypers = hyper\n",
    "gij = PairExpansion(**hypers)\n",
    "pair = gij.compute(frames)\n",
    "\n",
    "# compute features that will be used for the zero shift\n",
    "single = single_center_features(frames, hypers, 2, lcut=3)#, pca_final = True, npca = 0.9999, slice_samples = 4)\n",
    "pair_zero_shift = pair_features(frames, hypers, order_nu=1,  all_pairs=True, lcut=3, max_shift=[1,1,1], feature_names = single.property_names)\n",
    "\n",
    "# compute features that will be used for the other shifts\n",
    "pair = pair_features(frames, hypers, order_nu=1,  all_pairs=True, lcut=3, max_shift=[3,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cee9553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensormap beautification - adding relevant sample labels so the features for the zero transltion match the format of the other translations. \n",
    "# This and the next cell will be hidden from the user in the future.\n",
    "\n",
    "pair_feat = pair_zero_shift\n",
    "blocks = []\n",
    "for i, (key, block) in enumerate(pair_feat.items()):\n",
    "    samples_lab, idx = labels_where(block.samples, selection=Labels([\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\"], values=np.asarray([[0,0,0]]).reshape(-1, 3)), return_idx=True)\n",
    "    # print(samples_lab.names)\n",
    "    samples_lab =  Labels(samples_lab.names[:3], values=samples_lab.values[:,:3])\n",
    "    tb = TensorBlock(\n",
    "        samples=samples_lab,\n",
    "        values=block.values[idx],\n",
    "        properties=block.properties,\n",
    "        components = block.components\n",
    "    )\n",
    "    blocks.append(tb)\n",
    "pair_zero_shift = TensorMap(keys = pair_feat.keys, blocks=blocks)\n",
    "\n",
    "zeroshift = twocenter_hermitian_features_periodic(single, pair_zero_shift, shift=[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the features for +T and -T, we create sum and differences just like we did for the Hamiltonians. Note this is working on g_ij \n",
    "\n",
    "shifts = list(set(zip(pair[0].samples[\"cell_shift_a\"], pair[0].samples[\"cell_shift_b\"], pair[0].samples[\"cell_shift_c\"])))\n",
    "\n",
    "pair_sum = {str(x):[] for x in desired_shifts}\n",
    "pair_diff = {str(x):[] for x in desired_shifts}\n",
    "blocks_plus = [] # accumulate the blocks for feat[T] + feat[-T]\n",
    "blocks_minus = []\n",
    "\n",
    "#treat zero shift separately - we just treat the zero shift samples as blocks_plus in feat_plus later\n",
    "\n",
    "for i, (k,b) in enumerate(pair.items()):\n",
    "    for shift in desired_shifts[1:]: # skip zero shift. CHANGEe this if zeroshift is not the first element in desired_shifts\n",
    "        minus_shift = tuple(-1*np.array(shift))\n",
    "        slab, plusidx = labels_where(b.samples, selection=Labels(names=[\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\"], values=np.array(shift).reshape(1,-1)), return_idx=True)\n",
    "        slabm, minusidx = labels_where(b.samples, selection=Labels(names=[\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\"], values=np.array(minus_shift).reshape(1,-1)), return_idx=True)\n",
    "\n",
    "        #match samples [\"structure\", \"center\", \"neighbor\"] in blocks_plus and blocks_minus\n",
    "        sidx = []\n",
    "        try:\n",
    "            for i, [structure, center, neighbor] in enumerate(slab.values[:,:3]):   \n",
    "                sidx.append(np.where(np.all(slabm.values[:,:3] == [structure, center, neighbor], axis=1))[0][0])\n",
    "            \n",
    "        except:\n",
    "            print(k, \"skipped\", slab.values.shape, slabm.values.shape)\n",
    "\n",
    "        if len(sidx) != slab.values.shape[0]:\n",
    "            print(k, \"PROBLEM\", slabm.values, slabm.values)\n",
    "        assert np.allclose(slab.values[:,:3], slabm.values[sidx][:,:3]), (k,slab.values[np.where(slab.values - slabm.values[sidx])[0]][:,:3], slabm.values[sidx][np.where(slab.values - slabm.values)[0]][:,:3]) # so its the same samples under consideration\n",
    "        pvalues = b.values[plusidx] + b.values[minusidx]\n",
    "        mvalues = b.values[plusidx] - b.values[minusidx]\n",
    "        \n",
    "        blocks_plus.append(TensorBlock(values = pvalues,\n",
    "                                   components = b.components,\n",
    "                             samples = Labels(names = pair.sample_names[:-3], values=np.asarray(b.samples.values[plusidx])[:,:-3]),\n",
    "                                   properties = b.properties)\n",
    "                            )\n",
    "        \n",
    "        blocks_minus.append(TensorBlock(values = mvalues,\n",
    "                                   components = b.components,\n",
    "                             samples = Labels(names = pair.sample_names[:-3], values=np.asarray(b.samples.values[plusidx])[:,:-3]),\n",
    "                                   properties = b.properties)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 63\n",
      "Must equal the product of the two values below\n",
      "7 9\n"
     ]
    }
   ],
   "source": [
    "print(len(blocks_plus), len(blocks_minus))\n",
    "print(\"Must equal the product of the two values below\")\n",
    "print(len(pair.keys.values), len(desired_shifts[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "shift_trans = list(itertools.product(pair.keys.values.tolist(), desired_shifts[1:]))\n",
    "shift_trans = [(list(itertools.chain.from_iterable(_))) for _ in shift_trans]\n",
    "shift_trans_names = pair.keys.names  + pair.sample_names[-3:]\n",
    "shift_trans = Labels(shift_trans_names, np.array(shift_trans))\n",
    "pair_plus = TensorMap(shift_trans, blocks_plus)\n",
    "pair_minus = TensorMap(shift_trans, blocks_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally compute tensor product of rho_i with the sum and diff of pair feat g_ij obtained above\n",
    "feat_plus=twocenter_hermitian_features_periodic(single, pair_plus) \n",
    "feat_minus=twocenter_hermitian_features_periodic(single, pair_minus, antisymmetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59e8de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 9)\n",
      "(210, 9) 210\n"
     ]
    }
   ],
   "source": [
    "# We skipped over the zeroshift in the loop of computing sum and diff features. We add back the zeroshift features after \"beautification\" back to the positive shift \n",
    "blocks = []\n",
    "for i, (key, block) in enumerate(zeroshift.items()):\n",
    "    property_names = feat_plus.property_names # feat_plus.property_names\n",
    "    properties = Labels(property_names, values=block.properties.values)\n",
    "    tb = TensorBlock(\n",
    "        samples= block.samples, #Labels(sample_names, values=samp),\n",
    "        values=block.values,\n",
    "        properties=properties,\n",
    "        components = block.components\n",
    "    )\n",
    "    blocks.append(tb)\n",
    "\n",
    "key_names = zeroshift.keys.names[:-1] +[\"cell_shift_a\", \"cell_shift_b\", \"cell_shift_c\", \"block_type\"]\n",
    "keyvals = np.pad(zeroshift.keys.values, ((0,0),(0,3)))\n",
    "\n",
    "keyvals[:, [-4, -1]] = keyvals[:, [-1, -4]]\n",
    "# keyvals = np.swapaxes(keyvals, -4,-1) # block typ to the last axis\n",
    "keys = Labels(key_names, keyvals) \n",
    "zeroshift = TensorMap(keys = keys, blocks=blocks)\n",
    "\n",
    "block_plus_wzero = []\n",
    "for k, b in zeroshift.items():\n",
    "    block_plus_wzero.append(b.copy())\n",
    "for k, b in feat_plus.items():\n",
    "    block_plus_wzero.append(b.copy())\n",
    "keyvals = np.concatenate((zeroshift.keys.values, feat_plus.keys.values))\n",
    "\n",
    "keys = Labels(zeroshift.keys.names, keyvals )\n",
    "feat_plus  = TensorMap(keys, block_plus_wzero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.models.linear import MLP\n",
    "from mlelec.data.dataset import MLDataset, MoleculeDataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is changing at the moment. But for now, this is creating a model that spaws submodels for each translation. Each submodel is a MLP that takes as input the features for the translation and outputs the corresponding H(T).\n",
    "\n",
    "import torch.nn as nn\n",
    "class LinearModelPeriodic(nn.Module):\n",
    "    def __init__(self, feat_plus, feat_minus, target_blocks_sum, target_blocks_diff, cell_shifts, frames, orbitals, device=None, **kwargs):\n",
    "        \"\"\" \n",
    "        feat_plus: TensorMap of features feat(T)+feat(-T)\n",
    "        feat_minus: TensorMap of features feat(T)-feat(-T)\n",
    "        target_blocks_sum: TensorMap of **coupled** blocks of H(T)+H(-T)\n",
    "        target_blocks_diff: TensorMap of **coupled** blocks of H(T)-H(-T)\n",
    "        cell_shifts: list of shifts, i.e. keys of target_blocks_sum and target_blocks_diff. So this is a redundant argument.\n",
    "        frames: list of frames\n",
    "        orbitals: dict of basis set per species \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.feat_plus = feat_plus\n",
    "        self.feat_minus = feat_minus\n",
    "        self.target_blocks_sum = target_blocks_sum\n",
    "        self.target_blocks_diff = target_blocks_diff\n",
    "        self.cell_shifts = cell_shifts #Doesnt belong here #TODO extract this better \n",
    "        self.frames = frames\n",
    "        self.orbitals = orbitals\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dummy_property = next(iter(self.target_blocks_sum.values()))[0].properties\n",
    "        self._submodels(**kwargs)\n",
    "\n",
    "    def _submodels(self, **kwargs):\n",
    "        # create submodel for each translation\n",
    "        self.blockmodels = {}\n",
    "        for s in self.cell_shifts: \n",
    "            shiftmodels ={}\n",
    "            for k in self.target_blocks_sum[tuple(s)].keys:\n",
    "                blockval = torch.linalg.norm(self.target_blocks_sum[tuple(s)][k].values)\n",
    "                if blockval > 1e-5:\n",
    "                    \n",
    "                    feat = map_targetkeys_to_featkeys(self.feat_plus, k, cell_shift=s) # identify the features for the block of the matrix\n",
    "                    shiftmodels[str(tuple(k)+(1,))] = MLP(nin=feat.values.shape[-1], nout=1, nhidden=kwargs.get(\"nhidden\",10), nlayers=kwargs.get(\"nlayers\",2))\n",
    "            if self.target_blocks_diff is not None:\n",
    "                for k in self.target_blocks_diff[tuple(s)].keys:\n",
    "                    blockval = torch.linalg.norm(self.target_blocks_diff[tuple(s)][k].values)\n",
    "                    if blockval > 1e-5:\n",
    "                        feat = map_targetkeys_to_featkeys(self.feat_minus, k, cell_shift=s)\n",
    "                        shiftmodels[str(tuple(k)+(-1,))] = MLP(nin=feat.values.shape[-1], nout=1, nhidden=kwargs.get(\"nhidden\",10), nlayers=kwargs.get(\"nlayers\",2))\n",
    "                    \n",
    "            self.blockmodels[str(s)] = torch.nn.ModuleDict(shiftmodels)\n",
    "        self.model = torch.nn.ModuleDict(self.blockmodels)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def forward(self):\n",
    "        self.recon_sum = {}\n",
    "        if self.target_blocks_diff is not None:\n",
    "            self.recon_diff = {} \n",
    "        else: \n",
    "            self.recon_diff = None\n",
    "        for s in self.cell_shifts: \n",
    "            pred_blocks_sum = []\n",
    "            pred_blocks_diff =[]\n",
    "            for k in self.target_blocks_sum[tuple(s)].keys:\n",
    "                blockval = torch.linalg.norm(self.target_blocks_sum[tuple(s)][k].values)\n",
    "                if blockval > 1e-5:\n",
    "                    feat = map_targetkeys_to_featkeys(self.feat_plus, k, cell_shift=s)\n",
    "                    nsamples, ncomp, nprops = feat.values.shape\n",
    "                    pred = self.blockmodels[str(s)][str(tuple(k)+(1,))](feat.values) # call forward of the submodel for the translation\n",
    "                    pred_blocks_sum.append( TensorBlock(\n",
    "                    values=pred.reshape((nsamples, ncomp, 1)),\n",
    "                    samples=self.target_blocks_sum[tuple(s)][k].samples,\n",
    "                    components=self.target_blocks_sum[tuple(s)][k].components,\n",
    "                    properties=self.dummy_property,\n",
    "                    ))\n",
    "                else: \n",
    "                    pred_blocks_sum.append(self.target_blocks_sum[tuple(s)][k].copy())\n",
    "            pred_sum_tmap = TensorMap(self.target_blocks_sum[tuple(s)].keys, pred_blocks_sum)\n",
    "            self.recon_sum[tuple(s)] = _to_matrix(_to_uncoupled_basis(pred_sum_tmap), frames = self.frames, orbitals=self.orbitals)\n",
    "            if self.target_blocks_diff is not None:\n",
    "                for k in self.target_blocks_diff[tuple(s)].keys:\n",
    "                    blockval = torch.linalg.norm(self.target_blocks_diff[tuple(s)][k].values)\n",
    "                    if blockval > 1e-5:\n",
    "                        feat = map_targetkeys_to_featkeys(self.feat_minus, k, cell_shift=s)\n",
    "                        nsamples, ncomp, nprops = feat.values.shape\n",
    "                        pred = self.blockmodels[str(s)][str(tuple(k)+(-1,))](feat.values)\n",
    "                        pred_blocks_diff.append( TensorBlock(\n",
    "                        values=pred.reshape((nsamples, ncomp, 1)),\n",
    "                        samples=self.target_blocks_diff[tuple(s)][k].samples, \n",
    "                        components=self.target_blocks_diff[tuple(s)][k].components, \n",
    "                        properties=self.dummy_property,\n",
    "                        ))\n",
    "\n",
    "                    else:\n",
    "                        pred_blocks_diff.append(self.target_blocks_diff[tuple(s)][k].copy())\n",
    "            \n",
    "                pred_diff_tmap = TensorMap(self.target_blocks_diff[tuple(s)].keys, pred_blocks_diff)   \n",
    "            \n",
    "\n",
    "                self.recon_diff[tuple(s)] = _to_matrix(_to_uncoupled_basis(pred_diff_tmap), frames = self.frames, orbitals=self.orbitals, hermitian=False)\n",
    "        \n",
    "        return self.recon_sum, self.recon_diff\n",
    "    \n",
    "    # not currently in use - but could be used to fit an anlytical model to use as starting weights for gradient descent\n",
    "    \n",
    "    # def fit_ridge_analytical(self) -> None:\n",
    "    #     for shiftkey, model in self.models.item():\n",
    "    #         for i, (k, submodel) in model.items():\n",
    "    #             # feat = \n",
    "    #             feat_block = self.get_feature_block(\n",
    "    #                 features=features, key=key, core_features=core_features\n",
    "    #             )\n",
    "    #             targ_block = targets[key]\n",
    "\n",
    "    #             x = np.array(feat_block.values.reshape(-1, feat_block.values.shape[2]))\n",
    "    #             y = np.array(targ_block.values.reshape(-1, 1))\n",
    "\n",
    "    #             ridge = Ridge(alpha=self.alpha, fit_intercept=self.bias).fit(x, y)\n",
    "\n",
    "    #             model.layer.weight = torch.nn.Parameter(\n",
    "    #                 torch.from_numpy(ridge.coef_.copy().astype(np.float64))\n",
    "    #             )\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2fbd48",
   "metadata": {},
   "source": [
    "### diff loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "def loss_zero_shift(pred, target, device=None):\n",
    "    if device is None: \n",
    "        device = next(iter(pred.values())).device\n",
    "    assert pred.keys() == target.keys()\n",
    "    loss = 0\n",
    "    \n",
    "    loss += torch.sum((pred[tuple(s)]-target[tuple(s)])**2)\n",
    "    return loss\n",
    "\n",
    "def loss_fn_indiv_shift(rsum, rdiff, matrix_plust, matrix_minust, specific_shift_idx:Union[str, List]=None, device=None):\n",
    "    #compute loss for each shift separately, i.e. L = sum_T ||H(T) - \\tilde{H}(T)||^2 where H is the target and \\tilde{H} is the prediction. \n",
    "    # specific shift index specifies if this sum must be restricted over a particular subset of shifts. might be useful when combining with a loss over H(k).\n",
    "\n",
    "    #TODO: loss over particular shifts\n",
    "    if device is None: \n",
    "        device = next(iter(rsum.values())).device\n",
    "\n",
    "    weight_minus = 1\n",
    "    weight_plus = 1\n",
    "    if matrix_minust is not None:\n",
    "        # assert rsum.keys() == matrix_minust.keys()\n",
    "        if not isinstance(next(iter(matrix_minust.values())), torch.Tensor):\n",
    "            matrix_minust = {k:torch.from_numpy(v).type(torch.float).to(device) for k,v in matrix_minust.items()}\n",
    "    if not isinstance(next(iter(matrix_plust.values())), torch.Tensor):\n",
    "        matrix_plust = {k:torch.from_numpy(v).type(torch.float).to(device) for k,v in matrix_plust.items()}\n",
    "    loss = 0\n",
    "    if isinstance(specific_shift_idx, list):\n",
    "        raise NotImplementedError\n",
    "    elif isinstance(specific_shift_idx, str):\n",
    "        if specific_shift_idx == \"positive\":\n",
    "            weight_minus=0\n",
    "        elif specific_shift_idx == \"negative\":\n",
    "            weight_plus=0\n",
    "           \n",
    "    for s in rsum.keys():\n",
    "        if rdiff is not None:\n",
    "            plust = rsum[tuple(s)] + rdiff[tuple(s)]\n",
    "            minust = rsum[tuple(s)] - rdiff[tuple(s)]\n",
    "        else: \n",
    "            plust = rsum[tuple(s)]\n",
    "            minust = rsum[tuple(s)]\n",
    "        loss += weight_plus*torch.sum((plust-matrix_plust[tuple(s)])**2) + weight_minus*torch.sum((minust-matrix_minust[tuple(s)])**2)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss_fn_combined(rsum, rdiff, expkL:dict, complex_target, device = None):\n",
    "    # computes the prediction of H(k) from the sum and diff of \\tilde{H}(T) and computes the loss between \\tilde{H}(k) and the target H(k)\n",
    "    # \\tilde{H}(k) = \\sum_T \\tilde{H}(T) e^{ikT}\n",
    "    # L = \\sum_k ||H(k) - \\tilde{H}(k)||^2\n",
    "\n",
    "    #TODO : support the sum in the loss over specific k points \n",
    "    if device is None: \n",
    "        device = next(iter(rsum.values())).device\n",
    "        complex_target = complex_target.to(device)\n",
    "    assert rsum.keys() == rdiff.keys()\n",
    "    matrix = {}\n",
    "    for s in rsum.keys():\n",
    "        matrix[tuple(s)] = rsum[tuple(s)] + rdiff[tuple(s)]\n",
    "        matrix[tuple(-np.array(s))] = rsum[s] - rdiff[s]\n",
    "\n",
    "    recon_target = torch.zeros_like(complex_target, requires_grad=True, dtype = torch.complex64, device = device)\n",
    "    for s in matrix.keys():\n",
    "         recon_target = recon_target+ matrix[s]*expkL[s]\n",
    "    \n",
    "    loss = torch.tensordot((recon_target-complex_target),torch.conj(recon_target-complex_target)) \n",
    "    # equivalent to torch.linalg.norm((recon_target-complex_target))**2\n",
    "    assert torch.isclose(abs(loss), abs(loss.real))\n",
    "    return loss.real\n",
    "\n",
    "def get_predicted_matrices(rsum, rdiff, device=None):\n",
    "    if device is None: \n",
    "        device = next(iter(rsum.values())).device\n",
    "    assert rsum.keys() == rdiff.keys(), \"rsum and rdiff must have same keys\"\n",
    "    matrix = {}\n",
    "    for s in rsum.keys():\n",
    "        # sint = [int(x) for x in s[1:-1].split(\", \")]\n",
    "        matrix[s] = rsum[s] + rdiff[s]\n",
    "        matrix[tuple(-np.array(s))] = rsum[s] - rdiff[s]\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "93921e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModelPeriodic( zeroshift, None, target_coupled_blocks_sum, None, desired_shifts[:1], frames, orbs, nhidden=16, nlayers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f79c4355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0003953083069063723\n",
      "10 0.0003858382988255471\n",
      "20 0.00037969197728671134\n",
      "30 0.0003737412625923753\n",
      "40 0.0003678331268019974\n",
      "50 0.00036227484815753996\n",
      "60 0.00035727518843486905\n",
      "70 0.00043549900874495506\n",
      "80 0.0004191058105789125\n",
      "90 0.00035224761813879013\n",
      "100 0.000355529657099396\n",
      "110 0.00033384672133252025\n",
      "120 0.00032796338200569153\n",
      "130 0.00032371701672673225\n",
      "140 0.0004698431584984064\n",
      "150 0.0005213679978623986\n",
      "160 0.0005985862808302045\n",
      "170 0.0003761534171644598\n",
      "180 0.00031872763065621257\n",
      "190 0.0003236045013181865\n",
      "200 0.00030117708956822753\n",
      "210 0.0002947057946585119\n",
      "220 0.0002884854329749942\n",
      "230 0.0002843063557520509\n",
      "240 0.00028082949575036764\n",
      "250 0.00027749736909754574\n",
      "260 0.00027424393920227885\n",
      "270 0.0002711306151468307\n",
      "280 0.00026802613865584135\n",
      "290 0.0002666277578100562\n",
      "300 0.0005027313600294292\n",
      "310 0.00038971786852926016\n",
      "320 0.0006213482702150941\n",
      "330 0.0003061413299292326\n",
      "340 0.00026638031704351306\n",
      "350 0.00026559620164334774\n",
      "360 0.00024645542725920677\n",
      "370 0.0002459342358633876\n",
      "380 0.00024139534798450768\n",
      "390 0.00023896756465546787\n",
      "400 0.00023667345521971583\n",
      "410 0.0002342165680602193\n",
      "420 0.00023203162709251046\n",
      "430 0.00022986266412772238\n",
      "440 0.00022773865202907473\n",
      "450 0.00022566693951375782\n",
      "460 0.00022359687136486173\n",
      "470 0.0002215670538134873\n",
      "480 0.0002195902052335441\n",
      "490 0.0002177109126932919\n",
      "500 0.00022491259733214974\n",
      "510 0.00023357852478511631\n",
      "520 0.0009754170896485448\n",
      "530 0.0017329475376755\n",
      "540 0.0006730314344167709\n",
      "550 0.0003432349185459316\n",
      "560 0.0021311030723154545\n",
      "570 0.0008002562681213021\n",
      "580 0.0003718746593222022\n",
      "590 0.0002507234166841954\n",
      "600 0.0002104315790347755\n",
      "610 0.0001986203424166888\n",
      "620 0.0001961856905836612\n",
      "630 0.00019514255109243095\n",
      "640 0.00019317399710416794\n",
      "650 0.00019122418598271906\n",
      "660 0.00018992790137417614\n",
      "670 0.00019170422456227243\n",
      "680 0.0005341029609553516\n",
      "690 0.0002895246143452823\n",
      "700 0.0005964555311948061\n",
      "710 0.0018015834502875805\n",
      "720 0.0002084985317196697\n",
      "730 0.00039266396197490394\n",
      "740 0.00025116850156337023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[287], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1300\u001b[39m):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# loss = closure()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# optimizer.step(closure)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m     rsum, rdiff \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# print(rsum.keys())\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m )\n",
      "Cell \u001b[0;32mIn[280], line 48\u001b[0m, in \u001b[0;36mLinearModelPeriodic.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m pred_blocks_diff \u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_blocks_sum[\u001b[38;5;28mtuple\u001b[39m(s)]\u001b[38;5;241m.\u001b[39mkeys:\n\u001b[0;32m---> 48\u001b[0m     blockval \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_blocks_sum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m blockval \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-5\u001b[39m:\n\u001b[1;32m     50\u001b[0m         feat \u001b[38;5;241m=\u001b[39m map_targetkeys_to_featkeys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_plus, k, cell_shift\u001b[38;5;241m=\u001b[39ms)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/tensor.py:151\u001b[0m, in \u001b[0;36mTensorMap.__getitem__\u001b[0;34m(self, selection)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, selection) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorBlock:\n\u001b[1;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This is equivalent to self.block(selection)\"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselection\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/tensor.py:335\u001b[0m, in \u001b[0;36mTensorMap.block\u001b[0;34m(self, selection, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_by_id(selection)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     selection \u001b[38;5;241m=\u001b[39m \u001b[43m_normalize_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m matching \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks_matching(selection)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matching) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/tensor.py:618\u001b[0m, in \u001b[0;36m_normalize_selection\u001b[0;34m(selection)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m selection\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(selection, LabelsEntry):\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLabels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid type for block selection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(selection)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/labels.py:300\u001b[0m, in \u001b[0;36mLabels.__init__\u001b[0;34m(self, names, values)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels values must be convertible to integers\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib \u001b[38;5;241m=\u001b[39m _get_library()\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labels \u001b[38;5;241m=\u001b[39m \u001b[43m_create_new_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lib\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_names \u001b[38;5;241m=\u001b[39m names\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values \u001b[38;5;241m=\u001b[39m LabelsValues(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/labels.py:997\u001b[0m, in \u001b[0;36m_create_new_labels\u001b[0;34m(lib, names, values)\u001b[0m\n\u001b[1;32m    995\u001b[0m labels\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mctypes\u001b[38;5;241m.\u001b[39mdata_as(ctypes\u001b[38;5;241m.\u001b[39mPOINTER(ctypes\u001b[38;5;241m.\u001b[39mc_int32))\n\u001b[1;32m    996\u001b[0m labels\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 997\u001b[0m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmts_labels_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labels\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/metatensor/status.py:28\u001b[0m, in \u001b[0;36m_check_status\u001b[0;34m(status)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m LAST_EXCEPTION\n\u001b[1;32m     25\u001b[0m     LAST_EXCEPTION \u001b[38;5;241m=\u001b[39m e\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_status\u001b[39m(status):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m MTS_SUCCESS:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# # optimizer= torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# # optimizer = torch.optim.LBFGS(model.parameters(), lr=1e-4)\n",
    "# # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True)\n",
    "# def closure():\n",
    "#     optimizer.zero_grad()\n",
    "#     rsum, rdiff = model.forward()\n",
    "#     loss = loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx = 'positive' )\n",
    "#     loss.backward()\n",
    "#     return loss\n",
    "\n",
    "# losses = []\n",
    "for i in range(1300):\n",
    "    # loss = closure()\n",
    "    # optimizer.step(closure)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    rsum, rdiff = model.forward()\n",
    "    # print(rsum.keys())\n",
    "    loss = loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx = 'positive' )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step(loss)\n",
    "    losses.append(loss.item())\n",
    "    if i%10 ==0:\n",
    "        print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "be063a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsum_0 = model.forward()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModelPeriodic(feat_plus, feat_minus, target_coupled_blocks_sum, target_coupled_blocks_diff, desired_shifts[1:], frames, orbs, nhidden=16, nlayers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm the matrices before hand? but then how do we recover the actualm matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 68.53742980957031\n",
      "10 48.39392852783203\n",
      "20 28.66879653930664\n",
      "30 15.584948539733887\n",
      "40 8.948714256286621\n",
      "50 4.5619635581970215\n",
      "60 2.0557236671447754\n",
      "70 1.0691601037979126\n",
      "80 0.46487534046173096\n",
      "90 0.2473144829273224\n",
      "100 0.1785847246646881\n",
      "110 0.1247759610414505\n",
      "120 0.10865583270788193\n",
      "130 0.10135360062122345\n",
      "140 0.09514949470758438\n",
      "150 0.0898614302277565\n",
      "160 0.08484591543674469\n",
      "170 0.08012333512306213\n",
      "180 0.07572506368160248\n",
      "190 0.07157675176858902\n",
      "200 0.06765581667423248\n",
      "210 0.06394819170236588\n",
      "220 0.06044171005487442\n",
      "230 0.057127147912979126\n",
      "240 0.05399477109313011\n",
      "250 0.051035840064287186\n",
      "260 0.048241935670375824\n",
      "270 0.045605190098285675\n",
      "280 0.043117962777614594\n",
      "290 0.040773071348667145\n",
      "300 0.03856339305639267\n",
      "310 0.036482252180576324\n",
      "320 0.03452315926551819\n",
      "330 0.03267982602119446\n",
      "340 0.030946258455514908\n",
      "350 0.02931666374206543\n",
      "360 0.02778547629714012\n",
      "370 0.026347298175096512\n",
      "380 0.02499706670641899\n",
      "390 0.023729851469397545\n",
      "400 0.02254086546599865\n",
      "410 0.021425606682896614\n",
      "420 0.020379744470119476\n",
      "430 0.019399166107177734\n",
      "440 0.01847992092370987\n",
      "450 0.017618214711546898\n",
      "460 0.01681048423051834\n",
      "470 0.016053317114710808\n",
      "480 0.015343518927693367\n",
      "490 0.014677969738841057\n",
      "500 0.014053834602236748\n",
      "510 0.01346835121512413\n",
      "520 0.012918924912810326\n",
      "530 0.01240321435034275\n",
      "540 0.01191888190805912\n",
      "550 0.011463815346360207\n",
      "560 0.011035989038646221\n",
      "570 0.01063358224928379\n",
      "580 0.010254831984639168\n",
      "590 0.00989808514714241\n",
      "600 0.009561881422996521\n",
      "610 0.009244757704436779\n",
      "620 0.008945439010858536\n",
      "630 0.00866268202662468\n",
      "640 0.008395353332161903\n",
      "650 0.00814241822808981\n",
      "660 0.007902885787189007\n",
      "670 0.0076758358627557755\n",
      "680 0.007460453547537327\n",
      "690 0.007255963049829006\n",
      "700 0.0070615503937006\n",
      "710 0.0068766591139137745\n",
      "720 0.006700590252876282\n",
      "730 0.006532787811011076\n",
      "740 0.006372721400111914\n",
      "750 0.006219890434294939\n",
      "760 0.006073818542063236\n",
      "770 0.005934093613177538\n",
      "780 0.005800291895866394\n",
      "790 0.005672072991728783\n",
      "800 0.005549068562686443\n",
      "810 0.005430973134934902\n",
      "820 0.005317480303347111\n",
      "830 0.005208312068134546\n",
      "840 0.0051032304763793945\n",
      "850 0.0050019770860672\n",
      "860 0.004904340952634811\n",
      "870 0.00481010414659977\n",
      "880 0.004719087854027748\n",
      "890 0.004631112329661846\n",
      "900 0.004546006675809622\n",
      "910 0.004463993012905121\n",
      "920 0.004383852705359459\n",
      "930 0.004306452348828316\n",
      "940 0.004231339320540428\n",
      "950 0.004158458672463894\n",
      "960 0.004087665118277073\n",
      "970 0.004018864128738642\n",
      "980 0.003951946273446083\n",
      "990 0.0038868349511176348\n",
      "1000 0.0038234347011893988\n",
      "1010 0.0037616833578795195\n",
      "1020 0.0037015473935753107\n",
      "1030 0.0036428351886570454\n",
      "1040 0.0035855770111083984\n",
      "1050 0.0035297214053571224\n",
      "1060 0.003475168952718377\n",
      "1070 0.003421899862587452\n",
      "1080 0.0033698678016662598\n",
      "1090 0.003319007810205221\n",
      "1100 0.0032692605163902044\n",
      "1110 0.003220628947019577\n",
      "1120 0.003173047211021185\n",
      "1130 0.0031264799181371927\n",
      "1140 0.0030809082090854645\n",
      "1150 0.0030362873803824186\n",
      "1160 0.002992618130519986\n",
      "1170 0.0029497873038053513\n",
      "1180 0.0029078416991978884\n",
      "1190 0.0028667496517300606\n",
      "1200 0.002826712792739272\n",
      "1210 0.002786993980407715\n",
      "1220 0.0027482761070132256\n",
      "1230 0.0027103200554847717\n",
      "1240 0.0026731458492577076\n",
      "1250 0.0026366179808974266\n",
      "1260 0.0026007688138633966\n",
      "1270 0.002565619070082903\n",
      "1280 0.002531124744564295\n",
      "1290 0.0024972837418317795\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# optimizer= torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.LBFGS(model.parameters(), lr=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True)\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    rsum, rdiff = model.forward()\n",
    "    loss = loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx = 'positive' )\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "losses = []\n",
    "for i in range(1300):\n",
    "    # loss = closure()\n",
    "    # optimizer.step(closure)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    rsum, rdiff = model.forward()\n",
    "    loss = loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx = 'positive' )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step(loss)\n",
    "    losses.append(loss.item())\n",
    "    if i%10 ==0:\n",
    "        print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "f16b4ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0] tensor(0.0114, dtype=torch.float64)\n",
      "[0, 1, 0] tensor(0.0206, dtype=torch.float64)\n",
      "[1, 1, 0] tensor(0.0149, dtype=torch.float64)\n",
      "[1, -1, 0] tensor(0.0003, dtype=torch.float64)\n",
      "[2, 0, 0] tensor(0.0002, dtype=torch.float64)\n",
      "[0, 2, 0] tensor(0.0008, dtype=torch.float64)\n",
      "[2, 1, 0] tensor(0.0003, dtype=torch.float64)\n",
      "[3, 0, 0] tensor(0.0211, dtype=torch.float64)\n",
      "[0, 3, 0] tensor(0.0353, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "rsum_final_plus, rdiff_final_plus = model.forward()\n",
    "PLUS={}\n",
    "for s in desired_shifts[1:]:\n",
    "    PLUS[tuple(s)] = rsum_final_plus[tuple(s)] - rdiff_final_plus[tuple(s)]\n",
    "    print(s, torch.linalg.norm(PLUS[tuple(s)].detach().cpu() - matrices_sum[tuple(s)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModelPeriodic(feat_plus, feat_minus, target_coupled_blocks_sum, target_coupled_blocks_diff, desired_shifts[1:], frames, orbs, nhidden=16, nlayers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038311250507831573\n",
      "0.036352187395095825\n",
      "0.03450614586472511\n",
      "0.032767415046691895\n",
      "0.031130407005548477\n",
      "0.029589761048555374\n",
      "0.02814016304910183\n",
      "0.026776667684316635\n",
      "0.025494370609521866\n",
      "0.024288680404424667\n",
      "0.023155121132731438\n",
      "0.022089514881372452\n",
      "0.021087804809212685\n",
      "0.020146159455180168\n",
      "0.019260942935943604\n",
      "0.018428726121783257\n",
      "0.01764625310897827\n",
      "0.0169103741645813\n",
      "0.016218237578868866\n",
      "0.015567049384117126\n",
      "0.01495424099266529\n",
      "0.01437731459736824\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[295], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m800\u001b[39m):\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m     rsum, rdiff \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[280], line 62\u001b[0m, in \u001b[0;36mLinearModelPeriodic.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         pred_blocks_sum\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_blocks_sum[\u001b[38;5;28mtuple\u001b[39m(s)][k]\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m     61\u001b[0m pred_sum_tmap \u001b[38;5;241m=\u001b[39m TensorMap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_blocks_sum[\u001b[38;5;28mtuple\u001b[39m(s)]\u001b[38;5;241m.\u001b[39mkeys, pred_blocks_sum)\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecon_sum[\u001b[38;5;28mtuple\u001b[39m(s)] \u001b[38;5;241m=\u001b[39m \u001b[43m_to_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_uncoupled_basis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_sum_tmap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morbitals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morbitals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_blocks_diff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_blocks_diff[\u001b[38;5;28mtuple\u001b[39m(s)]\u001b[38;5;241m.\u001b[39mkeys:\n",
      "File \u001b[0;32m/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/twocenter_utils.py:324\u001b[0m, in \u001b[0;36m_to_matrix\u001b[0;34m(blocks, frames, orbitals, hermitian, device)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_matrix\u001b[39m(\n\u001b[1;32m    313\u001b[0m     blocks: TensorMap,\n\u001b[1;32m    314\u001b[0m     frames: List[ase\u001b[38;5;241m.\u001b[39mAtoms],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m#     return _vectorized_blocks_to_matrix(blocks=blocks, frames=frames, orbs=orbs)\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_blocks_to_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43morbitals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morbitals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhermitian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhermitian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/twocenter_utils.py:407\u001b[0m, in \u001b[0;36m_blocks_to_matrix\u001b[0;34m(blocks, frames, orbitals, device, hermitian)\u001b[0m\n\u001b[1;32m    405\u001b[0m         values \u001b[38;5;241m=\u001b[39m block_data[:, :, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m li \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m lj \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;66;03m# assign values\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m         _fill(\n\u001b[1;32m    408\u001b[0m             block_type,\n\u001b[1;32m    409\u001b[0m             matrix,\n\u001b[1;32m    410\u001b[0m             values,\n\u001b[1;32m    411\u001b[0m             ki_base,\n\u001b[1;32m    412\u001b[0m             kj_base,\n\u001b[1;32m    413\u001b[0m             ki_offset,\n\u001b[1;32m    414\u001b[0m             kj_offset,\n\u001b[1;32m    415\u001b[0m             same_koff,\n\u001b[1;32m    416\u001b[0m             li,\n\u001b[1;32m    417\u001b[0m             lj,\n\u001b[1;32m    418\u001b[0m             hermitian\u001b[38;5;241m=\u001b[39mhermitian,\n\u001b[1;32m    419\u001b[0m         )\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matrices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m matrices[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=1e-1, step_size_up=10, cycle_momentum=False)\n",
    "# losses = []\n",
    "for i in range(800):\n",
    "    optimizer.zero_grad()\n",
    "    rsum, rdiff = model.forward()\n",
    "    loss = loss_fn_indiv_shift(rsum, rdiff, matrices_sum, matrices_diff, specific_shift_idx = 'negative' )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step(loss)\n",
    "    losses.append(loss.item())\n",
    "    if i%10 ==0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c13cf759",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsum_final_minus, rdiff_final_minus = model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "8e302a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0] tensor(0.0376, dtype=torch.float64)\n",
      "[0, 1, 0] tensor(0.0375, dtype=torch.float64)\n",
      "[1, 1, 0] tensor(0.0689, dtype=torch.float64)\n",
      "[1, -1, 0] tensor(0.0013, dtype=torch.float64)\n",
      "[2, 0, 0] tensor(0.0002, dtype=torch.float64)\n",
      "[0, 2, 0] tensor(0.0002, dtype=torch.float64)\n",
      "[2, 1, 0] tensor(0.0007, dtype=torch.float64)\n",
      "[3, 0, 0] tensor(0.0447, dtype=torch.float64)\n",
      "[0, 3, 0] tensor(0.0672, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for s in desired_shifts[:]:\n",
    "#     print(s, torch.linalg.norm(rsum_final_minus[tuple(s)].detach().cpu() - matrices_sum[tuple(s)]))\n",
    "#     print(s, torch.linalg.norm(rdiff_final_minus[tuple(s)].detach().cpu() - matrices_diff[tuple(s)]))\n",
    "# predicted_matrices = get_predicted_matrices(rsum_final_minus, rdiff_final_minus)\n",
    "MINUS={}\n",
    "for s in desired_shifts[1:]:\n",
    "    MINUS[tuple(s)] = rsum_final_minus[tuple(s)] - rdiff_final_minus[tuple(s)]\n",
    "    print(s, torch.linalg.norm(MINUS[tuple(s)].detach().cpu() - matrices_diff[tuple(s)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "55349761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nigam/miniconda3/lib/python3.11/site-packages/hickle/lookup.py:1491: SerializedWarning: 'Tensor' type not understood, data is serialized:\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hickle.dump(PLUS, \"PLUS.hickle\")\n",
    "hickle.dump(MINUS, \"MINUS.hickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 0) tensor(0.0393, dtype=torch.float64) 2.1654325656602817\n",
      "(-1, 0, 0) tensor(0.0393, dtype=torch.float64) 2.165432565660281\n",
      "(0, 1, 0) tensor(0.0428, dtype=torch.float64) 6.561073399624882\n",
      "(0, -1, 0) tensor(0.0428, dtype=torch.float64) 6.561073399624881\n",
      "(1, 1, 0) tensor(0.0706, dtype=torch.float64) 6.647242582973258\n",
      "(-1, -1, 0) tensor(0.0705, dtype=torch.float64) 6.6472425829732575\n",
      "(1, -1, 0) tensor(0.0013, dtype=torch.float64) 0.1612810052173518\n",
      "(-1, 1, 0) tensor(0.0013, dtype=torch.float64) 0.16128100521735184\n",
      "(2, 0, 0) tensor(0.0003, dtype=torch.float64) 0.03207237927488492\n",
      "(-2, 0, 0) tensor(0.0003, dtype=torch.float64) 0.03207237927488493\n",
      "(0, 2, 0) tensor(0.0008, dtype=torch.float64) 0.2172764923110687\n",
      "(0, -2, 0) tensor(0.0008, dtype=torch.float64) 0.2172764923110686\n",
      "(2, 1, 0) tensor(0.0007, dtype=torch.float64) 0.16556662788895682\n",
      "(-2, -1, 0) tensor(0.0007, dtype=torch.float64) 0.16556662788895682\n",
      "(3, 0, 0) tensor(0.0494, dtype=torch.float64) 2.1654325656957245\n",
      "(-3, 0, 0) tensor(0.0494, dtype=torch.float64) 2.1654325656957245\n",
      "(0, 3, 0) tensor(0.0759, dtype=torch.float64) 6.561073400180453\n",
      "(0, -3, 0) tensor(0.0759, dtype=torch.float64) 6.561073400180453\n",
      "(0, 0, 0) tensor(0.0150, dtype=torch.float64) 54.20150788504924\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reconstruct_FIN = {}\n",
    "for s in desired_shifts[1:]:\n",
    "    reconstruct_FIN[tuple(s)] = PLUS[tuple(s)] + MINUS[tuple(s)] \n",
    "    reconstruct_FIN[tuple(-np.array(s))] = PLUS[tuple(s)] - MINUS[tuple(s)]\n",
    "\n",
    "for s in desired_shifts[:1]:\n",
    "    reconstruct_FIN[tuple(s)] = rsum_0[tuple(s)]\n",
    "\n",
    "for s in reconstruct_FIN.keys():\n",
    "    print(s, torch.linalg.norm(reconstruct_FIN[tuple(s)].detach().cpu() - selected_matrices[tuple(s)]), np.linalg.norm(selected_matrices[tuple(s)]))\n",
    "# reconstruct_FIN = get_predicted_matrices(PLUS, MINUS)\n",
    "# for s in desired_shifts[:]:\n",
    "#     print(s, torch.linalg.norm(reconstruct_FIN[tuple(s)].detach().cpu() - selected_matrices[tuple(s)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3f3c5",
   "metadata": {},
   "source": [
    "## reconstruct k-point target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "e6c0d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recons_kpt_target = []\n",
    "for ifr, frame in enumerate(frames): \n",
    "    kmatrix = torch.zeros((Nk, nao, nao)).type(torch.complex128)\n",
    "\n",
    "    # shift_indices = [np.where(np.all(Ls==np.asarray(s), axis=1))[0][0] for s in withnegative_shifts]\n",
    "    for key in withnegative_shifts:\n",
    "        key = tuple(key)\n",
    "        for kpt in range(Nk):\n",
    "            kmatrix[kpt] += reconstruct_FIN[key][ifr].detach().cpu() * weights[key] * expkL[key][kpt]\n",
    "    kmatrix = kmatrix / Nk\n",
    "    recons_kpt_target.append(kmatrix) \n",
    "recons_kpt_target = torch.stack(recons_kpt_target).swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "b72537d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0651, dtype=torch.float64)\n",
      "1 tensor(0.1141, dtype=torch.float64)\n",
      "2 tensor(0.0581, dtype=torch.float64)\n",
      "3 tensor(0.1141, dtype=torch.float64)\n",
      "4 tensor(0.1347, dtype=torch.float64)\n",
      "5 tensor(0.1178, dtype=torch.float64)\n",
      "6 tensor(0.1090, dtype=torch.float64)\n",
      "7 tensor(0.1183, dtype=torch.float64)\n",
      "8 tensor(0.0658, dtype=torch.float64)\n",
      "9 tensor(0.1248, dtype=torch.float64)\n",
      "10 tensor(0.0615, dtype=torch.float64)\n",
      "11 tensor(0.1248, dtype=torch.float64)\n",
      "12 tensor(0.1347, dtype=torch.float64)\n",
      "13 tensor(0.1183, dtype=torch.float64)\n",
      "14 tensor(0.1090, dtype=torch.float64)\n",
      "15 tensor(0.1178, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for kpt in range(Nk):\n",
    "    print(kpt, torch.linalg.norm(recons_kpt_target[kpt] - small_shifts_target[kpt]))\n",
    "# torch.linalg.norm(recons_kpt_target - small_shifts_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModelPeriodic(feat_plus, feat_minus, target_coupled_blocks_sum, target_coupled_blocks_diff, desired_shifts, frames, orbs, nhidden=16, nlayers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16023802757263184\n",
      "0.031491734087467194\n",
      "0.009251020848751068\n",
      "0.003096561646088958\n",
      "0.0009399798000231385\n",
      "0.0004689812776632607\n",
      "0.00024364719865843654\n",
      "0.00017960922559723258\n",
      "0.00014655283303000033\n",
      "0.00012858735863119364\n",
      "0.0001240893907379359\n",
      "0.00012232616427354515\n",
      "0.00012163409701315686\n",
      "0.00012134024291299284\n",
      "0.00012129032984375954\n",
      "0.00012125116336392239\n",
      "0.00012124008208047599\n",
      "0.00012123619671911001\n",
      "0.00012123472697567195\n",
      "0.0001212342904182151\n",
      "0.00012123407941544428\n",
      "0.00012123402848374099\n",
      "0.00012123401393182576\n",
      "0.00012123399937991053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb Cell 47\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m300\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     rsum, rdiff \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn_combined(rsum, rdiff, expkL_small, small_shifts_target)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32m/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb Cell 47\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     pred_diff_tmap \u001b[39m=\u001b[39m TensorMap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_blocks_diff[\u001b[39mstr\u001b[39m(s)]\u001b[39m.\u001b[39mkeys, pred_blocks_diff)   \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecon_sum[\u001b[39mstr\u001b[39m(s)] \u001b[39m=\u001b[39m _to_matrix(_to_uncoupled_basis(pred_sum_tmap), frames \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframes, orbitals\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morbitals)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecon_diff[\u001b[39mstr\u001b[39m(s)] \u001b[39m=\u001b[39m _to_matrix(_to_uncoupled_basis(pred_diff_tmap), frames \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframes, orbitals\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morbitals, hermitian\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcosmopc/home/nigam/scratch/MAC/k-hamiltonian/learn_kin.ipynb#Y236sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecon_sum, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecon_diff\n",
      "File \u001b[0;32m/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/twocenter_utils.py:544\u001b[0m, in \u001b[0;36m_to_uncoupled_basis\u001b[0;34m(blocks, cg, device)\u001b[0m\n\u001b[1;32m    533\u001b[0m     new_block \u001b[39m=\u001b[39m block_builder\u001b[39m.\u001b[39madd_block(\n\u001b[1;32m    534\u001b[0m         keys\u001b[39m=\u001b[39mblock_idx,\n\u001b[1;32m    535\u001b[0m         properties\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39masarray([[\u001b[39m0\u001b[39m]], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32),\n\u001b[1;32m    536\u001b[0m         components\u001b[39m=\u001b[39m[_components_idx(li), _components_idx(lj)],\n\u001b[1;32m    537\u001b[0m     )\n\u001b[1;32m    538\u001b[0m     new_block\u001b[39m.\u001b[39madd_samples(\n\u001b[1;32m    539\u001b[0m         labels\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39masarray(block\u001b[39m.\u001b[39msamples\u001b[39m.\u001b[39mvalues)\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m    540\u001b[0m             block\u001b[39m.\u001b[39msamples\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    541\u001b[0m         ),\n\u001b[1;32m    542\u001b[0m         data\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mmoveaxis(decoupled, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m    543\u001b[0m     )\n\u001b[0;32m--> 544\u001b[0m \u001b[39mreturn\u001b[39;00m block_builder\u001b[39m.\u001b[39;49mbuild()\n",
      "File \u001b[0;32m/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/metatensor_utils.py:86\u001b[0m, in \u001b[0;36mTensorBuilder.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m     blocks\u001b[39m.\u001b[39mappend(block\u001b[39m.\u001b[39mbuild())\n\u001b[1;32m     85\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(block, TensorBuilderPerSamples):\n\u001b[0;32m---> 86\u001b[0m     blocks\u001b[39m.\u001b[39mappend(block\u001b[39m.\u001b[39;49mbuild())\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid block type\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/nigam/b5749eb7-d3f1-4495-adeb-2c318fb7d0de/MAC/my_mlelec/src/mlelec/utils/metatensor_utils.py:129\u001b[0m, in \u001b[0;36mTensorBuilderPerSamples.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    128\u001b[0m     samples \u001b[39m=\u001b[39m Labels(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_names, np\u001b[39m.\u001b[39mvstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_samples))\n\u001b[0;32m--> 129\u001b[0m     block \u001b[39m=\u001b[39m TensorBlock(\n\u001b[1;32m    130\u001b[0m         values\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mcat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m),\n\u001b[1;32m    131\u001b[0m         samples\u001b[39m=\u001b[39;49msamples,\n\u001b[1;32m    132\u001b[0m         components\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_components,\n\u001b[1;32m    133\u001b[0m         properties\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_properties,\n\u001b[1;32m    134\u001b[0m     )\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_samples \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m         \u001b[39mraise\u001b[39;00m (\u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGradient data not implemented for BlockBuilderSamples\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/metatensor/block.py:72\u001b[0m, in \u001b[0;36mTensorBlock.__init__\u001b[0;34m(self, values, samples, components, properties)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m i, component \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(components):\n\u001b[1;32m     70\u001b[0m     components_array[i] \u001b[39m=\u001b[39m component\u001b[39m.\u001b[39m_as_mts_labels_t()\n\u001b[0;32m---> 72\u001b[0m values \u001b[39m=\u001b[39m ArrayWrapper(values)\n\u001b[1;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actual_ptr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lib\u001b[39m.\u001b[39mmts_block(\n\u001b[1;32m     75\u001b[0m     values\u001b[39m.\u001b[39minto_mts_array(),\n\u001b[1;32m     76\u001b[0m     samples\u001b[39m.\u001b[39m_as_mts_labels_t(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     properties\u001b[39m.\u001b[39m_as_mts_labels_t(),\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     81\u001b[0m _check_pointer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actual_ptr)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/metatensor/data/array.py:78\u001b[0m, in \u001b[0;36mArrayWrapper.__init__\u001b[0;34m(self, array)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m# `mts_array_t::ptr` is a pointer to the PyObject `self`\u001b[39;00m\n\u001b[1;32m     73\u001b[0m mts_array\u001b[39m.\u001b[39mptr \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mcast(\n\u001b[1;32m     74\u001b[0m     ctypes\u001b[39m.\u001b[39mpointer(ctypes\u001b[39m.\u001b[39mpy_object(\u001b[39mself\u001b[39m)), ctypes\u001b[39m.\u001b[39mc_void_p\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m \u001b[39m@catch_exceptions\u001b[39;49m\n\u001b[0;32m---> 78\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39mmts_array_origin\u001b[39;49m(this, origin):\n\u001b[1;32m     79\u001b[0m     origin[\u001b[39m0\u001b[39;49m] \u001b[39m=\u001b[39;49m array_origin\n\u001b[1;32m     81\u001b[0m \u001b[39m# use storage.XXX.__class__ to get the right type for all functions\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/metatensor/utils.py:31\u001b[0m, in \u001b[0;36mcatch_exceptions\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcatch_exceptions\u001b[39m(function):\n\u001b[1;32m     30\u001b[0m     \u001b[39m@functools\u001b[39;49m\u001b[39m.\u001b[39;49mwraps(function)\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mdef\u001b[39;49;00m \u001b[39minner\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs):\n\u001b[1;32m     32\u001b[0m         \u001b[39mtry\u001b[39;49;00m:\n\u001b[1;32m     33\u001b[0m             function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/functools.py:50\u001b[0m, in \u001b[0;36mupdate_wrapper\u001b[0;34m(wrapper, wrapped, assigned, updated)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_wrapper\u001b[39m(wrapper,\n\u001b[1;32m     36\u001b[0m                    wrapped,\n\u001b[1;32m     37\u001b[0m                    assigned \u001b[39m=\u001b[39m WRAPPER_ASSIGNMENTS,\n\u001b[1;32m     38\u001b[0m                    updated \u001b[39m=\u001b[39m WRAPPER_UPDATES):\n\u001b[1;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Update a wrapper function to look like the wrapped function\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m       wrapper is the function to be updated\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m       function (defaults to functools.WRAPPER_UPDATES)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m assigned:\n\u001b[1;32m     51\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m             value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(wrapped, attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True)\n",
    "losses = []\n",
    "for i in range(300):\n",
    "    optimizer.zero_grad()\n",
    "    rsum, rdiff = model.forward()\n",
    "    loss = loss_fn_combined(rsum, rdiff, expkL_small, small_shifts_target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step(loss)\n",
    "    losses.append(loss.item())\n",
    "    if i%10 ==0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
