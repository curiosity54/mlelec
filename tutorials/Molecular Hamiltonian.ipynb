{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359ff9a3-616a-4199-8aaa-b70b7a1f59bc",
   "metadata": {},
   "source": [
    "# Molecular Hamiltonian learning\n",
    "\n",
    "This notebook shows how to train a machine learning model to predict the hamiltonian matrix of water molecules, from `Nigam et al., J. Chem. Phys. 156, 014115 (2022), https://pubs.aip.org/aip/jcp/article/156/1/014115/2839817`, Equivariant representations for molecular Hamiltonians and N-center atomic-scale properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffeb27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.set_default_dtype(torch.float64) \n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "import ase \n",
    "from ase.io import read\n",
    "from ase.units import Hartree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json  \n",
    "import chemiscope\n",
    "import hickle\n",
    "\n",
    "from mlelec.features.acdc import compute_features_for_target\n",
    "from mlelec.data.dataset import get_dataloader\n",
    "from mlelec.data.dataset import MoleculeDataset\n",
    "from mlelec.data.dataset import MLDataset\n",
    "from mlelec.models.linear import LinearTargetModel\n",
    "from mlelec.data.pyscf_calculator import _instantiate_pyscf_mol\n",
    "from mlelec.utils.twocenter_utils import fix_orbital_order, unfix_orbital_order\n",
    "import mlelec.metrics as mlmetrics\n",
    "from mlelec.utils.learning_utils import compute_batch_dipole_moment, compute_dipole_moment_from_mf, instantiate_mf, compute_dipole_moment\n",
    "\n",
    "import os\n",
    "os.environ[\"PYSCFAD_BACKEND\"] = \"torch\"\n",
    "\n",
    "from pyscf import gto\n",
    "import pyscf.pbc.tools.pyscf_ase as pyscf_ase\n",
    "\n",
    "from pyscfad import numpy as pynp\n",
    "from pyscfad import ops\n",
    "from pyscfad.ml.scf import hf\n",
    "\n",
    "from IPython.utils import io\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a31516",
   "metadata": {},
   "source": [
    "# Create/load dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37810ad3-b1c0-4da1-98b9-a604f782138a",
   "metadata": {},
   "source": [
    "Loads the structures (that also contain properties in the info field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = ase.io.read('./data/water/water.xyz',':100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9380fa2-f7ce-45fd-a8fb-287004100574",
   "metadata": {},
   "source": [
    "Extract the information about dipoles from the `ASE` structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0ffe0-6eed-47c2-be5a-c6a75112208f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mu = chemiscope.ase_vectors_to_arrows(frames, key='mu')\n",
    "for m in mu['parameters']['structure']:\n",
    "    m['baseRadius'] *= 0.5\n",
    "    m['headRadius'] *= 0.5\n",
    "    m['color'] = 'green'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f86de-d104-4df4-b831-5a6f8c949da5",
   "metadata": {},
   "source": [
    "You can also visualize the structures and the dipoles with `chemiscope`. This runs only in a notebook, and requires having the `chemiscope` package installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e959700",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = chemiscope.show(frames, \n",
    "                         shapes = {'dipole': mu}, mode = 'structure',\n",
    "                         settings = {'structure': [{'shape': 'dipole'}]})\n",
    "\n",
    "if chemiscope.jupyter._is_running_in_notebook():\n",
    "    from IPython.display import display\n",
    "\n",
    "    display(widget)\n",
    "else:\n",
    "    widget.save(\"water_dipoles.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2677c4e",
   "metadata": {},
   "source": [
    "## Instantiate the Molecule Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8812c9",
   "metadata": {},
   "source": [
    "The MoleculeDataset contains all the information related to the molecules in the dataset. It defines target quantities such as the Fock matrices and the molecular dipoles. \n",
    "\n",
    "Here we are explicitly providing the data that belongs to this class. One can also provide the path to load the data from. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc44b97-319a-4ec9-bc8e-4b7397e60af7",
   "metadata": {},
   "source": [
    "If target and auxiliary data have already been computed, they can be loaded by setting `load_precomputed=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05b1ee-ef17-4811-8a0d-f70f226b085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_precomputed = True\n",
    "molecules_slice = slice(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9ad2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not load_precomputed:\n",
    "    \n",
    "    molecule_data = MoleculeDataset(mol_name = \"water\",\n",
    "                                    use_precomputed = False,\n",
    "                                    path = \"./data/water\", \n",
    "                                    aux_path = \"./data/water/sto-3g\", \n",
    "                                    frame_slice = molecules_slice,\n",
    "                                    device = device,\n",
    "                                    aux = [\"overlap\", \"orbitals\"],\n",
    "                                    target = [\"fock\", \"dipole_moment\"])\n",
    "                            \n",
    "else:\n",
    "    focks = hickle.load('./data/water/sto-3g/fock.hickle')\n",
    "    overlaps = hickle.load('./data/water/sto-3g/overlap.hickle')\n",
    "    orbitals = hickle.load('./data/water/sto-3g/orbitals.hickle')\n",
    "    molecule_data = MoleculeDataset(\n",
    "        mol_name = \"water\",\n",
    "        frames = frames,\n",
    "        frame_slice = molecules_slice, \n",
    "        device = device,\n",
    "        aux = [\"overlap\", \"orbitals\"],\n",
    "        target = [\"fock\", \"dipole_moment\"],\n",
    "        target_data = {\"fock\": focks, \"dipole_moment\": torch.from_numpy(np.array([frame.info['mu'] for frame in frames]))},\n",
    "        aux_data = {\"overlap\":overlaps, \"orbitals\":orbitals})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2ccbf",
   "metadata": {},
   "source": [
    "## Instantiate the MLDataset from the MoleculeDataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76363649",
   "metadata": {},
   "source": [
    "The MLDataset class contains all the information about the machine learning training process, such as features, targets, and training strategy.  \n",
    "\n",
    "Based on the strategy you'd like to use to build the model for the target (for example what kind of features must be used, what is the train-validation-test split of the dataset. \n",
    "Currently, the only implemented strategy is the `\"coupled\"` one, which means the training is performed on angular momentum coupled subblocks of the Hamiltonian matrix represented in a localized-orbital basis. Localized orbitals are labeled by angular momentum eigenvalues.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d649b-3166-46f4-9337-ed7fa686a163",
   "metadata": {},
   "source": [
    "Here we define the train-test-validation splitting fractions for the training, and define the MLDataset accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65977132-505c-4120-a7f2-5ee256cc3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_frac = 0.7\n",
    "val_frac = 0.2\n",
    "test_frac = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = MLDataset(\n",
    "    molecule_data = molecule_data,\n",
    "    device = device,\n",
    "    model_strategy = \"coupled\", \n",
    "    shuffle = True,\n",
    "    shuffle_seed = seed,\n",
    ")  \n",
    "\n",
    "ml_data._split_indices(train_frac = train_frac, val_frac = val_frac, test_frac = test_frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd75219",
   "metadata": {},
   "source": [
    "Features are computed with `rascaline`. \n",
    "We are building models based on the atom-centered density features. \n",
    "\n",
    "In `hypers` we explicitly define the hyperparameters of the features. Default values will be used if these were not specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b81d6ae-a6d0-452b-be49-4a7e36b970b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = {\n",
    "            \"cutoff\": 3.0,\n",
    "            \"max_radial\" : 8,\n",
    "            \"max_angular\": 6,\n",
    "            \"atomic_gaussian_width\": 0.3,\n",
    "            \"center_atom_weight\": 1,\n",
    "            \"radial_basis\": {\"Gto\": {}},\n",
    "            \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.1}},\n",
    "        }\n",
    "\n",
    "if hypers is None:\n",
    "    ml_data._set_features(compute_features_for_target(ml_data, device = \"cuda\"))\n",
    "else:\n",
    "    ml_data._set_features(compute_features_for_target(ml_data, device = device, hypers = hypers)) # one can pass hypers here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469bd1b5-ff84-429f-a9b3-a6f1763766ae",
   "metadata": {},
   "source": [
    "For batch training, one can pass the `batch_size` keyword to the `get_dataloader` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211cf1e-382c-4154-9834-6a9198f49dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dl, val_dl, test_dl = get_dataloader(ml_data, batch_size = batch_size, model_return = \"blocks\") # instantiate dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e6f60",
   "metadata": {},
   "source": [
    "# Model for Hamiltonian learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692668d7-40ab-45c5-a119-c37722aecb44",
   "metadata": {},
   "source": [
    "Here we define the model's architecture. The model maps the input features ($\\xi_{n_\\text{in}}$) with $n_\\text{in}$ dimensions to latent features with a (usually) smaller dimension $n_\\text{hidden}$. The last layer of the model maps the hidden features to the desired output ($y_{n_\\text{out}}$). \n",
    "\n",
    "If $n_\\text{layers} = 1$, no hidden layers are used.\n",
    "\n",
    "A bias parameter can be included when `bias = True`. In order not to break equivariance of the model's predictions, the bias is only allowed for the invariant channels, i.e., the ones with azimuthal quantum number $\\lambda=0$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57243f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearTargetModel(dataset = ml_data, nlayers = 3, nhidden = 16, bias = True, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c53792-99dc-4b4f-8d7c-3db5e4c47d99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ridge-Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf250f7-5651-4f8d-b2c8-5b061740ba0d",
   "metadata": {},
   "source": [
    "We can use the analytical ridge regression to find the optimum weights that map the features to the targets. We use the _scikit-learn_ implementation of ridge regression within our model \n",
    "\n",
    "DEFINE: Loss metric that we report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9fc662-97ee-4b86-be8c-26620eb1a958",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8953ea0-e803-4e29-802c-02ee8a45cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ridges, ridges  = model.fit_ridge_analytical(set_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4cd8d-0d46-441e-9fb7-cfd7c4521cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the loss over individual blocks\n",
    "block_losses = {}\n",
    "for k,b in ml_data.target_train.items():\n",
    "    block_losses[tuple(k.values)] = torch.linalg.norm(b.values - pred_ridges.block(k).values)**2\n",
    "loss_ = sum(block_losses.values()) # sum of squares losses of all the blocks \n",
    "\n",
    "\n",
    "# Get errors in eV from here \n",
    "normalizing_factor = 7 # TOFIX frames[0].get_global_number_of_atoms()*ml_data.molecule_data.basis. per species \n",
    "print(f\"Training Loss {np.sqrt(loss_/normalizing_factor) * Hartree*1000:.2f} meV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce1a029-73be-4c73-bc98-a324c1b9dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "x=[','.join([str(lbl[i]) for i in [0,2,3,5,6,7]]) for lbl in ml_data.target.blocks.keys.values.tolist()]\n",
    "fs = plt.rcParams['figure.figsize']\n",
    "fig, ax = plt.subplots(figsize = (fs[0]*2, fs[1]))\n",
    "# ax_loss = ax.twinx()\n",
    "# s = (0,0,0)\n",
    "prediction_ = np.array([torch.linalg.norm(b.values) for b in pred_ridges])\n",
    "target_ = np.array([torch.linalg.norm(b.values) for b in ml_data.target_train])\n",
    "loss_ = np.array([torch.linalg.norm(b.values-b1.values)**2 for b,b1 in zip(ml_data.target_train,pred_ridges)])\n",
    "print(np.sum(loss_))\n",
    "\n",
    "loss_blocks = list(block_losses.values())\n",
    "\n",
    "x_ = 3.5*np.arange(len(loss_blocks))\n",
    "\n",
    "labels = []\n",
    "handles = []\n",
    "pl = ax.bar(x_, prediction_, label = 'pred', width = 1, color = 'tab:blue');\n",
    "handles.append(pl)\n",
    "labels.append('Prediction')\n",
    "pl = ax.bar(x_+1, target_, alpha = 1, label = 'target', width = 1, color = 'tab:orange');\n",
    "handles.append(pl)\n",
    "labels.append('Target')\n",
    "\n",
    "# pl = ax_loss.bar(x_+2, loss_, alpha = 1, label = 'target', width = 1, color = 'tab:red');\n",
    "# handles.append(pl)\n",
    "# labels.append('Loss')\n",
    "\n",
    "# ax.set_ylim(1e-7, 1000)\n",
    "ax.set_xticks(3.5*np.arange(len(loss_blocks))+3.5/3-0.5)\n",
    "ax.set_xticklabels(x, rotation=90);\n",
    "ax.legend(handles, labels, loc = 'best')\n",
    "ax.set_ylabel('|H|')\n",
    "# ax_loss.set_ylabel('Loss')\n",
    "# ax_loss.set_yscale('log')\n",
    "# # ax_loss.set_ylim(1e-7)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Performance on the training set')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d45339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlelec.utils.twocenter_utils import _to_uncoupled_basis, _to_matrix\n",
    "reconstructed_uncoupled = _to_uncoupled_basis(pred_ridges,  device=model.device) # Convert the coupled blocks to uncoupled\n",
    "\n",
    "# Recover the predicted matrices for the training set \n",
    "fock_predictions_train = _to_matrix(\n",
    "   reconstructed_uncoupled,\n",
    "    ml_data.train_frames,\n",
    "     ml_data.aux_data['orbitals'],\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "print(f'Train RMSE: {torch.sqrt(torch.linalg.norm((fock_predictions_train - ml_data.target.tensor[ml_data.train_idx]))**2 / len(ml_data.train_idx) )}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887cdbd7-9416-4bff-b064-40ac4f041c2d",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26675680-8079-4fd8-bd3f-46c15425751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ridges_val  = model.predict_ridge_analytical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41632182-dce3-4b2d-8e57-eef8539f9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_losses = {}\n",
    "for k,b in ml_data.target_val.items():\n",
    "    block_losses[tuple(k.values)] = torch.linalg.norm(b.values - pred_ridges_val.block(k).values)**2\n",
    "loss_ = sum(block_losses.values()) # sum of squares losses of all the blocks \n",
    "\n",
    "\n",
    "# Get errors in eV from here \n",
    "normalizing_factor = 7 # TOFIX frames[0].get_global_number_of_atoms()*ml_data.molecule_data.basis.\n",
    "print(f\"Validation Loss {np.sqrt(loss_/normalizing_factor) * Hartree*1000:.2f} meV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc5373-0fa6-4888-9751-fcf4a61aa3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "x=[','.join([str(lbl[i]) for i in [0,2,3,5,6,7]]) for lbl in ml_data.target.blocks.keys.values.tolist()]\n",
    "fs = plt.rcParams['figure.figsize']\n",
    "fig, ax = plt.subplots(figsize = (fs[0]*2, fs[1]))\n",
    "# ax_loss = ax.twinx()\n",
    "# s = (0,0,0)\n",
    "prediction_ = np.array([torch.linalg.norm(b.values) for b in pred_ridges_val])\n",
    "target_ = np.array([torch.linalg.norm(b.values) for b in ml_data.target_val])\n",
    "loss_ = np.array([torch.linalg.norm(b.values-b1.values)**2 for b,b1 in zip(ml_data.target_val,pred_ridges_val)])\n",
    "print(np.sum(loss_))\n",
    "\n",
    "loss_blocks = list(block_losses.values())\n",
    "\n",
    "x_ = 3.5*np.arange(len(loss_blocks))\n",
    "\n",
    "labels = []\n",
    "handles = []\n",
    "pl = ax.bar(x_, prediction_, label = 'pred', width = 1, color = 'tab:blue');\n",
    "handles.append(pl)\n",
    "labels.append('Prediction')\n",
    "pl = ax.bar(x_+1, target_, alpha = 1, label = 'target', width = 1, color = 'tab:orange');\n",
    "handles.append(pl)\n",
    "labels.append('Target')\n",
    "\n",
    "# pl = ax_loss.bar(x_+2, loss_, alpha = 1, label = 'target', width = 1, color = 'tab:red');\n",
    "# handles.append(pl)\n",
    "# labels.append('Loss')\n",
    "\n",
    "# ax.set_ylim(1e-7, 1000)\n",
    "ax.set_xticks(3.5*np.arange(len(loss_blocks))+3.5/3-0.5)\n",
    "ax.set_xticklabels(x, rotation=90);\n",
    "ax.legend(handles, labels, loc = 'best')\n",
    "ax.set_ylabel('|H|')\n",
    "# ax_loss.set_ylabel('Loss')\n",
    "# ax_loss.set_yscale('log')\n",
    "# # ax_loss.set_ylim(1e-7)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Performance on the training set')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d59bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reconstructed_uncoupled = _to_uncoupled_basis(pred_ridges_val,  device=model.device) # Convert the coupled blocks to uncoupled\n",
    "\n",
    "# Recover the predicted matrices for the validation set \n",
    "fock_predictions_val = _to_matrix(\n",
    "   reconstructed_uncoupled,\n",
    "    ml_data.val_frames,\n",
    "     ml_data.aux_data['orbitals'],\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "print(f'Validation RMSE: {torch.sqrt(torch.linalg.norm((fock_predictions_val - ml_data.target.tensor[ml_data.val_idx]))**2 / len(ml_data.val_idx) )}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a5dc0-173f-449b-80f1-d9fc4fdb1343",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Predict properties from these trained Hamiltonians "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05ac60e-3de9-4ede-ab33-14e04e5c8c79",
   "metadata": {},
   "source": [
    "With ridge-regression based models, we can predict properties that can be derived from the Hamiltonians (but not use them in the training). We use PySCF-AD (for convenience) to provide the predicted Hamiltonian as the input and borrow their functions to compute the derived properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf16835-d6dd-46a0-a253-0e1ae3b2274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSCFAD_BACKEND\"] = \"torch\"\n",
    "import torch\n",
    "from pyscf import gto\n",
    "from pyscfad import numpy as pynp\n",
    "from pyscfad import ops\n",
    "from pyscfad.ml.scf import hf\n",
    "import pyscf.pbc.tools.pyscf_ase as pyscf_ase\n",
    "from mlelec.data.pyscf_calculator import _instantiate_pyscf_mol\n",
    "from mlelec.utils.twocenter_utils import fix_orbital_order, unfix_orbital_order\n",
    "# import mlelec.metrics as mlmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a87e6e-80a3-46cb-a987-7a85ab0eda17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dipole_moment(frames, fock_predictions, overlaps):\n",
    "    assert (\n",
    "        len(frames) == len(fock_predictions) == len(overlaps)\n",
    "    ), \"Length of frames, fock_predictions, and overlaps must be the same\"\n",
    "    dipoles = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        mol = _instantiate_pyscf_mol(frame)\n",
    "        mf = hf.SCF(mol)\n",
    "        fock = torch.autograd.Variable(\n",
    "            fock_predictions[i].type(torch.float64), requires_grad=True\n",
    "        )\n",
    "\n",
    "        mo_energy, mo_coeff = mf.eig(fock, overlaps[i])\n",
    "        mo_occ = mf.get_occ(mo_energy)  # get_occ returns a numpy array\n",
    "        mo_occ = ops.convert_to_tensor(mo_occ)\n",
    "        dm1 = mf.make_rdm1(mo_coeff, mo_occ)\n",
    "        dip = mf.dip_moment(dm=dm1)\n",
    "        dipoles.append(dip)\n",
    "    return torch.stack(dipoles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91546096-bbc1-471b-be61-32a51962f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "import mlelec.metrics as mlmetrics\n",
    "# with HiddenPrints():\n",
    "\n",
    "fock_reference_train = ml_data.molecule_data.target['fock'][ml_data.train_idx] \n",
    "fock_reference_val = ml_data.molecule_data.target['fock'][ml_data.val_idx] \n",
    "\n",
    "with io.capture_output() as captured:\n",
    "   # Compute the dipole moments of reference Fock matrix\n",
    "    dipole_reference_train = compute_dipole_moment(\n",
    "        ml_data.train_frames,\n",
    "        fock_reference_train,\n",
    "        ml_data.molecule_data.aux_data[\"overlap\"][ml_data.train_idx],\n",
    "    )\n",
    "    # convert prediction back to pyscf order before passing to pyscf \n",
    "    fock_predictions_train = unfix_orbital_order(\n",
    "        fock_predictions_train,\n",
    "        ml_data.train_frames,\n",
    "        ml_data.molecule_data.aux_data[\"orbitals\"],\n",
    "    )\n",
    "    # Compute the dipole moments of the prediction of the training set\n",
    "    dipole_prediction_train = compute_dipole_moment(\n",
    "        ml_data.train_frames,\n",
    "        fock_predictions_train,\n",
    "        ml_data.molecule_data.aux_data[\"overlap\"][ml_data.train_idx],\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Repeat the procedure above for the validation set \n",
    "    dipole_reference_val = compute_dipole_moment(\n",
    "        ml_data.val_frames,\n",
    "        fock_reference_val,\n",
    "        ml_data.molecule_data.aux_data[\"overlap\"][ml_data.val_idx],\n",
    "    )\n",
    "    \n",
    "    fock_predictions_val = unfix_orbital_order(\n",
    "        fock_predictions_val,\n",
    "        ml_data.val_frames,\n",
    "        ml_data.molecule_data.aux_data[\"orbitals\"],\n",
    "    )\n",
    "\n",
    "    dipole_prediction_val = compute_dipole_moment(\n",
    "        ml_data.val_frames,\n",
    "        fock_predictions_val,\n",
    "        ml_data.molecule_data.aux_data[\"overlap\"][ml_data.val_idx],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e0a9e-4faa-462e-95da-4d77b2b96cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "square_loss = mlmetrics.L2_loss(dipole_reference_train , dipole_prediction_train)\n",
    "print(f\"RMSE on dipoles (training set) (a.u.):  {torch.sqrt(square_loss / len(dipole_prediction_train)).item()}\")\n",
    "\n",
    "square_loss = mlmetrics.L2_loss(dipole_reference_val , dipole_prediction_val)\n",
    "print(f\"RMSE on dipoles (validation set) (a.u.): {torch.sqrt(square_loss / len(dipole_prediction_val)).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90682fe3-8efd-4b2f-b256-d752c160b1aa",
   "metadata": {},
   "source": [
    "### Maybe we make a plot of the dipoles?  (parity? or actual vector field?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f981be-3704-4a25-9bc2-74176b980354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fa6dd84-8c24-4456-aa97-9027524305d5",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e1b87f-2e0f-4fc4-8c97-88d6e7747193",
   "metadata": {},
   "source": [
    "We train the model by minimizing a loss function through stochastic gradient descent as implemented in PyTorch. The loss function is quadratic in the Hamiltonian matrix elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0b702-78c4-4ab9-b319-e51850e43451",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = getattr(mlmetrics, \"L2_loss\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8add75-dcf2-4ff1-b726-b087acb70e3c",
   "metadata": {},
   "source": [
    "We define an `early_stop_criterion` to stop the training before reaching the total number of epochs if the error on the validation set stops decreasing for 50 steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import inf\n",
    "best = inf\n",
    "early_stop_criterion = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184cc567-f7a0-4374-9108-8d6358fe40bb",
   "metadata": {},
   "source": [
    "We set up an Adam optimizer with initial learning rate `lr = 1e-3`. We schedule the reduction of a factor 0.5 of the learning rate when the value of the loss function does not change for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291bdbe2-846f-4e28-996c-b647143b7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.5, patience = 20, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef2e7f-cfb4-4ef7-bf5b-110a1f6dff93",
   "metadata": {},
   "source": [
    "We compute the loss function on the validation set every 10 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a6af6-ce3a-4c60-b846-de6e08b8eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_interval = 10\n",
    "losses = []\n",
    "early_stop_count = 0\n",
    "nepochs = 800\n",
    "Path('model').mkdir(exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(nepochs):\n",
    "    \n",
    "    model.train(True)\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data[\"input\"], return_type = \"coupled_blocks\", batch_indices = data[\"idx\"])  \n",
    "        loss = loss_fn(pred, data[\"output\"])\n",
    "        train_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(train_loss.item())\n",
    "    scheduler.step(train_loss)\n",
    "    model.train(False)\n",
    "\n",
    "    if epoch % val_interval == 0:\n",
    "        val_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(val_dl):\n",
    "            pred = model(data[\"input\"], return_type = \"coupled_blocks\", batch_indices = data[\"idx\"])\n",
    "            vloss = loss_fn(pred, data[\"output\"])\n",
    "            val_loss += vloss.item()\n",
    "        new_best = val_loss < best\n",
    "        \n",
    "        if new_best:\n",
    "            best = val_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            \n",
    "        if early_stop_count > early_stop_criterion:\n",
    "            print(f\"\\n\\nEarly stopping at epoch {epoch}\")\n",
    "            print(f\"Epoch {epoch}, train loss {train_loss/len(ml_data.train_idx)}\")\n",
    "            print(f\"Epoch {epoch} val loss {val_loss/len(ml_data.val_idx)}\")\n",
    "            break\n",
    "            \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:>5d}, train. loss: {train_loss/len(ml_data.train_idx):>10.6e}. Val. loss: {val_loss/len(ml_data.val_idx):>10.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211dc253-a46c-4a79-aa09-531706423607",
   "metadata": {},
   "source": [
    "Plot of the loss function with respect to the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80667377-9165-4e81-b9a3-caf3c5afbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.array(losses[5:])*Hartree**2)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel('Number of epochs')\n",
    "ax.set_ylabel(r'Loss function ($\\mathrm{eV}^2$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c945262-a196-4357-9eba-a364062b3968",
   "metadata": {},
   "source": [
    "# Indirect learning of molecular dipole moments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d4005c-4d1a-4af7-b036-002c45e25cb8",
   "metadata": {},
   "source": [
    "We can exploit the autodifferentiation functionalities of `PySCFAD` to indirectly learn the Hamiltonian matrices by optimizing some observable quantity such as molecular dipoles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57eb358-47a3-4d9a-b8e7-6c128ee3ebed",
   "metadata": {},
   "source": [
    "We instantiate `PySCF` calculators to be filled with the predictions of the previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419094a-4645-4681-8fb2-db2e96a20beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.capture_output() as captured:\n",
    "    all_mfs, fockvars = instantiate_mf(\n",
    "        ml_data,\n",
    "        fock_predictions=None,\n",
    "        batch_indices=list(range(len(ml_data.structures))),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f98641-e2bf-419b-bf5e-4cd98227d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import inf\n",
    "best = inf\n",
    "early_stop_criteria = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e3347-280e-4c1a-9470-aea5c7ea8eca",
   "metadata": {},
   "source": [
    "It is either possible to use a pretrained model, o we can reinstantiate a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db68cec-39b8-4f53-9ba6-d99eaddb2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_previous_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03027e-4bee-477b-8e65-19160ec42509",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_previous_model:\n",
    "    model = LinearTargetModel(dataset=ml_data, nlayers=1, nhidden=16, bias=False, device=\"cuda\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.5, patience = 10, verbose = True)\n",
    "\n",
    "val_interval = 10\n",
    "nepochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc00e8d-8c76-4170-a549-46177744ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = getattr(mlmetrics, \"L2_loss\")\n",
    "\n",
    "losses = []\n",
    "early_stop_count = 0\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    model.train(True)\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        batch_indices = [d.item() for d in data[\"idx\"]]\n",
    "        train_focks = model(\n",
    "            data[\"input\"], return_type=\"tensor\", batch_indices=batch_indices\n",
    "        ).type(torch.float64)\n",
    "        with io.capture_output() as captured:\n",
    "            train_dip_pred = compute_batch_dipole_moment(\n",
    "                ml_data, train_focks, batch_indices=batch_indices, mfs=all_mfs\n",
    "            )\n",
    "        loss = loss_fn(\n",
    "            train_dip_pred, ml_data.molecule_data.target[\"dipole_moment\"][batch_indices]\n",
    "        )\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(train_loss)\n",
    "    scheduler.step(train_loss)\n",
    "    model.train(False)\n",
    "\n",
    "    if epoch % val_interval == 0:\n",
    "        val_loss = 0\n",
    "        for i, data in enumerate(val_dl):\n",
    "            batch_indices = [d.item() for d in data[\"idx\"]]\n",
    "            val_focks = model(\n",
    "                data[\"input\"], return_type=\"tensor\", batch_indices=batch_indices\n",
    "            ).type(torch.float64)\n",
    "            with io.capture_output() as captured:\n",
    "                val_dip_pred = compute_batch_dipole_moment(\n",
    "                    ml_data, val_focks, batch_indices=batch_indices, mfs=all_mfs\n",
    "                )\n",
    "\n",
    "            vloss = loss_fn(\n",
    "                val_dip_pred,\n",
    "                ml_data.molecule_data.target[\"dipole_moment\"][batch_indices],\n",
    "            )\n",
    "            val_loss += vloss.item()\n",
    "        new_best = val_loss < best\n",
    "        if new_best:\n",
    "            best = val_loss\n",
    "            torch.save(model.state_dict(), './models/best_model_dipole.pt')\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "        if early_stop_count > early_stop_criteria:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            print(f\"Epoch {epoch}, train loss {train_loss/len(ml_data.train_idx)}\")\n",
    "\n",
    "            print(f\"Epoch {epoch} val loss {val_loss/len(ml_data.val_idx)}\")\n",
    "            break\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:>5d}, train. loss: {train_loss/len(ml_data.train_idx):>10.6e}. Val. loss: {val_loss/len(ml_data.val_idx):>10.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5408ac6-7fd6-4208-af1e-ab290fa675af",
   "metadata": {},
   "source": [
    "We plot the loss function on dipoles in Debye units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f500b4-d633-4b0e-a887-6a911c71a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Debye = 1/0.393456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739bd9a8-d203-4ae3-8cc9-0d6f007aea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot  as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.array(losses[5:])*Debye**2)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel('Number of epochs')\n",
    "ax.set_ylabel(r'Loss function ($\\mathrm{De}^2$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5574c37-7873-4286-8300-bc55039ff0f4",
   "metadata": {},
   "source": [
    "We can compute the root mean squared error (RMSE) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b325d-21a9-46e6-91d0-645af452bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.capture_output() as captured:\n",
    "    batch_indices = ml_data.test_idx\n",
    "    test_fock_predictions = model.forward(ml_data.feat_test, return_type = \"tensor\", batch_indices = ml_data.test_idx).type(torch.float64)\n",
    "    test_dip_pred = compute_batch_dipole_moment(ml_data, test_fock_predictions, batch_indices = batch_indices, mfs = all_mfs)\n",
    "\n",
    "error = mlmetrics.L2_loss(test_dip_pred, ml_data.molecule_data.target[\"dipole_moment\"][ml_data.test_idx])\n",
    "print(f\"Test RMSE on dipoles {Debye*np.sqrt(error.item() / len(test_dip_pred)):.4f} Debye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b2888-c3a3-4ab3-9eb7-04844566a36b",
   "metadata": {},
   "source": [
    "We can also compute the RMSE on the indirectly learnt Hamiltonians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d58a8-99a1-40dc-8904-44a48afd0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error on canonical ordered fock matrix\n",
    "\n",
    "rmse = Hartree*np.sqrt((mlmetrics.L2_loss(test_fock_predictions, ml_data.target.tensor[ml_data.test_idx])/len(test_fock_predictions)).item())\n",
    "print(f'Test RMSE on Hamiltonians: {rmse:.2f} eV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa305a95-539e-44b5-b836-d1e9e700a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dipole_moment(frames, fock_predictions, overlaps):\n",
    "    assert (\n",
    "        len(frames) == len(fock_predictions) == len(overlaps)\n",
    "    ), \"Length of frames, fock_predictions, and overlaps must be the same\"\n",
    "    dipoles = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        mol = _instantiate_pyscf_mol(frame)\n",
    "        mf = hf.SCF(mol)\n",
    "        fock = torch.autograd.Variable(\n",
    "            fock_predictions[i].type(torch.float64), requires_grad=True\n",
    "        )\n",
    "\n",
    "        mo_energy, mo_coeff = mf.eig(fock, overlaps[i])\n",
    "        mo_occ = mf.get_occ(mo_energy)  # get_occ returns a numpy array\n",
    "        mo_occ = ops.convert_to_tensor(mo_occ)\n",
    "        dm1 = mf.make_rdm1(mo_coeff, mo_occ)\n",
    "        dip = mf.dip_moment(dm=dm1)\n",
    "        dipoles.append(dip)\n",
    "    return torch.stack(dipoles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0dc0a2-5dc7-4bba-8ed2-e84bd0e5d63e",
   "metadata": {},
   "source": [
    "We can predict the dipoles of the test set and visually compare them to the target ones through chemiscope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f528e-aa1b-4a91-8e37-81646f91937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_dipoles = np.zeros((ml_data.test_idx.shape[0], 3))\n",
    "j = 0\n",
    "for i, data in enumerate(test_dl):\n",
    "    batch_indices = [d.item() for d in data[\"idx\"]]\n",
    "    test_focks = model(data[\"input\"], return_type = \"tensor\", batch_indices = batch_indices).type(torch.float64)\n",
    "    test_dip_pred = compute_batch_dipole_moment(ml_data, test_focks, batch_indices = batch_indices, mfs = all_mfs)\n",
    "    for p in test_dip_pred:\n",
    "        predicted_dipoles[j] = p.detach().numpy()\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48fe8a-6ec3-4f9f-a255-a5b03d3132d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frames = [frames[i] for i in ml_data.test_idx]\n",
    "mu_test = chemiscope.ase_vectors_to_arrows(test_frames, key = 'mu')\n",
    "for m in mu_test['parameters']['structure']:\n",
    "    m['baseRadius'] *= 0.5\n",
    "    m['headRadius'] *= 0.5\n",
    "    m['color'] = 'green'\n",
    "\n",
    "for f, p in zip(test_frames, predicted_dipoles):\n",
    "    f.info['mu_pred'] = p\n",
    "mu_pred = chemiscope.ase_vectors_to_arrows(test_frames, key = 'mu_pred')\n",
    "for m in mu_pred['parameters']['structure']:\n",
    "    m['baseRadius'] *= 0.5\n",
    "    m['headRadius'] *= 0.5\n",
    "    m['color'] = 'blue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa70ba1-6f55-4c69-8a77-a11480cdbf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = chemiscope.show(test_frames, shapes = {'dipole': mu_test, 'predicted_dipole': mu_pred}, mode = 'structure',\n",
    "                         settings = {'structure': [{'bonds': True, 'atoms': True, 'shape': ['dipole', 'predicted_dipole']}]})\n",
    "if chemiscope.jupyter._is_running_in_notebook():\n",
    "    from IPython.display import display\n",
    "    display(widget)\n",
    "else:\n",
    "    widget.save(\"water_dipoles_prediction.json.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
